[
  {
    "path": "posts/data-import/",
    "title": "How to Import Tabular, Plain-Text Data Files",
    "description": "Learn how to import (and export) various tabular data file formats such as CSV, TXT, and XLSX using a range of packages such as {utils}, {vroom}, {readr}, {data.table}, {readxl}, and {rio}. The article also includes best practices when specifying paths and a quick discussion on handling R specific formats such as RDS and Rdata.",
    "author": [
      {
        "name": "Cedric Scherer",
        "url": "https://cedricscherer.com"
      }
    ],
    "date": "2023-12-07",
    "categories": [
      "rstats",
      "workflow",
      "tutorial"
    ],
    "contents": "\r\n\r\nContents\r\nFile Paths\r\nAbsolute Paths: Start from the Root Directory\r\nRelative Paths: Start from the Working Directory\r\n\r\nData Import\r\n{utils} aka “base R”\r\n{vroom} and {readr}\r\n{data.table}\r\n{readxl}\r\n{rio}\r\nWorking with R-specific Formats\r\n\r\nData Export\r\n\r\nThere is one essential step to make use of the power of the R programming language to wrangle, analyze, visualize, and communicate our data: getting the data into R. In this blog post, we will show you multiple approaches for various tabular, plain-text file formats. Note that this blog post is focusing on tabular data — if you need to import spatial data files, have a look here.\r\nFile Paths\r\nIn this blog post, we will make use of files stored remotely but all the workflows would be the same if the file is placed in a local directory. There are multiple ways how to specify the path to a local file:\r\nAbsolute Paths: Start from the Root Directory\r\nsomething like C:\\\\Username\\\\Documents\\\\...\\\\data.csv for Windows or /Users/Username/Documents/.../data.csv for MacOS and Linux\r\nfor Windows paths you have to change the backslashes to forward slashes or escape them by using two backslashes (as in the path above)\r\nwill always point to the correct file on your current machine with the current setup\r\nhowever, this is not recommended as it fails to find the file with any other setup\r\nRelative Paths: Start from the Working Directory\r\nsomething like ./data/data.csv or simply data/data.csv\r\nwill always point to the correct file if the working directory is set correctly and the folder structure is the same\r\nthe current working directory can be retrieved via getwd() and is displayed in Rstudio at the top of your console\r\nthe recommended workflow to ensure comparability across machines, operating systems, and collaborators\r\nR Projects\r\nTo simplify your life, make use of R projects. The .Rproj file can be used as a shortcut for opening the project directly from the filesystem with the working directory set to that path. This way, you do not need to set the working directory manually (which again can cause issues on different machines, systems etc.).\r\n{here} Package\r\nWhen you are working with nested directories and especially notebook formats such as Rmarkdown or Quarto, the {here} package is helpful to make sure that the same working directory is used, no matter if you run the code locally or if you render a notebook that is placed in a subfolder. Read more about the {here} package here.\r\nData Import\r\nFor the showcase of different I/O packages we use data on deforestation provided by OurWorldInData which is hosted on the TidyTuesday repository:\r\n\r\n\r\npath_to_data <- \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-04-06/forest.csv\"\r\n\r\n\r\nUsually, you can specify the symbols used as separators (indicating columns), for quotation marks, for decimal points and more.\r\nIf the file would be stored locally, a relative path coudl look like this:\r\n\r\n\r\npath_to_data <- \"./data/raw/forest.csv\"\r\npath_to_data <- \"data/raw/forest.csv\"\r\n\r\n## with the {here} package\r\npath_to_data <- here::here(\"data\", \"raw\", \"forest.csv\")\r\n\r\n\r\n{utils} aka “base R”\r\nThe read.*() functions from the {utils} package return data frames.\r\nread.table() is a generic function that can be used for all kinds of tabular data formats; read.csv() is a short-hand wrapper that calls read.delim() with sensible defaults for CSV files.\r\n\r\n\r\ndat <- read.table(path_to_data, sep = \",\", quote = \"\\\"'\", dec = \".\")\r\ndat <- read.csv(path_to_data)\r\n\r\n\r\nOften (as in this case) we have column names that should be used. We can indicate that we want to use those variable names instead by setting header = TRUE:\r\n\r\n\r\ndat <- read.csv(path_to_data, header = TRUE)\r\n\r\n\r\nIf you have “German” CSV files which use a semicolon as column separator (so strictly speaking no CSV files at all) and a , as decimal separator, you can use read.csv2() which uses sep = \";\" and dec = \",\" as the defaults. There are also shorthand wrappers for tab-separated data formats (delimited files) named read.delim() and read.delim2().\r\nYou can directly specify the column types by passing a vector that contains the classes for all columns:\r\n\r\n\r\ndat <- \r\n  read.csv(\r\n    path_to_data, header = TRUE, \r\n    colClasses = c(code = \"factor\", year = \"factor\")\r\n)\r\n\r\n\r\nIf you want to inspect your data, you can use the nrows argument to specify the maximum number of rows to import. If the file contains any rows that should be skipped before reading the data (often the case if the file contains also meta data as in governmental data or NetLogo outputs), you can specify the number of rows to skip via skip.\r\n\r\n\r\ndat <- read.csv(path_to_data, nrows = 5, skip = 3)\r\n\r\n\r\nAnother argument you might commonly see is stringsAsFactors. As of version 4.0.0, the default behaviour has changed; R treats strings in data frames as strings rather than factors now. You can activate that functionality by setting stringsAsFactors = TRUE but this is usually not advised. Ig is likely better to specify factor columns via the colClasses argument or to convert them at a later point.\r\n{vroom} and {readr}\r\nThe vroom() function from the {vroom} package and the read_*() functions from the {readr} package return tibbles (the “modern data frames” used within the tidyverse). They work mostly the same as the read.*() functions but are meant to be faster and come with several additional options to control imports. The {readr} functions call vroom() so they behave exactly the same (and might be merged into a single package at some point).\r\n\r\n\r\ndat <- vroom::vroom(path_to_data)\r\ndat <- readr::read_delim(path_to_data, delim = \",\")\r\ndat <- readr::read_csv(path_to_data)\r\n\r\n\r\nThere are shorthand wrappers for different file formats including:\r\nread_delim() for delimited files in general\r\nread_table() for whitespace-separated files\r\nread_csv() for comma-separated values (CSV)\r\nread_csv2() for semicolon-separated values with comma as the decimal mark\r\nread_tsv() for tab-separated values (TSV)\r\nread_fwf() for fixed-width files\r\nAgain, one can control the column types in the same step. vroom() and read_*() functions allow to specify the types via the col_*() functions which allow for additional arguments or as a very compact combination of shortcuts. The shortcuts are representing columns of type character c, double d, factor f, logical l, integer i, and date D.\r\n\r\n\r\nlibrary(vroom)\r\n\r\n## specify types for certain columns\r\ndat <- \r\n  vroom(path_to_data, col_types = cols(\r\n    code = col_factor(), \r\n    year = col_factor(levels = c(\"1990\", \"2000\", \"2010\", \"2015\"))\r\n  ))\r\n\r\n## specify types as shortcuts for all columns\r\ndat <- vroom(path_to_data, col_types = \"cffi\")\r\n\r\n\r\n{data.table}\r\nThe {data.table} package offers a “fast read” function that is especially recommended when working with large files with many rows.\r\n\r\n\r\ndat <- data.table::fread(path_to_data)\r\n\r\n\r\nDifferent file types can be imported by specifying the separator between columns via sep. Nested lists are also supported, then you have to specify sep2 to define the separator within columns.\r\nAs in the read.*() functions, one can specify the type of the columns right away. Multiple ways are possible:\r\n\r\n\r\n## specified as in read.*()\r\ndat <- data.table::fread(path_to_data, colClasses = c(code = \"factor\", year = \"factor\")) \r\n\r\n## columns specified by type with names\r\ndat <- data.table::fread(path_to_data, colClasses = list(factor = c(\"code\", \"year\")))\r\n\r\n## columns specified by type with numbers\r\ndat <- data.table::fread(path_to_data, colClasses = list(factor = 2:3))\r\n\r\n\r\n{readxl}\r\nExcel files have a very specific format and can contain multiple sheets. In general, it is advised to avoid Excel files whenever it is possible. If you have to work with XLS or XLSX files, you likely should not open the files with Excel (as it can introduce multiple issues as it may change column formats). Better import it into R to inspect and wrangle the data. You can use the {readxl} package to handle those files:\r\n\r\n\r\ndat <- readxl::read_excel(\"data/data.xlsx\")\r\n\r\ndat <- readxl::read_xls(\"data/data.xls\")\r\ndat <- readxl::read_xlsx(\"data/data.xlsx\")\r\n\r\n\r\nThe functions from the {readxl} package only work with paths to local files, not URLs. The example above uses some placeholder names, the files do not exists. If you know the type of your file, you may want to use read_xls() or read_xlsx(), respectively, as read_excel() is guessing the right format.\r\nBy default, the functions import the first sheet. You can specify the sheet to import by passing either a number or the name of the sheet:\r\n\r\n\r\n## open sheet by number\r\ndat <- readxl::read_xlsx(\"data/data.xlsx\", sheet = 2)\r\n\r\n## open sheet by name\r\ndat <- readxl::read_xlsx(\"data/data.xlsx\", sheet = \"Table 2\") \r\n\r\n\r\n{rio}\r\nThe {rio} package describes itself as “the Swiss army knife for data I/O”. The idea is that a single function can be used for any file format which is detected on the fly. The package is using several of the previously mentioned packages to achieve full flexibility with regard to file support.\r\n\r\n\r\ndata <- rio::import(path_to_data)\r\n\r\n\r\nWhen passing a CSV file as in this example, the import() function uses fread() from the {data.table} package by default. The import() functions supports all kind of tabular data and also other file format including SAS, SPSS, Stata, R Objects, json, geojson, Apache Arrow, Feather and more.\r\nWorking with R-specific Formats\r\nR provides two file formats for storing data: .RDS and .RData. RDS files, formerly known as RDA files which one should avoid nowadays, can store a only single R object while RData files can store multiple R objects. Another important difference is that RDS files need to be assigned while Rdata files are loaded and use the object name that has been picked by the person saving the file (for good or worse, as it may overwrite existing objects with the same name in your environment).\r\nYou can open a RDS file with readRDS() or readr::read_rds():\r\n\r\n\r\ndat <- readRDS(\"data/file.Rds\")\r\ndat <- readr::read_rds(\"data/file.Rds\")\r\n\r\n\r\nOpening RData files is even easier, simply run the function load() with the file:\r\n\r\n\r\nload(\"data/file.RData\")\r\n\r\n\r\nData Export\r\nAll the packages mentioned (except for {readxl}) also offer functions to write your objects to disk.\r\n\r\n\r\nutils::write.csv(dat, file = \"data/file.csv\")\r\nvroom::vroom_write(dat, file = \"data/file.csv\")\r\nreadr::write_csv(dat, file = \"data/file.csv\")\r\ndata.table::fwrite(dat, file = \"data/file.csv\")\r\nrio::export(dat, file = \"data/file.csv\")\r\nsave(dat, file = \"data/file.Rdata\")\r\nsaveRDS(dat, file = \"data/file.Rds\")\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2024-02-20T11:07:58+01:00",
    "input_file": "data-import.knit.md"
  },
  {
    "path": "posts/d6-ggplot-theme/",
    "title": "Our Corporate D6 ggplot2 Theme",
    "description": "Learn how to use the theme_d6() featured in our {d6} package to create figures that match our lab identity. The theme function comes with a set of helpful additional options to simplify customization of our corporate D6 theme while ensuring consistency.",
    "author": [
      {
        "name": "Cedric Scherer",
        "url": "https://cedricscherer.com"
      }
    ],
    "date": "2023-11-30",
    "categories": [
      "rstats",
      "ggplot2",
      "workflow",
      "tutorial"
    ],
    "contents": "\r\n\r\nContents\r\nTypeface Choices\r\nLegend Position\r\nControl Grid\r\nModify Background Color\r\nSet Plot Margin\r\n\r\nRecently, I added a custom theme for {ggplot2} to our {d6} workflow package which makes it easy to create plots in a style that matches our lab identity used for our webpage, posters, and publications. Also, it hopefully simplifies your workflow by a ready-to-use theme with additional options that allow you to adjust the style of your ggplots.\r\n\r\nLet us know if there is a theme feature you often modify via the theme() function! We can add it as an argument to the complete theme function.\r\n\r\n\r\n\r\n## install (if needed) and load packages\r\nd6::simple_load(c(\"ggplot2\", \"EcoDynIZW/d6\"))\r\n\r\n\r\nThe default theme features a vertical and horizontal grid, our corporate sans font, and a transparent background. It also uses a larger default base_size of 14 pts that controls the size of text labels as well as line elements and borders (the ggplot2 default is 11 pts).\r\n\r\n\r\n## prepare data\r\ndata <- dplyr::filter(economics_long, !variable %in% c(\"pop\", \"pce\"))\r\n\r\n## create base plot\r\ng <- \r\n  ggplot(data, aes(x = date, y = value01, color = variable)) +\r\n  geom_line()\r\n\r\n## apply corporate theme\r\ng + d6::theme_d6()\r\n\r\n\r\n\r\nAs usual with complete themes, you can overwrite the base settings, namely base_family, base_size, base_size_line, and base_size_rect:\r\n\r\n\r\n## modify theme base setting\r\ng + d6::theme_d6(base_size = 18, base_rect_size = 2)\r\n\r\n\r\n\r\nTypeface Choices\r\nMain Typeface\r\nThe default typeface used is PT Sans. If you prefer a serif typeface, you can switch to PT Serif by setting serif = TRUE.\r\n\r\n\r\ng + d6::theme_d6(serif = FALSE) ## uses PT Sans (default)\r\ng + d6::theme_d6(serif = TRUE)  ## uses PT Serif\r\n\r\n\r\n\r\n\r\n\r\nNote that the typefaces need to be installed on your machine. A warning will inform you if the relevant fonts are not installed.\r\nPlease use the {systemfonts} package to access font files as this is the most recent and best implementation to use non-default fonts in combination with {ggplot2}. Loading other font packages, especially the {showtext} package, likely cause problems.\r\nTabular Font Option\r\nFor numeric axis labels and legends, you might want to use a tabular typeface (i.e. one where the characters all have the same width). You can set the axis and legend text individually by combining x, y, and l (uppercase letters work as well).\r\n\r\n\r\n## all combinations of upper- and lowercase work\r\ng + d6::theme_d6(mono = \"xyl\")\r\ng + d6::theme_d6(mono = \"xy\")\r\ng + d6::theme_d6(mono = \"y\")\r\n\r\n\r\n\r\n\r\n\r\nLegend Position\r\nBy default, the legend is placed at the bottom. You can easily change the position via the legend argument. This is a shortcut to theme(legend.position) and thus you can pass either strings specifying the position or a vector defining the x and y position as usual:\r\n\r\n\r\ng + d6::theme_d6(legend = \"right\")\r\ng + d6::theme_d6(legend = \"none\")\r\ng + d6::theme_d6(legend = c(.65, .8))\r\n\r\n\r\n\r\n\r\n\r\nControl Grid\r\nThe default themes comes with no grid lines but you can easily add them by specifying “x” for vertical, “y” for horizontal, or “xy” for both, vertical and horizontal grid lines (uppercase letters work as well).\r\nAll theme styles do not feature minor grid lines to avoid cluttering and distractions.\r\n\r\n\r\ng + d6::theme_d6(grid = \"none\") ## \"\" works as well (default)\r\ng + d6::theme_d6(grid = \"xy\") ## \"XY\" works as well, and even \"xY\" or \"Xy\"\r\ng + d6::theme_d6(grid = \"x\") ## \"x\" works as well\r\ng + d6::theme_d6(grid = \"y\") ## \"Y\" works as well\r\n\r\n\r\n\r\n\r\n\r\nModify Background Color\r\nBy default the background color of all boxes (plot, panel, and legends) is transparent. You can adjust the colors by passing any color name or hex code to the bg argument:\r\n\r\n\r\ng + d6::theme_d6(bg = \"orange\")\r\ng + d6::theme_d6(bg = \"grey95\")\r\ng + d6::theme_d6(bg = \"beige\")\r\n\r\n\r\n\r\n\r\n\r\nIt is not possible to control individual boxes via theme_d6(). If you wish to, for example, color the background of the panel area in a different, you have to specify that via the theme() function as usual:\r\n\r\n\r\ng + d6::theme_d6(bg = \"orange\") + \r\n  theme(panel.background = element_rect(fill = \"white\", color = \"transparent\"))\r\n\r\n\r\n\r\nSet Plot Margin\r\nDepending on the use case, you might want to adjust the margin around the plot. The default is related to the base_size and the same for all sides (top, right, bottom, left) specified as rep(base_size / 2, 4). You can modify the margins easily within the theme_d6().\r\n\r\n\r\ng + d6::theme_d6(margin = rep(0, 4), bg = \"grey95\")           ## no margins\r\ng + d6::theme_d6(margin = rep(50, 4), bg = \"grey95\")          ## same margins\r\ng + d6::theme_d6(margin = c(120, 40, 40, 10), bg = \"grey95\")  ## different margins\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/d6-ggplot-theme/d6-ggplot-theme_files/figure-html5/theme-d6-default-1.png",
    "last_modified": "2023-12-21T08:22:03+01:00",
    "input_file": {},
    "preview_width": 1820,
    "preview_height": 1118
  },
  {
    "path": "posts/rastercategories/",
    "title": "Categories in Raster Data",
    "description": "Learn how to work with categorial raster data and how to transform the rastervalues into a numerical values using terra package.",
    "author": [
      {
        "name": "Aimara Planillo & Moritz Wenzler-Meya",
        "url": {}
      }
    ],
    "date": "2023-11-15",
    "categories": [
      "terra",
      "raster",
      "spatial",
      "tutorial"
    ],
    "contents": "\r\n\r\nContents\r\nLoad {terra} package\r\nCreate example data\r\nAdd categorical column\r\nAdd numerical column\r\nMake the raster numerical\r\nCompare spatRasters\r\nCompare values\r\n\r\nCategorical rasters, such as land cover classes, can be tricky to deal with in R.\r\nImagine you get a raster (in {terra} the object is called spatRaster) with categories.\r\nIn {terra}, those categories are stored as labels. Additionally, a raster layer can have multiple labelling columns, in a way that we can activate the column with the information we want at the moment.\r\nFurthermore, we might want to operate with our rasters, for which we may need numerical values.\r\nFirst, we are going to see how to change or activate the labels to show in the categorical rasters.\r\nThen, we are going to see how to effectively transform this into numerical values.\r\nThe {Terra} changed the way how to do this compared to the {raster} package.\r\nHere are some tricks.\r\nLoad {terra} package\r\n\r\n\r\nlibrary(terra)\r\n\r\n\r\nCreate example data\r\nWe create a categorical raster as an example for our code\r\n\r\n\r\n# a simple spatRaster with 3 categories\r\nras <- rast(matrix(rep(c(\"forest\", \"farm\", \"urban\"), each = 3), nrow = 3, ncol = 3))\r\n\r\nras\r\n\r\nclass       : SpatRaster \r\ndimensions  : 3, 3, 1  (nrow, ncol, nlyr)\r\nresolution  : 1, 1  (x, y)\r\nextent      : 0, 3, 0, 3  (xmin, xmax, ymin, ymax)\r\ncoord. ref. :  \r\nsource(s)   : memory\r\ncategories  : label \r\nname        : lyr.1 \r\nmin value   :  farm \r\nmax value   : urban \r\n\r\nplot(ras)\r\n\r\n\r\n\r\nOur new raster has the categories we gave it as the labels in the plot.\r\nHowever, internally this categories as associated with “Values”. To see how the internal “Values” relate to the label, we can call the table of categories for the raster\r\nvalue\r\n\r\n\r\n# this shows the categories and the numeric internal representative numeric value it automatically gets.\r\ncats(ras)\r\n\r\n[[1]]\r\n  value  label\r\n1     1   farm\r\n2     2 forest\r\n3     3  urban\r\n\r\nThis is something like a “master table” that tell the raster what to show.\r\nNow that we have access to it, we can transform it and add information as needed, for example reclassifying the classes of the data, and relink this information back to the raster object.\r\nAdd categorical column\r\nlets say you want to reclassify the data, you can add another column by using this:\r\n\r\n\r\nrecl_df <- cbind(as.data.frame(cats(ras)),\r\n                 data.frame(new_label = c(\"d\", \"f\", \"e\"))) # I shuffled the characters to show the difference in the end\r\n\r\n\r\n# the function `categories` links the new table we created to the raster and \"activates\" the column we want to use. \r\n# In this case the new added column called new_label. The column order is counted as the position of the column AFTER the Value column\r\nras_new_cat <- categories(ras, \r\n                          layer = 1, \r\n                          value = recl_df, \r\n                          active = 2) # column 2 (do not count the value column! It has to be numeric)\r\n\r\n# now our raster has two entries for the categories. We can select which one to show.\r\nras_new_cat\r\n\r\nclass       : SpatRaster \r\ndimensions  : 3, 3, 1  (nrow, ncol, nlyr)\r\nresolution  : 1, 1  (x, y)\r\nextent      : 0, 3, 0, 3  (xmin, xmax, ymin, ymax)\r\ncoord. ref. :  \r\nsource(s)   : memory\r\ncategories  : label, new_label \r\nname        : new_label \r\nmin value   :         d \r\nmax value   :         e \r\n\r\n\r\n\r\nplot(ras_new_cat)\r\n\r\n\r\n\r\nIf we want to go to the old labels, we just need to activate the old labels by indicating the column they are stored in {activeCat} function\r\n\r\n\r\nactiveCat(ras_new_cat) <- 1\r\nplot(ras_new_cat)\r\n\r\n\r\n\r\nAdd numerical column\r\nLet’s say we want to reclassify the spatRaster with a numeric value, we have to take one more step\r\nFirst, we add a column with the numerical value we want to our categorical raster “master table”\r\n\r\n\r\nrecl_df_num <- cbind(as.data.frame(cats(ras_new_cat)),\r\n                                   new_value = c(5, 4, 6))\r\n\r\n# In this case the fourth (numeric) column will be activated\r\n\r\nras_new_cat_2 <- categories(ras, \r\n                          layer = 1, \r\n                          value = recl_df_num, \r\n                          active = 3) # column 4 (do not count the value column! It has to be numeric)\r\n\r\nplot(ras_new_cat_2)\r\n\r\n\r\n\r\nHere we see that the plot show us the numbers we just included, but the raster still reads then as categories\r\n\r\n\r\nras_new_cat_2\r\n\r\nclass       : SpatRaster \r\ndimensions  : 3, 3, 1  (nrow, ncol, nlyr)\r\nresolution  : 1, 1  (x, y)\r\nextent      : 0, 3, 0, 3  (xmin, xmax, ymin, ymax)\r\ncoord. ref. :  \r\nsource(s)   : memory\r\ncategories  : label, new_label, new_value \r\nname        : new_value \r\nmin value   :         5 \r\nmax value   :         6 \r\n\r\n# if we use a numerical call, like values for hte cells, we obtain an output that does not correspond with our new numbers\r\nvalues(ras_new_cat_2)\r\n\r\n      new_value\r\n [1,]         2\r\n [2,]         1\r\n [3,]         3\r\n [4,]         2\r\n [5,]         1\r\n [6,]         3\r\n [7,]         2\r\n [8,]         1\r\n [9,]         3\r\n\r\nMake the raster numerical\r\nFor this we need the function called {catalyze} and the column in the “master table” that actually has the values we want to use as numeric.\r\nWe specify the column using the index parameter.\r\n\r\n\r\n# this function activates our desired column of the spatRaster\r\nras_new_num <- catalyze(ras_new_cat_2, index = 3) # column 3 for new_value\r\n\r\n# we select only the correct numerical column for the new raster\r\nras_new_num <- ras_new_num$new_value\r\n\r\n# only the numerical column is left\r\nras_new_num\r\n\r\nclass       : SpatRaster \r\ndimensions  : 3, 3, 1  (nrow, ncol, nlyr)\r\nresolution  : 1, 1  (x, y)\r\nextent      : 0, 3, 0, 3  (xmin, xmax, ymin, ymax)\r\ncoord. ref. :  \r\nsource(s)   : memory\r\nname        : new_value \r\nmin value   :         4 \r\nmax value   :         6 \r\n\r\nplot(ras_new_num, type = \"continuous\") # with 'type' we decide to make numerical the legend\r\n\r\n\r\n\r\nCompare spatRasters\r\nLet’s compare the different spatRasters to see the differences\r\n\r\n\r\n\r\nAs you can see in the upper right plot that the categories got the internal count value from 1 to 3 instead of the given values.\r\nCompare values\r\nAnother way of checking if the correct numerical column is used is to show the categories in the spatRaster.\r\n\r\n\r\ncats(ras_new_cat_2)\r\n\r\n[[1]]\r\n  value  label new_label new_value\r\n1     1   farm         d         5\r\n2     2 forest         f         4\r\n3     3  urban         e         6\r\n\r\ncats(ras_new_num)\r\n\r\n[[1]]\r\nNULL\r\n\r\nThe numeric spatRaster does not have any categories left, as expected.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2023-11-16T14:14:44+01:00",
    "input_file": {}
  },
  {
    "path": "posts/scientificwritingtips/",
    "title": "Scientific Writing Tips",
    "description": "Find here some guidelines how to write and structure a scientific introduction, a scientific discussion or prepare a response letter for your revisions. You will find three pdf documents, one for each section.",
    "author": [
      {
        "name": "Aimara Planillo",
        "url": {}
      }
    ],
    "date": "2023-10-11",
    "categories": [
      "tutorial",
      "writing",
      "workflow"
    ],
    "contents": "\r\nIntroduction\r\nlink\r\nDiscussion\r\nlink\r\nResponse Letter\r\nlink\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2023-10-19T12:55:42+02:00",
    "input_file": {}
  },
  {
    "path": "posts/clippingmaskberlin/",
    "title": "Clipping a Raster to a Specific Area",
    "description": "Learn how to use a clipping mask to extract a portion of a raster, based on a template extent. In this example we show you how to clip a raster map to the extent and borders of Berlin boundaries.",
    "author": [
      {
        "name": "Moritz Wenzler-Meya",
        "url": {}
      }
    ],
    "date": "2023-06-26",
    "categories": [
      "tutorial",
      "spatial",
      "Berlin"
    ],
    "contents": "\r\n\r\nContents\r\nExample PopDynCloud\r\nExample geoboundaries\r\n\r\nSometimes you will get a dataset that is larger than your study area and you want to clip it to your specific extent or boundaries. There are two ways to do that:\r\nthe crop function of the {terra} package: this will crop the dataset to the extent of the cropping mask\r\nthe mask function of the {terra} package: this will crop the dataset to the extent of the cropping mask and set everything outside of the mask boundaries to NA (or to a custom set value)\r\nIn this tutorial only the mask function is covered because the crop function is straightforward to use. The mask function gives the opportunity to only get the raster cells that are covered by another raster or spatial object.\r\nHere are two examples showing how to use data from the PopDynCloud and one with data from the geoboundaries website/package:\r\n\r\n\r\nlibrary(d6geodata)\r\nlibrary(sf)\r\nlibrary(terra)\r\nlibrary(dplyr)\r\n\r\n\r\nExample PopDynCloud\r\nIf you have access to the PopDynCloud you can use the districts_berlin_layer like this:\r\n\r\n\r\nberlin_mask <- get_geodata(data_name =  \"districs_berlin_2022_poly_03035_gpkg\",\r\n                           path_to_cloud = \"E:/PopDynCloud\") # get_geodata function from the d6geodata package\r\n\r\nReading layer `districs_berlin_2022_poly_03035' from data source \r\n  `E:\\PopDynCloud\\GeoData\\data-raw\\berlin\\districs_berlin_2022_poly_03035_gpkg\\districs_berlin_2022_poly_03035.gpkg' \r\n  using driver `GPKG'\r\nSimple feature collection with 97 features and 6 fields\r\nGeometry type: MULTIPOLYGON\r\nDimension:     XY\r\nBounding box:  xmin: 4531043 ymin: 3253864 xmax: 4576654 ymax: 3290795\r\nProjected CRS: ETRS89-extended / LAEA Europe\r\n\r\nrast_example <- get_geodata(data_name =  \"tree-cover-density_berlin_2018_10m_03035_tif\",\r\n                           path_to_cloud = \"E:/PopDynCloud\")\r\n\r\n\r\nplot_quantitative_map(tif = rast_example) # plot not masked layer\r\n\r\n\r\nrast_example_masked <- mask(rast_example, # input raster \r\n                            berlin_mask) # mask to be clipped on \r\n\r\nplot_quantitative_map(tif = rast_example_masked) # plot masked layer\r\n\r\n\r\n\r\nExample geoboundaries\r\nIf not, you can use the data from the geoboundaries website or using the rgeoboundaries package from github:\r\n\r\n\r\nremotes::install_github(\"dickoa/rgeoboundaries\")\r\n\r\n\r\n\r\n\r\nlibrary(rgeoboundaries)\r\nrgeob_mask_berlin <- rgeoboundaries::gb_adm2(\"Germany\") %>% # set Country name(s)\r\n  filter(shapeName %in% \"Berlin\") %>% # filter for Berlin\r\n  st_transform(3035) # reproject to 3035 (or desired crs) \r\n\r\nrast_example_rgeob_masked <- mask(rast_example, # input raster \r\n                                  rgeob_mask_berlin) # mask to be clipped on\r\n\r\nplot_quantitative_map(tif = rast_example_rgeob_masked) # plot with function from d6geodata package\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/clippingmaskberlin/clippingmaskberlin_files/figure-html5/example-1.png",
    "last_modified": "2023-10-19T12:55:42+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/howtopost/",
    "title": "How to Make a Blogpost: a Brief Introduction",
    "description": "Learn how to contribute to the wiki section of the Ecological Dynamics Department webpage. If you think you have an important information, script, package or just a piece of code that might be interesting for your colleagues, consider to turn it into a blogpost for our webpage.",
    "author": [
      {
        "name": "Moritz Wenzler-Meya",
        "url": {}
      },
      {
        "name": "Cédric Scherer",
        "url": {}
      }
    ],
    "date": "2023-06-26",
    "categories": [
      "tutorial",
      "workflow"
    ],
    "contents": "\r\n\r\nContents\r\nCreate a Blogpost\r\nStep 0 — Install the {d6} Package\r\nStep 1 — Use the D6 Blogpost Template\r\nStep 2 — Fill the YAML Header of the Script\r\nStep 3 — Add the Content\r\nStep 4 — Render the Blogpost\r\nStep 5 — Review Process\r\nStep 6 — Published!\r\n\r\nGeneral Tips for Posting\r\n\r\nYou have a nice piece of code? You have developed a cool package? You have something to share within (or even outside) our department?\r\nYou can share it with a blogpost on our EcoDynIZW Website by following these simple steps!\r\nCreate a Blogpost\r\nStep 0 — Install the {d6} Package\r\nIf not yet installed, install the {d6} package. It provides several functions for our department along with Rmarkdown templates, including the D6 blogpost template.\r\n\r\n\r\n# install.packages(remotes)\r\nremotes::install_github(\"EcoDynIZW/d6\")\r\n\r\n\r\nStep 1 — Use the D6 Blogpost Template\r\nIn RStudio, navigate to File > New File... > R Markdown... .\r\nIn the From Template section, choose the D6 Blogpost Template from the list.\r\n\r\n\r\nIf the template is not listed, please make sure that the latest version of {d6} is installed.\r\nStep 2 — Fill the YAML Header of the Script\r\nFill in a proper name, please use uppercase for the name.\r\nWrite a short description. Please have in mind that this description will appear on the blogpost listing page as well as in the post. Usually, we start these descriptions with “Learn how to learn x to do y.”.\r\nAdd some categories that relate to your article. For examples, browse through other posts featured in the wiki section of our web page.\r\nEnter your name and the date of the post in the given format.\r\nStep 3 — Add the Content\r\nDescribe briefly what you will show us and describe each chunk separately.\r\nLeave a line of space between text and chunk for separating text and chunk output.\r\nPlease use the chunk options to name chunks, hide them or ignore them in the knitting process. This will help later to find possible errors.\r\nStep 4 — Render the Blogpost\r\nKnit the post and check if the knitted document looks as desired.\r\nStep 5 — Review Process\r\nSend the Rmd file to the data manager for review.\r\nWait for feedback by the data manager. If changes are requested, update the article accordingly and send the corrected script.\r\nStep 6 — Published!\r\nThe data manager will publish your post as soon as possible on the website.\r\nGeneral Tips for Posting\r\nPlease use the spell check before knitting and pushing the post.\r\nPlease check if the code is running (in a reasonable time) as we have to rebuild the page from time to time.\r\nUse styling like `plot()` for function names in the text and `{pckg}` package names.\r\nPick examples that are simple enough in terms of file size and performance so that they can be easily reproduced within the script.\r\nMake sure that your code can be run by anybody reading the post. If you need data inputs, generate made-up data examples, use data sets from packages, or link to the data source (make sure it is publicly available as readers are not necessarily part of the department!)\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2023-10-19T12:55:42+02:00",
    "input_file": {}
  },
  {
    "path": "posts/ggplot-workflow/",
    "title": "ggplot2 Workflow Tips",
    "description": "Learn how to efficiently use the powerful graphics library {ggplot2} by avoiding repeated code and manual adjustments. You will learn how to define your themes globally and how to use the {patchwork} package to create multi-panel plots with automated numbering and combined legends.",
    "author": [
      {
        "name": "Cedric Scherer",
        "url": "https://cedricscherer.com"
      }
    ],
    "date": "2023-06-22",
    "categories": [
      "rstats",
      "ggplot2",
      "workflow",
      "tutorial"
    ],
    "contents": "\r\n\r\nContents\r\nSetup\r\nTheming\r\nSet Themes Globally\r\nAdjust Theme Base Settings\r\nUpdate Theme Elements\r\nCustom Local Modifications\r\nSummary\r\n\r\nMulti-Panel Figures\r\nAdjust Layout\r\nAdd White Space\r\nNested Layouts\r\nMerge Legends\r\nAutomate Plot Tags\r\nInset Plots\r\nSummary\r\n\r\n\r\nSetup\r\nLet’s load the {ggplot2} library and create two basic ggplots, stored as g1 (scatter plot) and g2 (box-and-whisker plot) that can be used later.\r\n\r\n\r\nlibrary(ggplot2)\r\n\r\n\r\n\r\n\r\ng1 <- ggplot(mpg, aes(x = displ, y = hwy)) +\r\n  geom_point(aes(color = class))\r\n\r\ng1\r\n\r\n\r\ng2 <- ggplot(mpg, aes(x = class, y = hwy)) +\r\n  geom_boxplot()\r\n\r\ng2\r\n\r\n\r\n\r\n\r\n\r\nTheming\r\nThe resulting plots use the default gray theme: theme_gray() or theme_grey().\r\nWe can change the default theme by adding a complete theme, starting with theme_*(), and/or customizing single elements of the default theme via theme():\r\n\r\n\r\ng1 + \r\n  ## apply light complete theme\r\n  theme_light() + \r\n  ## remove minor grid + modify typeface\r\n  theme(panel.grid.minor = element_blank(),\r\n        text = element_text(family = \"PT Sans\"))\r\n\r\n\r\ng2 + \r\n  theme_light() + \r\n  theme(panel.grid.minor = element_blank(), \r\n        text = element_text(family = \"PT Sans\"))\r\n\r\n\r\n\r\n\r\n\r\nThis procedure involves a lot of copy-and-paste’ing, which makes it a tedious procedure especially in case you decide to make some general styling changes at a later point. It is also prone to mistakes as you might forget to set specific adjustments for single plots.\r\nCheck also our D6 corporate theme which is part of our {d6} R package and the respective blog post! The theme comes with a larger base size and several additional arguments to simplify customization with regard to typefaces, grid lines, margins and more.\r\n\r\n\r\ng1 + d6::theme_d6()\r\n\r\n\r\ng2 + d6::theme_d6(grid = \"y\", serif = TRUE)\r\n\r\n\r\n\r\n\r\n\r\nSet Themes Globally\r\nInstead of repeating the same code to change the appearance of your plots, it is more efficient and beneficial to overwrite the default global theme:\r\n\r\n\r\ntheme_set(theme_light())\r\n\r\n\r\nAfter setting the new theme, all plots created within the same environment are styled accordingly:\r\n\r\n\r\ng1\r\n\r\n\r\ng2\r\n\r\n\r\n\r\n\r\n\r\nAdjust Theme Base Settings\r\nComplete themes allow for some general modifications, no matter if added locally to your plot or if set globally. The setting include the typeface used for all text elements (base_family), the general base size (base_size) as well as dedicated relative sizes for line elements (base_line_size) and rect elements (base_rect_size).\r\n\r\n\r\ng1 + \r\n  theme_light(\r\n    base_family = \"PT Serif\", ## default: depends on OS\r\n    base_size = 18,           ## default: 11\r\n    base_line_size = 3,       ## default: base_size/22 -> 0.5\r\n    base_rect_size = 10       ## default: base_size/22 -> 0.5\r\n  )\r\n\r\n\r\n\r\nKnowing of this feature, we can already adjust the general size (which tends to be too small by default) as well as the typeface of our custom global theme:\r\n\r\n\r\ntheme_set(theme_light(base_size = 15, base_family = \"PT Sans\")) \r\n\r\n\r\n… which is then used for all following plots:\r\n\r\n\r\ng1\r\n\r\n\r\ng2\r\n\r\n\r\n\r\n\r\n\r\nUpdate Theme Elements\r\nComplete themes are great but in most circumstances we likely want to adjust a few things. Usually, you do that by adding the theme() function to your ggplot (as shown in the beginning). However, similarly to theme_set() we can apply the modifications globally:\r\n\r\n\r\ntheme_update(\r\n  panel.grid.minor = element_blank(),\r\n  axis.title = element_text(face = \"bold\"),\r\n  legend.title = element_text(face = \"bold\")\r\n)\r\n\r\n\r\n\r\n\r\ng1\r\n\r\n\r\ng2\r\n\r\n\r\n\r\n\r\n\r\nCustom Local Modifications\r\nOf course, you can still either overwrite the global theme as before or modify specific elements for a single plot if needed:\r\n\r\n\r\ng1 + \r\n  ## new complete theme\r\n  theme_classic(base_family = \"PT Serif\", base_size = 15) + \r\n  ## add grid lines\r\n  theme(panel.grid.major = element_line(color = \"grey90\"))\r\n\r\n\r\ng2 + \r\n  ## remove vertical grid lines + overwrite axis title styling\r\n  theme(panel.grid.major.x = element_blank(), \r\n           axis.title = element_text(color = \"red\", face = \"italic\")) \r\n\r\n\r\n\r\n\r\n\r\nSummary\r\nSetting and updating ggplot themes globally is efficient and avoids potential mistakes.\r\nAs a workflow routine, add a chunk that loads {ggplot2} and afterwards sets and updates your theme at the beginning of a script rather than adding the same code to each plot individually.\r\nMulti-Panel Figures\r\nWe often use multi-panel visualizations, i.e. several plots layed out in a single graphic. Instead of combining single figures manually, we make use of a coding-first approach.\r\nThere are many packages to combine ggplots such as {gridExtra}, {cowplot}, and {ggarrange}. The most recent, and IMHO the best in terms of functionality and simplicity, is the {patchwok} package by Thomas Lin Pedersen. For simple multi-panel graphics, mathematical operators can be used – easy to use and remember.\r\n\r\n\r\nlibrary(patchwork)\r\n\r\n\r\n\r\n\r\ng1 + g2\r\n\r\n\r\n\r\n\r\n\r\ng1 / g2\r\n\r\n\r\n\r\nAdjust Layout\r\nBy default, both plots take the same space. In case you want to adjust how the plots are laid out, use plot_layout() in combination with either widths or heights. These arguments take a vector with the relative width or height for each plot, respectively.\r\n\r\n\r\ng1 + g2 + \r\n  plot_layout(widths = c(.5, 1))\r\n\r\n\r\n\r\nAdd White Space\r\n{patchwok} comes with a placeholder to add empty space between plots. Once can add a plot_spacer() similar to a regular plot:\r\n\r\n\r\ng1 + plot_spacer() + g2 + \r\n  plot_layout(widths = c(.5, .1, 1))\r\n\r\n\r\n\r\nNested Layouts\r\nAlso more complex layouts can be created:\r\n\r\n\r\n\r\n\r\n\r\n(g1 + g3 + g4) / (g2 + g5)\r\n\r\n\r\n\r\nCode to create g3, g4, and g5\r\n\r\n\r\ng3 <- ggplot(mpg, aes(x = cyl, y = hwy)) +\r\n  geom_point(aes(color = class))\r\n\r\ng4 <- ggplot(mpg, aes(x = cty, y = hwy)) +\r\n  geom_point(aes(color = class))\r\n\r\ng5 <- ggplot(mpg, aes(x = class, y = hwy)) +\r\n  stat_summary(fun.data = \"mean_sdl\", fun.args = list(mult = 1))\r\n\r\n\r\n\r\n\r\n\r\nAlternatively, you can also create a design layout to have full control:\r\n\r\n\r\nlayout <- \r\n  \"\r\n  ABBCCC#\r\n  DDDD#EE\r\n  \"\r\n\r\ng1 + g3 + g4 + g2 + g5 + \r\n  plot_layout(design = layout)\r\n\r\n\r\n\r\nThe letters refer to the single plots (in the order you combine them later) and a hash # indicates empty space, similar to plot_spacer().\r\nMerge Legends\r\nDisplaying the legend three times makes no sense. {patchwork} offers the utility to “collect your guides” inside the plot_layout() function:\r\n\r\n\r\n(g1 + g3 + g4) / (g2 + g5) + \r\n  plot_layout(guides = \"collect\")\r\n\r\n\r\n\r\nNow we may want to move it to the top so it is shown next to the relevant colored scatter plots, not in the middle. We can adjust the theme for all plots inside another {patchwork} function called plot_annotation()—or by updating your global theme 😉\r\n\r\n\r\n((g1 + g3 + g4) / (g2 + g5)) + \r\n  plot_layout(guides = \"collect\") +\r\n  plot_annotation(theme = theme(legend.justification = \"top\"))\r\n\r\n\r\n\r\nAutomate Plot Tags\r\nWhen preparing such multi-panel figures for publications, we usually want to add tags to be able to refer to subplots in the figure caption or main text. Again, we can do this inside R instead of adding them afterwards by hand (which either takes very long or results in irregularly aligned labels).\r\n\r\n\r\n((g1 + g3 + g4) / (g2 + g5)) + \r\n  plot_layout(guides = \"collect\") +\r\n  plot_annotation(tag_levels = \"A\", \r\n                  theme = theme(legend.justification = \"top\"))\r\n\r\n\r\n\r\n{patchwork} understands a range of numbering formats such as a for lowercase letters, 1 for numbers, or i and I for lowercase and uppercase Roman numerals, respectively. Furthermore we can style the tag by defining a pre- and/or suffix:\r\n\r\n\r\n((g1 + g3 + g4) / (g2 + g5)) + \r\n  plot_layout(guides = \"collect\") +\r\n  plot_annotation(tag_levels = \"i\", tag_prefix = \"(\", tag_suffix = \")\",\r\n                  theme = theme(legend.justification = \"top\"))\r\n\r\n\r\n\r\nInset Plots\r\nSimilar to other arrangement packages, we can use {patchwork} also to add inset plots. Inside the inside_element() function, we specify the plot to draw and then the outer bounds (left, bottom, right top).\r\n\r\n\r\ng4 + inset_element(g1 + guides(color = \"none\"), .5, 0, 1, .5)\r\n\r\n\r\n\r\nBy default, the inset plot is aligned with the panel of the main plot. If you want to modify the behavior, overwrite the default input of align_to.\r\n\r\n\r\ng4 + inset_element(g1 + guides(color = \"none\"), .5, 0, 1, .5, align_to = \"plot\")\r\ng4 + inset_element(g1 + guides(color = \"none\"), .5, 0, 1, .5, align_to = \"full\")\r\n\r\n\r\n\r\n\r\n\r\nSummary\r\n{patchwork} offers some great functionality to create basic and pretty complex layouts, add inset plots, merge repeated legends, and automate tag numbering. This makes it a powerful tool as you do not need to adjust tag labels, legends, and more for the individual ggplot.\r\n\r\n\r\n\r\n",
    "preview": "posts/ggplot-workflow/ggplot-workflow_files/figure-html5/basic-ggplots-plots-1.png",
    "last_modified": "2023-12-21T08:22:03+01:00",
    "input_file": {},
    "preview_width": 3640,
    "preview_height": 1118
  },
  {
    "path": "posts/netlogoturtlespatialprojection/",
    "title": "Netlogo Turtle Spatial Projection",
    "description": "Learn how to transform the relative coordinates of the individuals from Netlogo into coordinates from real maps. This code is especially designed for spatially explicit netlogo models that were set to store the individual coordinates (xcor, ycor) in the output for the turtle data.",
    "author": [
      {
        "name": "Aimara Planillo",
        "url": {}
      }
    ],
    "date": "2023-06-15",
    "categories": [
      "NetLogo",
      "spatial",
      "tutorial"
    ],
    "contents": "\r\n\r\nContents\r\nHow to project Netlogo Turtle coordinates into a real map\r\n0. Load libraries\r\n1. Create data\r\n2. Get reference coordinates\r\n3. Transform turtle coordinates into map coordinates\r\n4. Quick plot with tmap\r\n5. Plot with ggplot2\r\n\r\n\r\nHow to project Netlogo Turtle coordinates into a real map\r\nWhen using spatial data in Netlogo, the coordinates of a raster get transformed to relative coordinates. This means, the cell in the bottom left gets coordinate (1,1), the one on top of it is (1,2), and so on.\r\nAfter running a model, usually we want to reproject the output back to the spatial data coordinates used, either for post-simulation analyses or for plotting.\r\nThis code shows how to project the turtles’ coordinates back into a map, when a raster was\r\nused to create the Netlogo landscape.\r\nFor this we need:\r\n- The raster used as netlogo input\r\n- The turtle coordinates in the output\r\n0. Load libraries\r\n\r\n\r\nlibrary(terra)\r\nlibrary(dplyr)\r\nlibrary(sf)\r\nlibrary(tmap)\r\nlibrary(ggplot2)\r\nlibrary(ggspatial)\r\n\r\n\r\n1. Create data\r\nIn this example we create a raster and some turtle data to use.\r\nWith real data, you will load your raster here and make sure it has a PROJECTED coordinate system.\r\nTurtle data will have different formats depending how it was created, the basic data we need for this is the identity of the turtle and the coordinates.\r\n\r\n\r\n## Create raster with 100 cells for the example\r\nmyraster <- rast(nrows = 100, ncols = 100, \r\n                 xmin = 4541100, xmax = 4542100, \r\n                 ymin = 3265800, ymax = 3266800)\r\n## give random values to the raster\r\nmyraster <- init(myraster, sample(1:1000))\r\n\r\n## assign projection\r\ncrs(myraster) <- \"epsg:3035\"\r\nplot(myraster)\r\n\r\n\r\n## turtle data - example data\r\nwho <- seq(1,10)\r\nxcoord <- sample(1:100, 10) # create random integers for x coordinate\r\nycoord <- sample(1:100, 10) # create random integers for y coordinate\r\n\r\nturtle_variables <- cbind.data.frame(who, xcoord, ycoord)\r\nhead(turtle_variables)\r\n\r\n  who xcoord ycoord\r\n1   1      8     23\r\n2   2     51     97\r\n3   3     87     74\r\n4   4     11     24\r\n5   5     69     26\r\n6   6     68     81\r\n\r\nNow that we have our data, let’s extract the map coordinates as reference and transform the turtle ones. This process will work with any PROJECTED coordinate system.\r\n2. Get reference coordinates\r\nWe need the bottom left corner of the map as a reference point and the resolution of the map\r\n\r\n\r\n# we are going to trasnform the cell relative numbering to real coordinates, starting left down as this is where netlogo starts numbering the cells\r\nstart_left <- xmin(myraster)\r\nstart_down <- ymin(myraster)\r\nmy_res <- res(myraster)[1]\r\n\r\n\r\n3. Transform turtle coordinates into map coordinates\r\nNow we use the reference point to transform our coordinates into the projected coordinates and the resolution to correct for the size of the cells\r\n\r\n\r\nturtle_spatial <- turtle_variables %>% \r\n  mutate(spatial_xcoord = start_left + ((xcoord * my_res) + my_res/2), #divided by 2 to locate in the center of the cell\r\n         spatial_ycoord = start_down + ((ycoord * my_res) + my_res/2))\r\n\r\n\r\n## make spatial points\r\nturtle_sf <- st_as_sf(turtle_spatial, \r\n                      coords = c(\"spatial_xcoord\", \"spatial_ycoord\"), \r\n                      crs = crs(myraster))\r\n\r\n\r\n4. Quick plot with tmap\r\n\r\n\r\n\r\n5. Plot with ggplot2\r\n\r\n        x       y  hs\r\n1 4541105 3266795 902\r\n2 4541115 3266795 698\r\n3 4541125 3266795 500\r\n4 4541135 3266795 492\r\n5 4541145 3266795 405\r\n6 4541155 3266795 151\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2023-10-19T12:55:42+02:00",
    "input_file": {}
  },
  {
    "path": "posts/r-spatial-data/",
    "title": "Spatial Data Sources in R",
    "description": "Learn how to use a suite of R packages to download various spatial data sources. This guide illustrates how to download data of countries, physical objects, and cultural properties as vector or raster objects that can be assessed from within R.",
    "author": [
      {
        "name": "Cedric Scherer",
        "url": "https://cedricscherer.com"
      }
    ],
    "date": "2023-05-25",
    "categories": [
      "tutorial",
      "rstats",
      "workflow",
      "spatial",
      "data sources"
    ],
    "contents": "\r\n\r\nContents\r\nPreparation\r\n{rnaturalearth}\r\nCountry Data\r\nPhysical Data Sources\r\nCultural Data Sources\r\nRelief Data\r\n\r\n{rgeoboundaries}\r\n{osmdata}\r\n{elevatr}\r\n\r\nPreparation\r\nTo visualize the data sets, we use the {ggplot2} package. We will also use the {sf} and the {terra} packages to work and plot spatial data–vector and raster, respectively–in R. Make sure all packages are installed when running the code snippets.\r\n\r\n\r\n# install.packages(\"ggplot2\")\r\n# install.packages(\"sf\")\r\n# install.packages(\"terra\")\r\n\r\nlibrary(ggplot2)\r\n\r\n### set \"empty\" theme with centered titles for ggplot output\r\ntheme_set(theme_void())\r\ntheme_update(plot.title = element_text(face = \"bold\", hjust = .5))\r\n\r\n\r\n\r\n{rnaturalearth}\r\nNaturalEarth is a public domain map data set that features vector and raster data of physical and cultural properties. It is available at 1:10m, 1:50m, and 1:110 million scales.\r\n{rnaturalearth} is an R package to hold and facilitate interaction with NaturalEarth map data via dedicated ne_* functions. After loading the package, you can for example quickly access shapefiles of all countries–the resulting spatial object contains vector data that is already projected and can be stored as either sp or sf format:\r\n\r\n\r\n## install development version of {rnaturalearth} as currently the \r\n## download doesn't work in the CRAN package version\r\n# install.packages(\"remotes\")\r\n# remotes::install_github(\"ropensci/rnaturalearth\")\r\n\r\n# if downloading raster data such as 'MRS_50m' is not working try to install the developer version of 'rnaturalearth' by using\r\n# remotes::install_dev(\"rnaturalearth\")\r\n\r\n## for high resolution data, also install {rnaturalearthhires}\r\n# remotes::install_github(\"ropensci/rnaturalearthhires\")\r\n\r\nlibrary(rnaturalearth)\r\n\r\n## store as sf object (simple features)\r\nworld <- ne_countries(returnclass = \"sf\")\r\nclass(world)\r\n\r\n[1] \"sf\"         \"data.frame\"\r\n\r\nsf::st_crs(world)[1]\r\n\r\n$input\r\n[1] \"WGS 84\"\r\n\r\nCountry Data\r\nThis country data set (which is actually not downloaded but stored locally by installing the package) already contains several useful variables, mostly referring to different naming conventions (helpful when joining with other data sets), to identify continents and regions, and also some information on population size, GDP, and economy:\r\n\r\n\r\nnames(world)\r\n\r\n  [1] \"featurecla\" \"scalerank\"  \"labelrank\"  \"sovereignt\" \"sov_a3\"    \r\n  [6] \"adm0_dif\"   \"level\"      \"type\"       \"tlc\"        \"admin\"     \r\n [11] \"adm0_a3\"    \"geou_dif\"   \"geounit\"    \"gu_a3\"      \"su_dif\"    \r\n [16] \"subunit\"    \"su_a3\"      \"brk_diff\"   \"name\"       \"name_long\" \r\n [21] \"brk_a3\"     \"brk_name\"   \"brk_group\"  \"abbrev\"     \"postal\"    \r\n [26] \"formal_en\"  \"formal_fr\"  \"name_ciawf\" \"note_adm0\"  \"note_brk\"  \r\n [31] \"name_sort\"  \"name_alt\"   \"mapcolor7\"  \"mapcolor8\"  \"mapcolor9\" \r\n [36] \"mapcolor13\" \"pop_est\"    \"pop_rank\"   \"pop_year\"   \"gdp_md\"    \r\n [41] \"gdp_year\"   \"economy\"    \"income_grp\" \"fips_10\"    \"iso_a2\"    \r\n [46] \"iso_a2_eh\"  \"iso_a3\"     \"iso_a3_eh\"  \"iso_n3\"     \"iso_n3_eh\" \r\n [51] \"un_a3\"      \"wb_a2\"      \"wb_a3\"      \"woe_id\"     \"woe_id_eh\" \r\n [56] \"woe_note\"   \"adm0_iso\"   \"adm0_diff\"  \"adm0_tlc\"   \"adm0_a3_us\"\r\n [61] \"adm0_a3_fr\" \"adm0_a3_ru\" \"adm0_a3_es\" \"adm0_a3_cn\" \"adm0_a3_tw\"\r\n [66] \"adm0_a3_in\" \"adm0_a3_np\" \"adm0_a3_pk\" \"adm0_a3_de\" \"adm0_a3_gb\"\r\n [71] \"adm0_a3_br\" \"adm0_a3_il\" \"adm0_a3_ps\" \"adm0_a3_sa\" \"adm0_a3_eg\"\r\n [76] \"adm0_a3_ma\" \"adm0_a3_pt\" \"adm0_a3_ar\" \"adm0_a3_jp\" \"adm0_a3_ko\"\r\n [81] \"adm0_a3_vn\" \"adm0_a3_tr\" \"adm0_a3_id\" \"adm0_a3_pl\" \"adm0_a3_gr\"\r\n [86] \"adm0_a3_it\" \"adm0_a3_nl\" \"adm0_a3_se\" \"adm0_a3_bd\" \"adm0_a3_ua\"\r\n [91] \"adm0_a3_un\" \"adm0_a3_wb\" \"continent\"  \"region_un\"  \"subregion\" \r\n [96] \"region_wb\"  \"name_len\"   \"long_len\"   \"abbrev_len\" \"tiny\"      \r\n[101] \"homepart\"   \"min_zoom\"   \"min_label\"  \"max_label\"  \"label_x\"   \r\n[106] \"label_y\"    \"ne_id\"      \"wikidataid\" \"name_ar\"    \"name_bn\"   \r\n[111] \"name_de\"    \"name_en\"    \"name_es\"    \"name_fa\"    \"name_fr\"   \r\n[116] \"name_el\"    \"name_he\"    \"name_hi\"    \"name_hu\"    \"name_id\"   \r\n[121] \"name_it\"    \"name_ja\"    \"name_ko\"    \"name_nl\"    \"name_pl\"   \r\n[126] \"name_pt\"    \"name_ru\"    \"name_sv\"    \"name_tr\"    \"name_uk\"   \r\n[131] \"name_ur\"    \"name_vi\"    \"name_zh\"    \"name_zht\"   \"fclass_iso\"\r\n[136] \"tlc_diff\"   \"fclass_tlc\" \"fclass_us\"  \"fclass_fr\"  \"fclass_ru\" \r\n[141] \"fclass_es\"  \"fclass_cn\"  \"fclass_tw\"  \"fclass_in\"  \"fclass_np\" \r\n[146] \"fclass_pk\"  \"fclass_de\"  \"fclass_gb\"  \"fclass_br\"  \"fclass_il\" \r\n[151] \"fclass_ps\"  \"fclass_sa\"  \"fclass_eg\"  \"fclass_ma\"  \"fclass_pt\" \r\n[156] \"fclass_ar\"  \"fclass_jp\"  \"fclass_ko\"  \"fclass_vn\"  \"fclass_tr\" \r\n[161] \"fclass_id\"  \"fclass_pl\"  \"fclass_gr\"  \"fclass_it\"  \"fclass_nl\" \r\n[166] \"fclass_se\"  \"fclass_bd\"  \"fclass_ua\"  \"geometry\"  \r\n\r\nWe can quickly plot it:\r\n\r\n\r\nggplot(world) + \r\n  geom_sf(aes(fill = economy)) + \r\n  coord_sf(crs = \"+proj=eqearth\")\r\n\r\n\r\n\r\nNOTE: Unfortunately, NaturalEarth is using weird de-facto and on-the-ground rules to define country borders which do not follow the borders the UN and most countries agree on. For correct and official borders, please use the {rgeoboundaries} package (see below).\r\nPhysical Data Sources\r\nYou can specify the scale, category, and type you want as in the examples below.\r\n\r\n\r\nglacier_small <- ne_download(type = \"glaciated_areas\", category = \"physical\", \r\n                             scale = \"small\", returnclass = \"sf\")\r\n\r\nglacier_large <- ne_download(type = \"glaciated_areas\", category = \"physical\", \r\n                             scale = \"large\", returnclass = \"sf\")\r\n\r\n\r\nNow we can compare the impact of different scales specified–there is a notable difference in detail here (and also in size of the object with 11 versus 1886 observations).\r\n\r\n\r\nggplot() + \r\n  geom_sf(data = world, color = \"grey85\", fill = \"grey85\") +\r\n  geom_sf(data = glacier_small, color = \"grey40\", fill = \"grey40\") + \r\n  coord_sf(crs = \"+proj=eqearth\")\r\n\r\nggplot() +\r\n  geom_sf(data = world, color = \"grey85\", fill = \"grey85\") +\r\n  geom_sf(data = glacier_large, color = \"grey40\", fill = \"grey40\") +\r\n  coord_sf(crs = \"+proj=eqearth\")\r\n\r\n\r\n\r\n\r\nlibrary(patchwork)\r\n\r\nsmall <- ggplot() + \r\n  geom_sf(data = world, color = \"grey85\", fill = \"grey85\", lwd = .05) +\r\n  geom_sf(data = glacier_small, color = \"grey40\", fill = \"grey40\") + \r\n  coord_sf(crs = \"+proj=eqearth\") +\r\n  labs(title = 'scale = \"small\"')\r\n\r\nlarge <- ggplot() +\r\n  geom_sf(data = world, color = \"grey85\", fill = \"grey85\", lwd = .05) +\r\n  geom_sf(data = glacier_large, color = \"grey40\", fill = \"grey40\") +\r\n  coord_sf(crs = \"+proj=eqearth\") + \r\n  labs(title = 'scale = \"large\"')\r\n\r\nsmall + large * theme(plot.margin = margin(0, -20, 0, -20))\r\n\r\n\r\n\r\nCultural Data Sources\r\nNaturalEarth also provides several cultural data sets, such as airports, roads, disputed areas. Let’s have a look at the urban areas across the world:\r\n\r\n\r\nurban <- ne_download(type = \"urban_areas\", category = \"cultural\", \r\n                     scale = \"medium\", returnclass = \"sf\")\r\n\r\nggplot() + \r\n  geom_sf(data = world, color = \"grey90\", fill = \"grey90\") +\r\n  geom_sf(data = urban, color = \"firebrick\", fill = \"firebrick\") + \r\n  coord_sf(crs = \"+proj=eqearth\")\r\n\r\n\r\n\r\nRelief Data\r\nThe physical and cultural data sets showcased above are all vector data. NaturalEarth also provides raster data, namely gridded relief data:\r\n\r\n\r\nrelief <- ne_download(type = \"MSR_50M\", category = \"raster\",\r\n                      scale = 50, returnclass = \"sf\")\r\n\r\nterra::plot(relief)\r\n\r\n\r\n\r\n\r\n{rgeoboundaries}\r\nThe {rgeoboundaries} package uses the Global Database of Political Administrative Boundaries that provide generally accepted political borders. The data are licensed openly.\r\n\r\n\r\n## install package from GitHub as it is not featured on CRAN yet\r\n# install.packages(\"remotes\")\r\n# remotes::install_github(\"wmgeolab/rgeoboundaries\")\r\n\r\nlibrary(rgeoboundaries)\r\n\r\ngb_adm0()\r\n\r\nSimple feature collection with 218 features and 3 fields\r\nGeometry type: MULTIPOLYGON\r\nDimension:     XY\r\nBounding box:  xmin: -180 ymin: -90 xmax: 180 ymax: 83.63339\r\nGeodetic CRS:  WGS 84\r\nFirst 10 features:\r\n   shapeGroup shapeType         shapeName\r\n1         AFG      ADM0       Afghanistan\r\n2         GBR      ADM0    United Kingdom\r\n3         ALB      ADM0           Albania\r\n4         DZA      ADM0           Algeria\r\n5         USA      ADM0     United States\r\n6         ATA      ADM0        Antarctica\r\n7         ATG      ADM0 Antigua & Barbuda\r\n8         ARG      ADM0         Argentina\r\n9         AND      ADM0           Andorra\r\n10        AGO      ADM0            Angola\r\n                         geometry\r\n1  MULTIPOLYGON (((74.88986 37...\r\n2  MULTIPOLYGON (((33.01302 34...\r\n3  MULTIPOLYGON (((20.07889 42...\r\n4  MULTIPOLYGON (((8.641941 36...\r\n5  MULTIPOLYGON (((-168.1579 -...\r\n6  MULTIPOLYGON (((-60.06171 -...\r\n7  MULTIPOLYGON (((-62.34839 1...\r\n8  MULTIPOLYGON (((-63.83417 -...\r\n9  MULTIPOLYGON (((1.725802 42...\r\n10 MULTIPOLYGON (((11.7163 -16...\r\n\r\nggplot(gb_adm0()) + \r\n  geom_sf(color = \"grey40\", lwd = .2) + \r\n  coord_sf(crs = \"+proj=eqearth\") \r\n\r\n\r\n\r\nLower administrative levels are available as well, e.g. in Germany adm1 represents federal states (“Bundesländer”), adm2 districts (“Kreise”) and so on.\r\nLet’s plot the admin 1 levels for the DACH countries:\r\n\r\n\r\ndach <- gb_adm1(c(\"germany\", \"switzerland\", \"austria\"), type = \"sscgs\")\r\n\r\nggplot(dach) +\r\n  geom_sf(aes(fill = shapeGroup)) +\r\n  scale_fill_brewer(palette = \"Set2\")\r\n\r\n\r\n\r\n{osmdata}\r\nOpenStreetMap (https://www.openstreetmap.org) is a collaborative project to create a free editable geographic database of the world. The geodata underlying the maps is considered the primary output of the project and is accessible from R via the {osmdata} package.\r\nWe first need to define our query and limit it to a region. You can explore the features and tags (also available as information via OpenStreetMap directly).\r\n\r\n\r\n## install package\r\n# install.packages(\"osmdata\")\r\n\r\nlibrary(osmdata)\r\n\r\n## explore features + tags\r\nhead(available_features())\r\n\r\n[1] \"4wd_only\"  \"abandoned\" \"abutters\"  \"access\"    \"addr\"     \r\n[6] \"addr:city\"\r\n\r\nhead(available_tags(\"craft\"))\r\n\r\n# A tibble: 6 × 2\r\n  Key   Value               \r\n  <chr> <chr>               \r\n1 craft agricultural_engines\r\n2 craft atelier             \r\n3 craft bag_repair          \r\n4 craft bakery              \r\n5 craft basket_maker        \r\n6 craft beekeeper           \r\n\r\n## building the query, e.g. beekeepers\r\nbeekeeper_query <- \r\n  ## you can automatically retrieve a boudning box (pr specify one manually)\r\n  getbb(\"Berlin\") %>%\r\n  ## build an Overpass query\r\n  opq(timeout = 999) %>%\r\n  ## access particular feature\r\n  add_osm_feature(\"craft\", \"beekeeper\")\r\n  \r\n## download data\r\nsf_beekeepers <- osmdata_sf(beekeeper_query)\r\n\r\n\r\nNow we can investigate beekeepers in Berlin:\r\n\r\n\r\nnames(sf_beekeepers)\r\n\r\n[1] \"bbox\"              \"overpass_call\"     \"meta\"             \r\n[4] \"osm_points\"        \"osm_lines\"         \"osm_polygons\"     \r\n[7] \"osm_multilines\"    \"osm_multipolygons\"\r\n\r\nhead(sf_beekeepers$osm_points)\r\n\r\nSimple feature collection with 6 features and 27 fields\r\nGeometry type: POINT\r\nDimension:     XY\r\nBounding box:  xmin: 13.24443 ymin: 52.35861 xmax: 13.69093 ymax: 52.573\r\nGeodetic CRS:  WGS 84\r\n             osm_id name addr:city addr:country addr:housenumber\r\n358407135 358407135 <NA>      <NA>         <NA>             <NA>\r\n358407138 358407138 <NA>      <NA>         <NA>             <NA>\r\n417509803 417509803 <NA>      <NA>         <NA>             <NA>\r\n417509805 417509805 <NA>      <NA>         <NA>             <NA>\r\n597668310 597668310 <NA>      <NA>         <NA>             <NA>\r\n597668311 597668311 <NA>      <NA>         <NA>             <NA>\r\n          addr:postcode addr:street addr:suburb check_date\r\n358407135          <NA>        <NA>        <NA>       <NA>\r\n358407138          <NA>        <NA>        <NA>       <NA>\r\n417509803          <NA>        <NA>        <NA>       <NA>\r\n417509805          <NA>        <NA>        <NA>       <NA>\r\n597668310          <NA>        <NA>        <NA>       <NA>\r\n597668311          <NA>        <NA>        <NA>       <NA>\r\n          contact:email contact:phone contact:website craft email\r\n358407135          <NA>          <NA>            <NA>  <NA>  <NA>\r\n358407138          <NA>          <NA>            <NA>  <NA>  <NA>\r\n417509803          <NA>          <NA>            <NA>  <NA>  <NA>\r\n417509805          <NA>          <NA>            <NA>  <NA>  <NA>\r\n597668310          <NA>          <NA>            <NA>  <NA>  <NA>\r\n597668311          <NA>          <NA>            <NA>  <NA>  <NA>\r\n          facebook instagram man_made opening_hours operator organic\r\n358407135     <NA>      <NA>     <NA>          <NA>     <NA>    <NA>\r\n358407138     <NA>      <NA>     <NA>          <NA>     <NA>    <NA>\r\n417509803     <NA>      <NA>     <NA>          <NA>     <NA>    <NA>\r\n417509805     <NA>      <NA>     <NA>          <NA>     <NA>    <NA>\r\n597668310     <NA>      <NA>     <NA>          <NA>     <NA>    <NA>\r\n597668311     <NA>      <NA>     <NA>          <NA>     <NA>    <NA>\r\n          phone product shop source website wheelchair works\r\n358407135  <NA>    <NA> <NA>   <NA>    <NA>       <NA>  <NA>\r\n358407138  <NA>    <NA> <NA>   <NA>    <NA>       <NA>  <NA>\r\n417509803  <NA>    <NA> <NA>   <NA>    <NA>       <NA>  <NA>\r\n417509805  <NA>    <NA> <NA>   <NA>    <NA>       <NA>  <NA>\r\n597668310  <NA>    <NA> <NA>   <NA>    <NA>       <NA>  <NA>\r\n597668311  <NA>    <NA> <NA>   <NA>    <NA>       <NA>  <NA>\r\n                           geometry\r\n358407135 POINT (13.69068 52.35918)\r\n358407138 POINT (13.69093 52.35894)\r\n417509803 POINT (13.68991 52.35888)\r\n417509805  POINT (13.6902 52.35861)\r\n597668310   POINT (13.24445 52.573)\r\n597668311 POINT (13.24443 52.57295)\r\n\r\nbeekeper_locations <- sf_beekeepers$osm_points\r\n\r\n## Berlin borders via {geoboundaries}\r\nsf_berlin <- gb_adm1(c(\"germany\"), type = \"sscgs\")[6,] # the sixth element is Berlin\r\n\r\n## Berlin border incl. district borders via our {d6berlin}\r\n# remotes::install_github(\"EcoDynIZW/d6berlin\")\r\nsf_berlin <- d6berlin::sf_districts\r\n\r\nggplot(beekeper_locations) + \r\n  geom_sf(data = sf_berlin, fill = \"grey10\", color = \"grey30\") +\r\n  geom_sf(size = 4, color = \"#FFB000\", alpha = .3) +\r\n  labs(title = \"Beekeepers in Berlin\",\r\n       caption = \"© OpenStreetMap contributors\")\r\n\r\n\r\n\r\n{elevatr}\r\nThe {elevatr} (https://github.com/jhollist/elevatr/) is an R package that provides access to elevation data from AWS Open Data Terrain Tiles and the Open Topography Global data sets API for raster digital elevation models (DEMs).\r\nWe first need to define a location or bounding box for our elevation data. This can either be a data frame or a spatial object. We use an sf object which holds the projection to be used when assessing the elevation data:\r\n\r\n\r\n## install package\r\n# install.packages(\"elevatr\")\r\n\r\nlibrary(elevatr)\r\n\r\n## manually specify corners of the bounding box of the US\r\nbbox_usa <- data.frame(x = c(-125.0011, -66.9326), \r\n                       y = c(24.9493, 49.5904))\r\n\r\n## turn into spatial, projected bounding box\r\nsf_bbox_usa <- sf::st_as_sf(bbox_usa, coords = c(\"x\", \"y\"), crs = 4326)\r\n\r\n\r\nNow we can download the elevation data with a specified resolution z (ranging from 1 to 14 with 1 being very coarse and 14 being very fine).\r\n\r\n\r\nelev_usa <- get_elev_raster(locations = sf_bbox_usa, z = 5)\r\n\r\nterra::plot(elev_usa)\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/r-spatial-data/r-spatial-data_files/figure-html5/plot-world-rnaturalearth-1.png",
    "last_modified": "2024-02-20T11:14:01+01:00",
    "input_file": "r-spatial-data.knit.md",
    "preview_width": 1920,
    "preview_height": 768
  },
  {
    "path": "posts/d6geodatapackage/",
    "title": "Manage Spatial Data from Our Geodata Archive",
    "description": "Learn how to use the {d6geodata} R package that provides functions for accessing data from the Geodata archive of the Department of Ecological Dynamics. The two functions `geo_overview()` and `get_geodata()` are the main components for all members of our Department. Several other functions are within this package but only meant to be used by the Geodata Manager of the Department.",
    "author": [
      {
        "name": "Moritz Wenzler-Meya",
        "url": {}
      }
    ],
    "date": "2023-03-03",
    "categories": [
      "tutorial",
      "spatial",
      "rstats"
    ],
    "contents": "\r\nThe {d6geodata} package aims to access the data from the Geodata archive of the EcoDyn Department for members only!\r\nThe two main functions are:\r\ngeo_overview()\r\nget_geodata()\r\n\r\n\r\n## remotes::install_github(\"EcoDynIZW/d6geodata\")\r\n## library(d6geodata)\r\n\r\n\r\nIf you want to work with geodata that is already stored in our Geodata archive you have two options:\r\nGo to the EcoDynIZW Website, click on wikis and select Geodata. There you find several spatial data sets with respective metadaat and visualizations. In the metadata section, you’ll find the information . To donwload the data, cope the folder_name information provided in the metadata and use it as an input in the get_geodata() function from our {d6geodata} to get the data from our PopDynCloud. Another option is the function called geo_overview(). There you can select which data and from which location you want to have a list of data.\r\nIf you run the function geo_overview you have to decide if you want to see the raw or processed data by typing 1 for raw and 2 for processed data. Afterwards, you have to decide if you want to see the main (type 1) folders (the regions or sub-regions we have data from) or the sub (type 2) folders (the actually data we have in each region).\r\nExample 1: Main Folder\r\n\r\nd6geodata::geo_overview(path_to_cloud = \"E:/PopDynCloud\")\r\nRaw or processed data: \r\n\r\n1: raw\r\n2: processed\r\n\r\nAuswahl: 2\r\nchoose folder type: \r\n\r\n1: main\r\n2: sub\r\n\r\nAuswahl: 1\r\n[1] \"atlas\" \"BB_MV_B\" \"berlin\" \"europe\" \"germany\" \"world\"\r\n\r\nExample 2: Sub Folder\r\n\r\nd6geodata::geo_overview(path_to_cloud = \"E:/PopDynCloud\")\r\nRaw or processed data: \r\n\r\n1: raw\r\n2: processed\r\n\r\nAuswahl: 2\r\nchoose folder type: \r\n\r\n1: main\r\n2: sub\r\n\r\nAuswahl: 2\r\n$atlas\r\n[1] \"distance-to-human-settlements_atlas_2009_1m_03035_tif\"\r\n[2] \"distance-to-kettleholes_atlas_2022_1m_03035_tif\"      \r\n[3] \"distance-to-rivers_atlas_2009_1m_03035_tif\"           \r\n[4] \"distance-to-streets_atlas_2022_1m_03035_tif\"          \r\n[5] \"landuse_atlas_2009_1m_03035_tif\"                      \r\n\r\n$BB_MV_B\r\n[1] \"_archive\" \"_old_not_verified\" \"dist_path_bb_agroscapelabs\"\r\n[4] \"scripts\"                   \r\n\r\n$berlin\r\n [1] \"_old_not_verified\"                            \r\n [2] \"corine_berlin_2015_20m_03035_tif\"            \r\n [3] \"distance-to-paths_berlin_2022_100m_03035_tif\" \r\n [4]  \"green-capacity_berlin_2020_10m_03035_tif\"    \r\n [5] \"imperviousness_berlin_2018_10m_03035_tif\"     \r\n [6]  \"light-pollution_berlin_2021_100m_03035_tif\"  \r\n [7] \"light-pollution_berlin_2021_10m_03035_tif\"    \r\n [8]  \"motorways_berlin_2022_100m_03035_tif\"        \r\n [9] \"noise-day-night_berlin_2017_10m_03035_tif\"    \r\n[10]  \"population-density_berlin_2019_10m_03035_tif\"\r\n[11] \"template-raster_berlin_2018_10m_03035_tif\"    \r\n[12] \"tree-cover-density_berlin_2018_10m_03035_tif\"\r\n\r\n$europe\r\n[1] \"imperviousness_europe_2018_10m_03035_tif\"\r\n\r\n$germany\r\n [1] \"_old_not_verified\"                                          \r\n [2] \"distance-to-motorway-rural-road_germany_2022_100m_03035_tif\"\r\n [3] \"distance-to-motorways_germany_2022_100m_03035_tif\"          \r\n [4] \"distance-to-paths_germany_2022_100m_03035_tif\"              \r\n [5] \"distance-to-roads-paths_germany_2022_100m_03035_tif\"        \r\n [6] \"distance-to-roads_germany_2022_100m_03035_tif\"              \r\n [7] \"distance_to_paths_germany_2022_100m_03035_tif\"              \r\n [8] \"motoroways_germany_2022_03035_osm_tif\"                      \r\n [9] \"motorway-rural-road_germany_2022_100m_03035_tif\"            \r\n[10] \"motorways_germany_2022_100m_03035_tif\"                      \r\n[11] \"paths_germany_2022_100m_03035_tif\"                          \r\n[12] \"Roads-germany_2022_100m_03035_tif\"                          \r\n[13] \"roads_germany_2022_100m_03035_tif\"                          \r\n[14] \"tree-cover-density_germany_2015_100m_03035_tif\"             \r\n\r\n$world\r\ncharacter(0)\r\n\r\nNow you can copy the name of one of the layers and paste it into the get_geodata() function\r\n\r\n\r\ncorine <-\r\n  d6geodata::get_geodata(\r\n    data_name = \"corine_berlin_2018_20m_03035_tif\",\r\n    path_to_cloud = \"E:/PopDynCloud\",\r\n    download_data = FALSE\r\n  )\r\n\r\n\r\nIf you set download_data = TRUE the data will be download and copied to your data-raw folder. If the data-raw folder doesn’t exist, it will be created.\r\nIf you want to download more than one file, you can simply use lapply() and add multiple file names like this:\r\n\r\n\r\ndata_list <-\r\n  lapply(\r\n    c(\r\n      \"corine_berlin_2018_20m_03035_tif\",\r\n      \"motorways_berlin_2022_100m_03035_tif\"\r\n    ),\r\n    FUN = function(x) {\r\n      d6geodata::get_geodata(\r\n        data_name = x,\r\n        path_to_cloud = \"E:/PopDynCloud\",\r\n        download_data = FALSE\r\n      )})\r\n\r\n\r\nAdditional functions\r\nThe three functions plot_binary_map(), plot_qualitative_map() and plot plot_quantitative_map() can be used to plot raster data with the respective color schemes we used for the Geodata wiki page (note that this function works only for raster data).\r\n\r\n\r\nplot_binary_map(tif = tif)\r\nplot_qualitative_map(tif = tif)\r\nplot_quantitative_map(tif = tif)\r\n\r\n\r\nExample plot\r\n\r\n\r\nlibrary(d6geodata)\r\nplot_qualitative_map(tif = corine)\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/d6geodatapackage/d6geodatapackage_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2023-10-19T12:55:42+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/d6berlin-fisbroker/",
    "title": "Download Spatial Data from the Berlin FIS-Broker Data Base in R",
    "description": "Learn how to download WFS (vector) data and ATOM (raster) data from the Berlin fisbroker data base using the our custom functions `download_fisbroker_wfs()` and `download_fisbroker_atom()`.",
    "author": [
      {
        "name": "Moritz Wenzler-Meya",
        "url": {}
      }
    ],
    "date": "2023-02-15",
    "categories": [
      "tutorial",
      "spatial",
      "rstats",
      "data sources",
      "Berlin"
    ],
    "contents": "\r\n\r\nContents\r\nThe FIS-Broker database\r\nWFS Data\r\nATOM Data\r\n\r\nThe {d6berlin} package provides several functions for the members of the Ecological Department of the IZW. Now two functions are added to the package: download_fisbroker_wfs() and download_fisbroker_atom().\r\n\r\n\r\ninstall.packages(\"remotes\")\r\nremotes::install_github(\"EcoDynIZW/d6berlin\")\r\ninstall.packages(\"rcartocolor\")\r\ninstall.packages(\"stars\")\r\n\r\n\r\n\r\n\r\nlibrary(d6berlin)\r\nlibrary(dplyr)\r\nlibrary(ggplot2)\r\n\r\n\r\nThe FIS-Broker database\r\nThe FIS-Broker database is hosted by the Berlin Senate and provides several geographical data sets. The file formats differ and some data sets have just one of the file formats to offer. The file formats are WMS (Web Media Service: just like a png or jpg), WFS (Web Feature Service: Shapefiles) and ATOM (xml format: raster layers data). This function is only looking for WFS files (shapefiles), because these are the polygons, lines or points that we are looking for.\r\nFor using these two functions you have to select the layer you aim to download from the online data base.\r\nWFS Data\r\nAs an example we will download the layer containing the districts of Berlin (“ALKIS Bezirke”):\r\n\r\n\r\n\r\nurl <- \"https://fbinter.stadt-berlin.de/fb/wfs/data/senstadt/s_wfs_alkis_bezirk\"\r\n\r\ndata_wfs <- d6berlin::download_fisbroker_wfs(link = url)\r\n\r\nReading layer `s_wfs_alkis_bezirk' from data source \r\n  `https://fbinter.stadt-berlin.de/fb/wfs/data/senstadt/s_wfs_alkis_bezirk?service=wfs&version=2.0.0&request=GetFeature&typenames=s_wfs_alkis_bezirk&srsName=EPSG%3A25833' \r\n  using driver `GML'\r\nSimple feature collection with 12 features and 6 fields\r\nGeometry type: MULTIPOLYGON\r\nDimension:     XY\r\nBounding box:  xmin: 370000.8 ymin: 5799521 xmax: 415786.6 ymax: 5837259\r\nProjected CRS: ETRS89 / UTM zone 33N\r\n\r\nglimpse(data_wfs)\r\n\r\nRows: 12\r\nColumns: 7\r\n$ gml_id <chr> \"s_wfs_alkis_bezirk.445\", \"s_wfs_alkis_bezirk.446\", \"…\r\n$ gem    <int> 3, 12, 8, 10, 6, 9, 4, 1, 11, 7, 5, 2\r\n$ namgem <chr> \"Pankow\", \"Reinickendorf\", \"Neukölln\", \"Marzahn-Helle…\r\n$ namlan <chr> \"Berlin\", \"Berlin\", \"Berlin\", \"Berlin\", \"Berlin\", \"Be…\r\n$ lan    <int> 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11\r\n$ name   <int> 11000003, 11000012, 11000008, 11000010, 11000006, 110…\r\n$ geom   <MULTIPOLYGON [m]> MULTIPOLYGON (((399003.5 58..., MULTIPOLYGON (((38999…\r\n\r\nggplot() +\r\n  geom_sf(data = data_wfs, aes(fill = namgem)) +\r\n  rcartocolor::scale_fill_carto_d(palette = \"Bold\")\r\n\r\n\r\n\r\nYou got a spatial layer which you can save to disk or to use it directly.\r\nATOM Data\r\nAs an example we will download a raster of vegetation heights (“Vegetationshöhen 2020 (Umweltatlas)”):\r\n\r\n\r\n\r\nurl <- \"https://fbinter.stadt-berlin.de/fb/atom/Vegetationshoehen/veghoehe_2020.zip\"\r\n\r\ndata_atom <-\r\n  d6berlin::download_fisbroker_atom(\r\n    zip_link = url,\r\n    path = here::here(\"_posts\", \"d6berlin-fisbroker\", \"man\"),\r\n    name = \"vegetation_heights\"\r\n  )\r\n\r\nglimpse(data_atom)\r\n\r\nS4 class 'SpatRaster' [package \"terra\"]\r\n\r\ndata_atom_10 <- terra::aggregate(data_atom, 10)\r\n\r\n\r\n\r\n\r\nggplot() +\r\n  stars::geom_stars(data = stars::st_as_stars(data_atom_10)) +\r\n  coord_sf(expand = FALSE) + \r\n  rcartocolor::scale_fill_carto_c(\r\n    palette = \"Emrld\", name = NULL, \r\n    guide =  guide_legend(label.position = \"bottom\")\r\n  ) + \r\n  theme_void()\r\n\r\n\r\n\r\nA shortcut to plot this kind of data is the plot_qualitative_map() function from our dedicated {d6geodata} package. You can install this package with devtools::install_github(“EcoDynIZW/d6geodata”).\r\n\r\n\r\nd6geodata::plot_quantitative_map(tif = data_atom_10)\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/d6berlin-fisbroker/d6berlin-fisbroker_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2023-10-30T11:29:36+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/NLRX/",
    "title": "Run NetLogo Simulations in R",
    "description": "Learn how to use the {NLRX} R package to run NetLogo models/experiments. This guide gives a brief overview of the basic functionality, how to quickly apply it to a model and shows some example (visual) outputs.",
    "author": [
      {
        "name": "Tobias Kürschner",
        "url": {}
      }
    ],
    "date": "2022-11-23",
    "categories": [
      "tutorial",
      "NetLogo",
      "rstats",
      "modelling"
    ],
    "contents": "\r\n\r\nContents\r\nLibraries and Folders\r\nSetting Up the Library to Work with Your NetLogo Model\r\nCreating an Experiment\r\nCreating a Simulation\r\nRunning the Experiment\r\nNLRX output handling\r\n\r\nCleaning the data\r\nExample plots\r\n\r\nThe NLRX package provides tools to setup and execute NetLogo simulations from R developed by Salecker et al. 2019. NetLogo is a free, open-source and cross-platform modelling environment for simulating natural and social phenomena.\r\nLibraries and Folders\r\n\r\n\r\n# libraries\r\n\r\nfor (pckg in c('dplyr', 'ggplot2', 'scico', 'nlrx'))\r\n{\r\n  if (!require(pckg, character.only = TRUE))\r\n    install.packages(pckg, dependencies = TRUE)\r\n  require(pckg, character.only = TRUE)\r\n}\r\n\r\n# output folder\r\n\r\ncurrentDate <- gsub(\"-\", \"\", Sys.Date())\r\n# todaysFolder <- paste(\"Output\", currentDate, sep = \"_\")\r\n# dir.create(todaysFolder) #in case output folder is wanted\r\n\r\n#virtual-ram if needed\r\n#memory.limit(85000)\r\n\r\n\r\nSetting Up the Library to Work with Your NetLogo Model\r\n\r\n\r\n#NetLogo path\r\nnetlogoPath <- file.path(\"C:/Program Files/NetLogo 6.2.2/\")\r\n\r\n#Model location\r\nmodelPath <- file.path(\"./Model/myModel.nlogo\")\r\n\r\n#Output location (hardly ever used)\r\noutPath <- file.path(\"./\")\r\n\r\n#Java\r\nSys.setenv(JAVA_HOME=\"C:/Program Files/Java/jre1.8.0_331\") \r\n\r\n\r\nConfigure the NetLogo object:\r\n\r\n\r\nnl <- nlrx::nl(\r\n  nlversion = \"6.2.2\",\r\n  nlpath = netlogoPath,\r\n  modelpath = modelPath,\r\n  jvmmem = 1024 # Java virtual machine memory capacity\r\n)\r\n\r\n\r\nCreating an Experiment\r\n\r\n\r\nnl@experiment <- nlrx::experiment(\r\n  expname = 'Exp_1_1',\r\n  # name\r\n  outpath = outPath,\r\n  # outpath\r\n  repetition = 1,\r\n  # number of times the experiment is repeated with the !!!SAME!!! random seed\r\n  tickmetrics = 'true',\r\n  # record metrics at every step\r\n  idsetup = 'setup',\r\n  # in-code setup function\r\n  idgo = 'go',\r\n  # in-code go function\r\n  runtime = 200,\r\n  # soft runtime-cap\r\n  metrics = c('population',  # global variables to be recorded, can also use NetLogos 'count' e.g. count turtles\r\n              'susceptible', # but requires escaped quotation marks for longer commands when strings are involved\r\n              'infected',    # functions similar for both patch and turtle variables (below)\r\n              'immune'),\r\n  metrics.patches = c('totalINfectionsHere',\r\n                      'pxcor',\r\n                      'pycor'),\r\n # metrics.turtles = list(\r\n #   \"turtles\" = c(\"xcor\", \"ycor\")\r\n # ),\r\n  constants = list(  # model parameters that are fixed. In theory all the constant values set in the UI before saving are the ones used\r\n    'duration' = 20, # however, I would always make sure to 'fix\" them though the constant input\r\n    'turtle-shape' = \"\\\"circle\\\"\",\r\n    'runtime' = 5\r\n  ),\r\n  variables = list( # model parameters you want to 'test' have several ways to be set\r\n    'number-people' = list(values = c(150)),     # simple list of values\r\n    'infectiousness' = list(                      #stepwise value change\r\n      min = 50,\r\n      max = 100,\r\n      step = 25,\r\n      qfun = 'qunif'\r\n    ),\r\n    # string based inputs such as used in NetLogo's 'choosers' or inputs require escaped quotation marks\r\n    # \"turtle-shape\" = list(values = c(\"\\\"circle\\\"\",\"\\\"person\\\"\")), \r\n    'chance-recover' = list(values = c(50, 75, 95))\r\n  )\r\n)\r\n\r\n\r\nCreating a Simulation\r\nHere we have several choices of simulation types. The full factorial simdesign used below creates a full-factorial parameter matrix with all possible combinations of parameter values. There are however, other options to choose from if needed and the vignette provides a good initial overview.\r\nA bit counter intuitive but nseeds used in the simdesign is actually the number of repeats we want to use for our simulation. In case we set nseed = 10 and have set the the repetitions above to 1, we would run each parameter combination with 10 different random seeds i.e. 10 times per combination. If we would have set the repetitions to 2 we would run each random seed 2 times i.e. 20 times in total but twice for each seed so 2 results should be identical.\r\nHowever, if your model handles seeds internally such as setting a random seed every time the model is setup, repetitions could be used instead.\r\n\r\n\r\nnl@simdesign <- nlrx::simdesign_ff(nl=nl, nseeds=1) \r\n\r\nprint(nl)\r\n\r\nnlrx::eval_variables_constants(nl)\r\n\r\n\r\nRunning the Experiment\r\nSince the simulations are executed in a nested loop where the outer loop iterates over the random seeds of the simdesign, and the inner loop iterates over the rows of the parameter matrix. These loops can be executed in parallel by setting up an appropriate plan from the future package which is built into nlrx.\r\n\r\n\r\n#plan(multisession, workers = 12) # one worker represents one CPU thread\r\n\r\nresults<- nlrx::run_nl_all(nl = nl)\r\n\r\n\r\nto speed up this this tutorial we load a pre-generated simulation result\r\n\r\n\r\nresults <- readr::read_rds(\"example1.rds\")\r\n\r\ndplyr::glimpse(results)\r\n\r\nRows: 1,809\r\nColumns: 15\r\n$ `[run number]`   <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\r\n$ `number-people`  <dbl> 150, 150, 150, 150, 150, 150, 150, 150, 150…\r\n$ infectiousness   <dbl> 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,…\r\n$ `chance-recover` <dbl> 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,…\r\n$ duration         <dbl> 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,…\r\n$ `turtle-shape`   <chr> \"circle\", \"circle\", \"circle\", \"circle\", \"ci…\r\n$ runtime          <dbl> 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5…\r\n$ `random-seed`    <dbl> -662923684, -662923684, -662923684, -662923…\r\n$ `[step]`         <dbl> 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1…\r\n$ population       <dbl> 0, 153, 155, 156, 156, 157, 158, 159, 160, …\r\n$ susceptible      <dbl> 0, 10, 10, 10, 11, 11, 13, 13, 16, 16, 17, …\r\n$ infected         <dbl> 0, 143, 145, 146, 145, 146, 145, 146, 144, …\r\n$ immune           <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\r\n$ metrics.patches  <list> [<tbl_df[1225 x 5]>], [<tbl_df[1225 x 5]>]…\r\n$ siminputrow      <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\r\n\r\nNLRX output handling\r\nThere are several ways to use / analyze the output directly via nlrx (although I have not used them personally) detailed information can be found here: https://cran.r-project.org/web/packages/nlrx/nlrx.pdf\r\n\r\n\r\nnlrx::setsim(nl, \"simoutput\") <- results # attaching simpout to our NetLogo object\r\n\r\n#write_simoutput(nl) # having nlrx write the output into a file only works without patch or turtle metrics\r\n\r\nnlrx::analyze_nl(nl, metrics = nlrx::getexp(nl, \"metrics\"), funs = list(mean = mean))\r\n\r\n\r\nCleaning the data\r\nNetLogo really dislikes outputting nice and readable variable names so some renaming is in order:\r\n\r\n\r\nraw1 <- results\r\n\r\nraw2 <-\r\n  raw1 %>% dplyr::rename(\r\n    run = `[run number]`,\r\n    recoveryChance = `chance-recover`,\r\n    StartingPop = `number-people`,\r\n    t = `[step]`\r\n  )\r\n\r\n\r\nIn case of very large datasets we want to separate the patch (or turtle) specific data from the global data to speed up the analysis of the global data\r\n\r\n\r\nglobalData <- raw2 %>% dplyr::select(!metrics.patches)#, !metrics.turtles)\r\n\r\n\r\nand summarise like any other dataset\r\n\r\n\r\nsum_1 <-\r\n  globalData %>%\r\n  dplyr::group_by(t, recoveryChance, infectiousness) %>%\r\n  dplyr::summarise(\r\n    across(\r\n      c('infected',\r\n        'immune',\r\n        'susceptible',\r\n        'population'),\r\n      mean\r\n    )\r\n  ) %>%\r\n  dplyr::filter(t < 201) %>%\r\n  tidyr::pivot_longer(cols = -c(recoveryChance, infectiousness, t),\r\n                      names_to = \"EpiStat\") %>%\r\n  dplyr::ungroup()\r\n\r\n\r\nExample plots\r\nWe use the global data to get an overview of the simulated SIR dynamics:\r\n\r\n\r\nggplot(sum_1) +\r\n  geom_line(aes(x = t, y = value, colour = EpiStat), size = 1) +\r\n  facet_grid(infectiousness ~ recoveryChance, labeller = label_both) +\r\n  scale_colour_scico_d(\r\n    palette = \"lajolla\",\r\n    begin = 0.1,\r\n    end = 0.9,\r\n    direction = 1\r\n  ) +\r\n  theme_bw()\r\n\r\n\r\n\r\n… and use the patch specific data to create a heatmap to visualize where most infections have happened in one specific scenario:\r\n\r\n\r\nhmpRaw <- raw2 %>%\r\n  dplyr::filter(infectiousness == 75 & recoveryChance == 75) %>%\r\n  dplyr::select(metrics.patches, t) %>%\r\n  dplyr::rename(pm = metrics.patches)\r\n\r\nhmpSplit <- hmpRaw %>% split(hmpRaw$t)\r\ntempDf <- hmpSplit[[1]]$pm \r\ntempDf <- tempDf %>% as.data.frame() %>%\r\n  dplyr::select(totalINfectionsHere, pxcor, pycor)\r\n\r\nfor (i in 1:length(hmpSplit))\r\n{\r\n  tmp1 <- as.data.frame(hmpSplit[[i]]$pm)\r\n  colnames(tmp1) <- c(paste0(\"tab_\", i), 'pxcor', 'pycor', 'agent', 'breed')\r\n  tmp1 <- tmp1 %>% \r\n    dplyr::select(-c('pxcor', 'pycor', 'agent', 'breed'))\r\n  tempDf <- cbind(tempDf, tmp1)\r\n}\r\n\r\ntempDf2 <- tempDf %>% \r\n  dplyr::select(-c(pxcor, pycor)) \r\n\r\ntempDf$summ <- rowSums(tempDf2)\r\nhmpData <- tempDf %>% dplyr::select(summ, pxcor, pycor)\r\n\r\nggplot(hmpData)+\r\n  geom_tile(aes(x = pxcor, y = pycor, fill = summ)) +\r\n  scale_fill_scico(palette = 'roma', direction = -1, name= 'N infected') +\r\n  ggtitle(\"N infections per cell\") +\r\n  theme_bw()\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/NLRX/nlrxpackage_files/figure-html5/spatial-plots-1.png",
    "last_modified": "2023-10-19T12:55:42+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/coding-basics/",
    "title": "Coding Basics 2: Loops and Functions",
    "description": "A brief introduction into various coding basics for people who are beginning to use R or other programming languages. In this session, we will be looking at both functions and loops in R, with examples from NetLogo and C++ and go over use and basic functionality.",
    "author": [
      {
        "name": "Tobias Kürschner",
        "url": {}
      }
    ],
    "date": "2022-11-14",
    "categories": [
      "tutorial",
      "workflow",
      "rstats",
      "NetLogo"
    ],
    "contents": "\r\n\r\nContents\r\nFunctions and Loops\r\nWhat are they for?\r\nFunctions\r\nLoops\r\nA Classic, the ‘for’ Loop\r\n‘while’ Loops\r\n\r\nBonus Round: Conditionals\r\n\r\n\r\n\r\n\r\n\r\nFunctions and Loops\r\nWhat are they for?\r\nIn short: making you life easier. They are used to automate certain steps in your code to make the execution faster.\r\nFunctions\r\nA simple example: you have multiple data sets with a measurement in inches. To continue working with that data you need to convert it meter, but doing that manually would take hours. Solution: a function!!\r\nIn R:\r\n\r\n\r\ninch_to_meter <-   #function name\r\n  function(inch) #function input parameter(s)\r\n  { # function body\r\n    inM <- (inch * 0.0254) # in our case: transformation\r\n    \r\n    return(inM) # function reporter\r\n  }\r\n\r\n\r\nNow lets apply that function to some random example data:\r\n\r\n\r\nmyOldData <- c(15,98,102,5,17)\r\n\r\nmyNewData <- inch_to_meter(myOldData)\r\n\r\n\r\nOur function is applied to all elements of the old data, giving us the converted measurements.\r\n\r\n\r\nmyOldData\r\n\r\n[1]  15  98 102   5  17\r\n\r\nmyNewData\r\n\r\n[1] 0.3810 2.4892 2.5908 0.1270 0.4318\r\n\r\nFor a more general approach we can use the following template for simple functions:\r\n\r\n\r\nmyFunction <-\r\n  function(input1, input2, input3)\r\n  {\r\n    output <- input1 * input3 / input2 # example operation\r\n    \r\n    return(output)\r\n  }\r\n\r\n\r\nIn C++:\r\n\r\n# the viableCell function of the class Grid returns a bool and takes two inputs\r\n# (x and y coordinates)\r\nbool Grid::viableCell(CellCount_t x, CellCount_t y) \r\n{\r\n    return !m_grid.empty() && x < m_grid.cbegin() -> size() && y < m_grid.size();\r\n}\r\n\r\nIn NetLogo, the closest thing we have to functions are reporters:\r\n\r\nto-report Male_Alpha_Alive \r\n\r\n  ifelse (any? turtles-here with [isAlpha = true AND isFemale = false])\r\n  [report true]\r\n  [report false] #else\r\n\r\nend\r\n\r\nLoops\r\nWhat is a loop? It a simply a piece of code we want to repeat. But wait, isn’t that exactly what a function does? Well, yes and no. A function (after it is declared) is an embodiment of a piece of code that we can run anytime just by calling it. A loop is a local repetition of code.\r\nLets stay in R and look at some examples:\r\nA Classic, the ‘for’ Loop\r\nIf you auto fill “for” in R, it will give you the following structure:\r\n\r\n\r\nfor (variable in vector)\r\n{\r\n  #body\r\n}\r\n\r\n\r\nSo what is variable and what is vector? In this case the variable, you could also call iterator (often you will see the letter i used) is simply put a counter that tells our loop how many times it has repeated itself. The vector is determined by us and tells the loop how many repetitions we want before it stops.\r\n\r\n\r\nfor (i in 1:10)\r\n{\r\n  #body\r\n}\r\n\r\n\r\nThe loop above will now repeat exactly 10 times and then stop. R is helpful in the sense that it automatically increments i after each repetition. Other language like C++ for example need a manual increment of i:\r\n\r\nfor (i = 0, i <= 10, ++i) #C++ \r\n{\r\n  #body\r\n}\r\n\r\nWith the basics out of the way lets look at an example:\r\n\r\n\r\nmyData <- rnorm(30) # random numbers\r\nmyResults <- 0 # initializing myResults\r\n\r\nfor(i in 1:10)\r\n{\r\n  # i-th element of `myData` squared into `i`-th position of `myResults`\r\n  myResults[i] <- myData[i] * myData[i]  \r\n  print(i)\r\n}\r\n\r\n[1] 1\r\n[1] 2\r\n[1] 3\r\n[1] 4\r\n[1] 5\r\n[1] 6\r\n[1] 7\r\n[1] 8\r\n[1] 9\r\n[1] 10\r\n\r\nmyResults\r\n\r\n [1] 2.348470e+00 8.045558e-02 9.254717e+00 6.298287e-01 5.587004e-02\r\n [6] 5.496863e-01 2.161664e+00 1.326536e-05 2.026844e+00 2.065682e-01\r\n\r\nThis was of course a very simple loop and there is pretty much no limit to the level of complexity those loops can have and how many loops could be nested. But be warned there can be some pitfalls with loops. Exercise: Would the following loop work?\r\n\r\n\r\nfor (i in 1:length(summIdentSplit))\r\n{\r\n  tmpSumm <- summIdentSplit[[i]]\r\n  tmpName <- SummIdentVector[[i]]\r\n  \r\n  if (base::grepl(\"HD\", tmpName) == TRUE)\r\n  {\r\n    clearName <- \"Habitat-driven movement\"\r\n    cn <- \"HD\"\r\n  }\r\n  \r\n  combiMelt <- tmpSumm %>%\r\n    dplyr::ungroup() %>%\r\n    dplyr::select(t, Ninfected_mean, Nimmune_mean) %>%\r\n    dplyr::rename(Infected = Ninfected_mean, Immune = Nimmune_mean)\r\n  \r\n  combiMelt_sd <- tmpSumm %>%\r\n    dplyr::ungroup() %>%\r\n    dplyr::select(t, Ninfected_sd, Nimmune_sd) %>%\r\n    dplyr::rename(Inf_sd = Ninfected_sd, Imm_sd = Nimmune_sd)\r\n  \r\n  combiMelt_c <- dplyr::left_join(combiMelt, combiMelt_sd , by = \"t\")\r\n  \r\n  q <- 0\r\n  combiMelt_c$quarter <- 0\r\n  \r\n  for (i in 1:nrow(combiMelt_c))\r\n  {\r\n    if (i %% 13 == 0)\r\n    {\r\n      q <- q + 1\r\n    }\r\n    combiMelt_c$quarter[i] <- q\r\n  }\r\n}\r\n\r\n\r\nThe answer is no. There are two loops involved where one is nested inside the other. So far, that would not be an issue, however, both loops use the iterator (variable) i. To see what would happen:\r\n\r\nFirst iteration:\r\n  i = 1\r\n\r\nfor (i in 1:length(summIdentSplit))\r\n{\r\n  tmpSumm <- summIdentSplit[[i]]\r\n  tmpName <- SummIdentVector[[i]]\r\n  \r\n  if (base::grepl(\"HD\", tmpName) == TRUE)\r\n  {\r\n    clearName <- \"Habitat-driven movement\"\r\n    cn <- \"HD\"\r\n  }\r\n  \r\n  combiMelt <- tmpSumm %>%\r\n    dplyr::ungroup() %>%\r\n    dplyr::select(t, Ninfected_mean, Nimmune_mean) %>%\r\n    dplyr::rename(Infected = Ninfected_mean, Immune = Nimmune_mean)\r\n  \r\n  combiMelt_sd <- tmpSumm %>%\r\n    dplyr::ungroup() %>%\r\n    dplyr::select(t, Ninfected_sd, Nimmune_sd) %>%\r\n    dplyr::rename(Inf_sd = Ninfected_sd, Imm_sd = Nimmune_sd)\r\n  \r\n  combiMelt_c <- dplyr::left_join(combiMelt, combiMelt_sd , by = \"t\")\r\n  \r\n  q <- 0\r\n  combiMelt_c$quarter <- 0\r\n\r\nAt this point i is still 1 and lets say nrow(combiMelt_c) is also 10 (like in our outer loop)\r\n\r\n  for (i in 1:nrow(combiMelt_c))\r\n  {\r\n    if (i %% 13 == 0)\r\n    {\r\n      q <- q + 1\r\n    }\r\n    combiMelt_c$quarter[i] <- q\r\n\r\nat this point i is iterated within the inner loop\r\n\r\n  }\r\n\r\nonce we reach this point i is 10, so the for the next iteration of the outer\r\nloop, i = 10 so most iterations of the outer loop will be skipped!\r\n\r\n}\r\n\r\nSolution: make sure to use different iterators in nested loops. For example the outer loop uses i, the inner loop uses j and maybe that loop has also a nested loop which then uses k as its iterator.\r\n‘while’ Loops\r\nSimilar to for loops, while loops also repeat a certain block of code. The difference here is, that while loop repeat until a certain condition is fulfilled, potentially forever. R’s auto fill provides us the following code snippet:\r\n\r\n\r\nwhile (condition)\r\n{\r\n  \r\n}\r\n\r\n\r\nA simple example:\r\n\r\n\r\nn <- 0\r\n\r\nwhile(n < 100)\r\n{\r\n  n = n + 1\r\n}\r\n\r\nprint(n)\r\n\r\n[1] 100\r\n\r\nAs long as n is below 100 we increment n every repeat. Take note: if the condition is never fulfilled, the loop will run forever and the software might crash. In complex loops you could run a “escape timer” such as in this example plucked from an IBM:\r\n\r\n\r\nwhile(!viableCell())\r\n{\r\n   turn(2 * (atan (( (1 - 0.5) / (1 + 0.9)) * tan ((randomDouble(1) - 0.5)) * 180)) + 1) \r\n}\r\n\r\n\r\nThis little loop runs on an individual and check’s the cell in front of the individual for its viability to move into. As long as viableCell() reports false, the individual turns. However, there are cases when they are no viable cells around so the individual would turn in a circle forever. Introducing a random timer (note, this is not an optimal solution just a quick fix).\r\n\r\nint timer = 0\r\n\r\nwhile(!viableCell() && timer < 50 )\r\n{\r\n   turn(2 * (atan (( (1 - 0.5) / (1 + 0.9)) * tan ((randomDouble(1) - 0.5)) * 180)) + 1) \r\n  \r\n  ++timer #increment the timer\r\n}\r\n\r\nNow the individual turns a maximum of 50 times before the loop ends. Another option would be conditionals:\r\nBonus Round: Conditionals\r\nMost people already know what a conditional is. Basically, when a certain condition is fulfilled something happens (usually done via if and/or else).\r\n\r\nint timer = 0\r\n\r\nwhile(!viableCell())\r\n{\r\n   turn(2 * (atan (( (1 - 0.5) / (1 + 0.9)) * tan ((randomDouble(1) - 0.5)) * 180)) + 1) \r\n  \r\n  ++timer #increment the timer\r\n  \r\n  if(timer == 50)\r\n  {\r\n    break # break is a function that for example ends a loop\r\n  }\r\n}\r\n\r\nConditionals can be used in a variety of way within and outside of loops but have the advantage that they can be used stop loops under certain conditions. Lets say you are running a complex construct of multiple nested for loops to find a certain value in your data. You don’t need to always iterate through all the data if you can create certain logical stop conditions.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2023-10-19T12:55:42+02:00",
    "input_file": {}
  },
  {
    "path": "posts/coding-goodpractice/",
    "title": "Coding Basics 1: General Tips",
    "description": "A brief introduction into various coding basics for people who are beginning to use R or other programming languages. In this session, we will be looking at some general tips and information that helps creating good and understandable code. Specifically we are looking at naming conventions, indentations and commenting of code.",
    "author": [
      {
        "name": "Tobias Kürschner",
        "url": {}
      }
    ],
    "date": "2022-11-13",
    "categories": [
      "tutorial",
      "workflow",
      "rstats"
    ],
    "contents": "\r\n\r\nContents\r\nGeneral:\r\nVariable Naming Conventions\r\nSnakecase:\r\nCamelcase:\r\nPascalcase:\r\nHungarian Notation:\r\nSidenote: Namespaces and Function Names\r\n\r\nIndentation\r\nCommenting Your Code\r\nDon’t Repeat Yourself\r\n\r\nGeneral:\r\nWrite as few lines as possible.\r\nUse appropriate naming conventions.\r\nSegment blocks of code in the same section into paragraphs.\r\nUse indentation to marks the beginning and end of control structures.\r\nDon’t repeat yourself.\r\n\r\n\r\n\r\nVariable Naming Conventions\r\nVariable naming is an important aspect in making your code readable. Create variables that describe their function and follow a consistent theme throughout your code. Separate words in a variable name without the use of whitespace and do not repeat variable names (unless in certain circumstances such as temporary variable within loops).\r\nSnakecase:\r\nWords are delimited by an underscore.\r\n\r\n\r\nvariable_one <- 1\r\n\r\nvariable_two <- 2\r\n\r\n\r\nCamelcase:\r\nWords are delimited by capital letters, except the initial word.\r\n\r\n\r\nvariableOne <- 1\r\n\r\nvariableTwo <- 2\r\n\r\n\r\nPascalcase:\r\nWords are delimited by capital letters.\r\n\r\n\r\nVariableOne <- 1\r\n\r\nVariableTwo <- 2\r\n\r\n\r\nHungarian Notation:\r\nThis notation describes the variable type or purpose at the start of the variable name, followed by a descriptor that indicates the variable’s function. The Camelcase notation is used to delimit words.\r\n\r\n\r\niVariableOne   <- 1  # i - integer\r\n\r\nsVariableTwo   <- \"two\" # s - string\r\n\r\nlVariableThree <- list() # l - list\r\n\r\n\r\nSidenote: Namespaces and Function Names\r\nOften when using for example R we will use libraries and many of them at the same time. Quite a few of those libraries are using the same names for their functions. This can be a big issues and cause code to suddenly not run anymore even though the only difference my be the order in which libraries are loaded. The last library loaded with a certain function will be the default one used by R. A prominent example would be the ‘raster’ package and ‘tidyverse’ (dplyr) who share the name ‘select’ for a function. So, when in doubt use namespaces to make sure to link a function to their library.\r\n\r\nuse the select function from the dplyr library\r\ndplyr::select(.....)\r\n\r\nIndentation\r\nI assume you already know that your code should have some sort of indentation. However, it’s also worth noting that its a good idea to keep your indentation style consistent.\r\nAs a quick reminder:\r\nBad:\r\nLong lines of text with no separation:\r\n\r\n\r\nmydata <- iris %>% dplyr::filter(Species == \"virginica\") %>% summarise_at(.vars = c(\"Sepal.Length\", \"Sepal.Width\"),.funs = \"mean\")\r\n\r\nfor (i in 1:length(hmpSplit)){ tmp1 <- as.data.frame(hmpSplit[[i]]$pm); colnames(tmp1) <- c(paste0(\"tab_\", i), 'pxcor', 'pycor', 'agent', 'breed'); tmp1 <- tmp1 %>% dplyr::select(-c('pxcor', 'pycor', 'agent', 'breed')); tempDf <- cbind(tempDf, tmp1)}\r\n\r\n\r\nGood:\r\n\r\n\r\nmydata <- iris %>%\r\n  dplyr::filter(Species == \"virginica\") %>%\r\n  summarise_at(.vars = c(\"Sepal.Length\", \"Sepal.Width\"),\r\n               .funs = \"mean\")\r\n\r\n\r\nfor (i in 1:length(hmpSplit))\r\n{\r\n  tmp1 <- as.data.frame(hmpSplit[[i]]$pm)\r\n  \r\n  colnames(tmp1) <- c(paste0(\"tab_\", i), 'pxcor', 'pycor', 'agent', 'breed')\r\n  \r\n  tmp1 <- tmp1 %>% \r\n    dplyr::select(-c('pxcor', 'pycor', 'agent', 'breed'))\r\n  \r\n  tempDf <- cbind(tempDf, tmp1)\r\n}\r\n\r\n\r\nThe details on how to indent or use white spaces are up to individual styles with some guidelines, such as avoiding long lines of text, to keep in mind. Also fine would be something like the following.\r\n\r\n\r\nfor (i in 1:length(hmpSplit))\r\n{\r\n  tmp1 <- as.data.frame(hmpSplit[[i]]$pm)\r\n  colnames(tmp1) <- c(paste0(\"tab_\", i), 'pxcor', 'pycor', 'agent', 'breed')\r\n  tmp1 <- tmp1 %>% dplyr::select(-c('pxcor', 'pycor', 'agent', 'breed'))\r\n  tempDf <- cbind(tempDf, tmp1)\r\n}\r\n\r\n\r\nCommenting Your Code\r\nCommenting your code is fantastic but it can be overdone or just be plain redundant. Comments should add information or explanations to make your code understandable for people who didn’t write it or yourself in a year from now:\r\n\r\n\r\n# comment that adds information:\r\n\r\nwrite_simoutput(nl) # having nlrx write the output into a file - only works without patch or turtle metrics\r\n\r\n\r\n# redundant comments:\r\n\r\nif (col == \"blue\") # if colour is blue\r\n{\r\n  print('colour is blue') # print that the colour is blue\r\n}\r\n\r\n\r\nA better solution (if a comment is absolutely necessary) would be:\r\n\r\n\r\n# display selected colour\r\nif (colour == \"blue\")\r\n{\r\n  print('colour is blue')\r\n}\r\n\r\n\r\nDon’t Repeat Yourself\r\nAs a rule of thumb, if you have to do the same task multiple times in your code: automate it. A while back, I was decomposing many time series and I needed only part of the output, in this case the ‘trend’. Instead of running the same lines of code that remove all other components for each time series individually, a short function reduced the amount of needed code substantially.\r\n\r\n\r\nDecompTrend <- function(ts){\r\n  \r\n  temp1<-stats::decompose(ts)\r\n  return(temp1$trend)\r\n}\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2023-10-19T12:55:42+02:00",
    "input_file": {}
  },
  {
    "path": "posts/netlogo-weibull/",
    "title": "Turning Equations into Functions in NetLogo",
    "description": "While some programming languages, such as R, offer many native functions, others (e.g., NetLogo) offer fewer built-in options. However, users can create their own functions easily. Here, we will show the example of a Weibull density distribution function & associated cumulative density distribution function—both not yet implemented in NetLogo or NetLogo extensions.",
    "author": [
      {
        "name": "Marius Grabow",
        "url": {}
      }
    ],
    "date": "2022-11-11",
    "categories": [
      "NetLogo",
      "rstats",
      "distributions"
    ],
    "contents": "\r\n\r\nContents\r\nDensity distribution function\r\nR\r\nNetLogo\r\n\r\nCumulative density function\r\nR\r\nNetLogo\r\n\r\n\r\nDensity distribution\r\nfunction\r\nLet’s first take a look at the Weibull density distribution\r\nfunction:\r\n\\[\\begin{equation}\r\n\r\nf(x) = \\frac{\\gamma} {\\alpha} (\\frac{x-\\mu}\r\n{\\alpha})^{(\\gamma - 1)}\\exp{(-((x-\\mu)/\\alpha)^{\\gamma})}\r\n\\hspace{.3in}  x \\ge \\mu; \\gamma, \\alpha > 0\r\n\r\n\\end{equation}\\]\r\nR\r\nIn R, this is already implemented:\r\n\r\n\r\nscale <- 3\r\nshape <- 1\r\n\r\ndweibull(scale, shape = shape)\r\n\r\n\r\n[1] 0.04978707\r\n\r\nNetLogo\r\nIn Netlogo we can simply translate the mathematical equation into a\r\nfunction:\r\nto-report weibull [a_scale a_shape x]\r\n\r\n  let Wei (a_shape / a_scale ) * ((x / a_scale)^(a_shape - 1)) * exp( - ((x / a_scale)^ a_shape))\r\n\r\n  report Wei\r\n\r\nend\r\nCumulative density function\r\nLet’s also take a look at the Weibull cumulative density\r\nfunction:\r\n\\[\\begin{equation}\r\n\r\nF(x) = 1 - e^{-(x^{\\gamma})} \\hspace{.3in}  x \\ge 0; \\gamma > 0\r\n\r\n\\end{equation}\\]\r\nR\r\nAgain, fully implemented in R already:\r\n\r\n\r\nscale <- 3\r\nshape <- 1\r\nx <- 5\r\n\r\npweibull(q = x, scale = scale, shape = shape)\r\n\r\n\r\n[1] 0.8111244\r\n\r\nNetLogo\r\nIn NetLogo we can simply translate the mathematical equation into a\r\nfunction:\r\nto-report weibull_cumulative [a_scale a_shape x]\r\n\r\n  let Wei_cumu 1 - exp( - ((x / a_scale)^ a_shape))\r\n\r\n  report Wei_cumu\r\n\r\nend\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2023-10-19T12:55:42+02:00",
    "input_file": {}
  },
  {
    "path": "posts/imageseg/",
    "title": "imageseg",
    "description": "imageseg is an R package for deep learning image segmentation using the U-Net model architecture by Ronneberger (2015), implemented in Keras and TensorFlow. It provides pre-trained models for forest structural metrics (canopy density and understory vegetation density) and a workflow to apply these on custom images",
    "author": [
      {
        "name": "Jürgen Niedballa",
        "url": {}
      }
    ],
    "date": "2022-09-12",
    "categories": [],
    "contents": "\r\nimageseg\r\nR package for deep learning image segmentation using the U-Net model architecture by Ronneberger (2015), implemented in Keras and TensorFlow. It provides pre-trained models for forest structural metrics (canopy density and understory vegetation density) and a workflow to apply these on custom images.\r\nIn addition, it provides a workflow for easily creating model input and model architectures for general-purpose image segmentation based on the U-net architecture. Model can be trained on grayscale or color images, and can provide binary or multi-class image segmentation as output.\r\nThe package can be found on CRAN.\r\nThe preprint of the paper describing the package is available on bioRxiv.\r\nInstallation\r\nFirst, install the R package “R.rsp” which enables the static vignettes.\r\n\r\n\r\n\r\nInstall the imageseg package from CRAN via:\r\n\r\n\r\n\r\nAlternatively you can install from GitHub (requires remotes package and R.rsp):\r\n\r\n\r\n\r\nUsing imageseg requires Keras and TensorFlow. See the vignette for information about installation and initial setup:\r\nTutorial\r\nSee the vignette for an introduction and tutorial to imageseg.\r\n\r\n\r\n\r\nThe vignette covers:\r\n- Installation and setup\r\n- Sample workflow for canopy density assessments\r\n- Training new models\r\n- Continued training of existing models\r\n- Multi-class image segmentation models\r\n- Image segmentation based on grayscale images\r\nForest structure model download\r\nThe models, example predictions, training data and R script for model training for both the canopy and understory model are available from Dryad as a single download.\r\nSee the “Usage Notes” section for details on the dataset.\r\nThe models and script (without the training data) are also hosted on Zenodo and can be downloaded individually from zenodo.\r\nThe pre-trained models for forest canopy density and understory vegetation density are available for download. The zip files contain the model (as .hdf5 files) and example classifications to give an impression of model performance and output:\r\nCanopy model\r\nUnderstory model\r\nPlease see the vignette for further information on how to use these models.\r\nTraining data download\r\nTraining data for both the canopy and understory model are included in the Dryad dataset download in the zip files:\r\nimageseg_canopy_training_data.zip\r\nimageseg_understory_training_data.zip\r\nFor details, please see the Usage Notes and the info.txt files contained in the zip files.\r\nThe training data are not required for users who only wish to use the pre-trained models on their own images.\r\n\r\n\r\n\r\n",
    "preview": "https://raw.githubusercontent.com/EcoDynIZW/EcoDynIZW.github.io/main/img/wiki/hex-imageseg.png",
    "last_modified": "2023-10-19T12:55:42+02:00",
    "input_file": {}
  },
  {
    "path": "posts/isorix/",
    "title": "Isorix",
    "description": "IsoriX is an R package aiming at building isoscapes using mixed models and inferring the geographic origin of organisms based on their isotopic ratios",
    "author": [
      {
        "name": "Alexandre Courtiol",
        "url": {}
      }
    ],
    "date": "2022-09-12",
    "categories": [],
    "contents": "\r\nHow to download and install IsoriX?\r\nYou can download and install the stable version of IsoriX directly from within R by typing:\r\n\r\n\r\ninstall.packages(\"IsoriX\", dependencies = TRUE)\r\n\r\n\r\nNote: if you get into troubles due to gmp, magick, maps, maptools, RandomFields, rgeos, or rgl, retry using simply:\r\n\r\n\r\ninstall.packages(\"IsoriX\")\r\n\r\n\r\nThese packages offer additional functionalities but some of them are particularly difficult to install on some systems.\r\nIf you want the development version of IsoriX, you can download and install it by typing:\r\n\r\n\r\nremotes::install_github(\"courtiol/IsoriX/IsoriX\")\r\n\r\n\r\nMind that you need the R package remotes to be installed for that to work. Mind also that the development version, being under development, can sometimes be broken. So before downloading it make sure that the current build status is build passing. The current built status is provided at the top of this readme document.\r\nAlso, if you access the network via a proxy, you may experience troubles with install_github. In such case try something like:\r\n\r\n\r\nlibrary(httr)\r\nwith_config(use_proxy(\"192.168.2.2:3128\"), devtools::install_github(\"courtiol/IsoriX/IsoriX\"))\r\n\r\n\r\nOff course, unless you are in the same institute than some of us, replace the numbers with your proxy settings!\r\nWhere to learn about IsoriX?\r\nYou can start by reading our bookdown!\r\nThen, if may not be a bad idea to also have a look at our papers: here and there.\r\nAnother great source of help is our mailing list. First register for free (using your Google account) and then feel free to send us questions.\r\nFor specific help on IsoriX functions and objects, you should also check the documentation embedded in the package:\r\n\r\n\r\nhelp(package = \"IsoriX\")\r\n\r\n\r\nin R after having installed and attached (= loaded) the package.\r\nHow can you contribute?\r\nThere are plenty way you can contribute! If you are fluent in R programming, you can improve the code and develop new functions. If you are not so fluent, you can still edit the documentation files to make them more complete and clearer, write new vignettes, report bugs or make feature requests.\r\nSome useful links\r\n\r\n\r\n\r\n",
    "preview": "https://raw.githubusercontent.com/EcoDynIZW/EcoDynIZW.github.io/main/img/wiki/hex-isorix.png",
    "last_modified": "2023-10-19T12:55:42+02:00",
    "input_file": {}
  },
  {
    "path": "posts/camtrapr/",
    "title": "Manage Camera Trap Data in R",
    "description": "Learn how to use the {camtrapR} R package for streamlined and flexible camera trap data management. It should be most useful to researchers and practitioners who regularly handle large amounts of camera trapping data.",
    "author": [
      {
        "name": "Jürgen Niedballa",
        "url": {}
      }
    ],
    "date": "2022-05-23",
    "categories": [
      "tutorial",
      "rstats",
      "camera trapping",
      "data management"
    ],
    "contents": "\r\n\r\nContents\r\nInstallation\r\nExiftool\r\n\r\nThe {camtrapR} R package simplifies camera trap data management in R.\r\nInstallation\r\nYou can install the release version of {camtrapR} from CRAN:\r\n\r\n\r\ninstall.packages(\"camtrapR\")\r\n\r\n\r\nExiftool\r\nNumerous important {camtrapR} functions read EXIF metadata from JPG images (and videos). This is done via Exiftool, a free and open-source sofware tool developed by Phil Harvey and available for Windows, MacOS and Linux.\r\nTo make full use of {camtrapR}, you will need Exiftool on your system. You can download it from the Exiftool homepage and follow the installation instruction in vignette 1.\r\nYou may not need Exiftool if you do not work with image files, but only use {camtrapR} to create input for occupancy or spatial capture-recapture models from existing record tables.\r\nSee the article in Methods in Ecology and Evolution for an overview of the package. The five vignettes provide examples for the entire workflow.\r\nCitation\r\n\r\n\r\n\r\n",
    "preview": "https://raw.githubusercontent.com/EcoDynIZW/EcoDynIZW.github.io/main/img/wiki/hex-camtrapr.png",
    "last_modified": "2023-10-30T10:34:49+01:00",
    "input_file": {}
  },
  {
    "path": "posts/nlmr/",
    "title": "Simulate Neutral Landscape Models",
    "description": "Learn how to use the {NLMR} R package to simulate neutral landscape models (NLMs), a common set of various null models for spatial analysis and as input for spatially-explicit, generic models.",
    "author": [
      {
        "name": "Cedric Scherer",
        "url": "https://cedricscherer.com"
      }
    ],
    "date": "2021-08-06",
    "categories": [
      "tutorial",
      "rstats",
      "spatial",
      "modelling"
    ],
    "contents": "\r\nThe {NLMR} R package to simulate neutral landscape models (NLM). Designed to be a generic framework like NLMpy, it leverages the ability to simulate the most common NLM that are described in the ecological literature.\r\nIf you want to learn more about the {NLMR} package and the accompanying {landscapetools} package, check the publication Sciaini, Fritsch, Scherer 6 Simpkins (2019) Methods in Ecology and Evolution.\r\nInstallation\r\nInstall the release version from CRAN:\r\n\r\n\r\ninstall.packages(\"NLMR\")\r\n\r\n\r\nor the development version from Github, along with two packages that are needed for the generation of random fields:\r\n\r\n\r\ninstall.packages(\"remotes\")\r\nremotes::install_github(\"cran/RandomFieldsUtils\")\r\nremotes::install_github(\"cran/RandomFields\")\r\nremotes::install_github(\"ropensci/NLMR\")\r\n\r\n\r\nUsage\r\nEach neutral landscape models is simulated with a single function (all starting with nlm_) in NLMR, e.g.:\r\n\r\n\r\nrandom_cluster <- NLMR::nlm_randomcluster(\r\n  nrow = 100,\r\n  ncol = 100,\r\n  p    = 0.5,\r\n  ai   = c(0.3, 0.6, 0.1),\r\n  rescale = FALSE\r\n)\r\n\r\nrandom_curdling <- NLMR::nlm_curds(\r\n  curds = c(0.5, 0.3, 0.6),\r\n  recursion_steps = c(32, 6, 2)\r\n)\r\n\r\nmidpoint_displacement <- NLMR::nlm_mpd(\r\n  ncol = 100,\r\n  nrow = 100,\r\n  roughness = 0.61\r\n)\r\n\r\n\r\n{NLMR} supplies 15 NLM algorithms, with several options to simulate derivatives of them. The algorithms differ from each other in spatial auto-correlation, from no auto-correlation (random NLM) to a constant gradient (planar gradients).\r\nThe package builds on the advantages of the raster package and returns all simulation as RasterLayer objects, thus ensuring a direct compatibility to common GIS tasks and a flexible and simple usage:\r\n\r\n\r\nclass(random_cluster)\r\n\r\n[1] \"RasterLayer\"\r\nattr(,\"package\")\r\n[1] \"raster\"\r\n\r\nrandom_cluster\r\n\r\nclass      : RasterLayer \r\ndimensions : 100, 100, 10000  (nrow, ncol, ncell)\r\nresolution : 1, 1  (x, y)\r\nextent     : 0, 100, 0, 100  (xmin, xmax, ymin, ymax)\r\ncrs        : NA \r\nsource     : memory\r\nnames      : clumps \r\nvalues     : 1, 3  (min, max)\r\n\r\nVisualization\r\nThe {landscapetools} package provides a function show_landscape that was developed to plot raster objects and help users to adhere to some standards concerning color scales and typography. This means for example that by default the viridis color scale is applied (and you can pick others from the {viridis} package, too).\r\n\r\n\r\n#install.packages(\"landscapetools\")\r\n\r\n## plotting continuous values\r\nlandscapetools::show_landscape(random_cluster)\r\n\r\n\r\n## plotting discrete values\r\nlandscapetools::show_landscape(random_curdling, discrete = TRUE)\r\n\r\n\r\n## using another viridis palette\r\nlandscapetools::show_landscape(midpoint_displacement, viridis_scale = \"magma\")\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "https://raw.githubusercontent.com/EcoDynIZW/EcoDynIZW.github.io/main/img/wiki/hex-nlmr.png",
    "last_modified": "2023-10-19T12:55:42+02:00",
    "input_file": {}
  },
  {
    "path": "posts/d6berlinpackage/",
    "title": "Spatial Datasets and Template Maps for Berlin",
    "description": "Learn how to use the {d6berlin} R package that provides spatial data sets and template maps for Berlin with carefully chosen and aesthetically pleasing defaults. The template maps include Berlin districts, green spaces, imperviousness levels, water bodies, district borders, roads, metro stations, and railways. Templates maps are created with a {ggplot2} wrapper function with the utility to add an inset globe with a locator pin, a scalebar, and a caption stating the data sources.",
    "author": [
      {
        "name": "Cedric Scherer",
        "url": "https://cedricscherer.com"
      }
    ],
    "date": "2021-03-18",
    "categories": [
      "tutorial",
      "rstats",
      "spatial",
      "Berlin",
      "ggplot2",
      "data sources",
      "workflow"
    ],
    "contents": "\r\n\r\nContents\r\nInstallation\r\nA Basic Template Map of Imperviousness\r\nBerlin Data Sets\r\nAdding Locations to the Map\r\nCustom Styling\r\nSave Map\r\n\r\nThe {d6berlin} package aims provide template maps for Berlin with carefully chosen and aesthetically pleasing defaults. Template maps include green spaces, imperviousness levels, water bodies, district borders, roads, and railways, plus the utility to add a globe with locator pin, a scalebar, and a caption to include the data sources.\r\nThere are two main functionalities:\r\nCreate a template map with imperviousness and green spaces with base_map_imp()\r\nProvide various ready-to-use Berlin data sets with sf_*\r\nFurthermore, the package provides utility to add a globe with locator pin, a scalebar, and a caption to include the data sources.\r\nInstallation\r\nYou can install the {d6berlin} package from GitHub:\r\n\r\n\r\n## install.packages(\"remotes\")\r\n## remotes::install_github(\"EcoDynIZW/d6berlin\")\r\nlibrary(d6berlin)\r\n\r\n\r\nNote: If you are asked if you want to update other packages either press “No” (option 3) and continue or update the packages before running the install command again.\r\nA Basic Template Map of Imperviousness\r\nThe basic template map shows levels of imperviousness and green areas in Berlin. The imperviousness raster data was derived from Geoportal Berlin (FIS-Broker) with a resolution of 10m. The vector data on green spaces was collected from data provided by the OpenStreetMap Contributors. The green spaces consist of a mixture of land use and natural categories (namely “forest”, “grass”, “meadow”, “nature_reserve”, “scrub”, “heath”, “beach”, “cliff”).\r\nThe map is projected in EPSG 4326 (WGS84).\r\n\r\n\r\nd6berlin::base_map_imp()\r\n\r\n#> Aggregating raster data.\r\n#> Plotting basic map.\r\n#> Styling map.\r\n\r\n\r\nYou can also customize the arguments, e.g. change the color intensity, add a globe with a locator pin, change the resolution of the raster, and move the legend to a custom position:\r\n\r\n\r\nbase_map_imp(color_intensity = 1, globe = TRUE, resolution = 500,\r\n             legend_x = .17, legend_y = .12)\r\n\r\n\r\n\r\nIf you think the legend is not need, there is also an option called \"none\". (The default is \"bottom\". You can also use of the predefined setting \"top\" as illustrated below or a custom position as shown in the previous example.)\r\nBerlin Data Sets\r\nThe package contains several data sets for Berlin. All of them start with sf_, e.g. d6berlin::sf_roads. Here is a full overview of the data sets that are available:\r\n\r\n\r\n\r\nAdding Locations to the Map\r\nLet’s assume you have recorded some animal locations or you want to plot another information on to of this plot. For example, let’s visualize the Berlin metro stations by adding geom_sf(data = x) to the template map:\r\n\r\n\r\nlibrary(ggplot2)\r\nlibrary(sf)\r\n\r\nmap <- base_map_imp(color_intensity = .4, resolution = 500, legend = \"top\")\r\n\r\nmap + geom_sf(data = sf_metro) ## sf_metro is contained in the {d6berlin} package\r\n\r\n\r\n\r\nNote: Since the template map contains many filled areas, we recommend to add geometries with variables mapped to color|colour|col to the template maps.\r\nYou can, of course, style the appearance of the points as usual:\r\n\r\n\r\nmap + geom_sf(data = sf_metro, shape = 8, color = \"red\", size = 2)\r\n\r\n\r\n\r\nIt is also possible to filter the data inside the geom_sf function — no need to use subset:\r\n\r\n\r\nlibrary(dplyr) ## for filtering\r\nlibrary(stringr) ## for filtering based on name\r\n\r\nmap + \r\n  geom_sf(data = filter(sf_metro, str_detect(name, \"^U\")), \r\n          shape = 21, fill = \"dodgerblue\", size = 2) +\r\n  geom_sf(data = filter(sf_metro, str_detect(name, \"^S\")), \r\n          shape = 21, fill = \"forestgreen\", size = 2)\r\n\r\n\r\n\r\nYou can also use the mapping functionality of ggplot2 to address variables from your data set.\r\n\r\n\r\nmap + \r\n  geom_sf(data = sf_metro, aes(color = type), size = 2) +\r\n  scale_color_discrete(type = c(\"forestgreen\", \"dodgerblue\"), \r\n                       name = NULL) +\r\n  guides(color = guide_legend(direction = \"horizontal\",\r\n                              title.position = \"top\", \r\n                              title.hjust = .5))\r\n\r\n\r\n\r\n(It looks better if you style the legend in the same horizontal layout.)\r\nCustom Styling\r\nSince the output is a ggplot object, you can manipulate the result as you like (but don’t apply a new theme, this will mess up the legend design):\r\n\r\n\r\nlibrary(systemfonts) ## for title font\r\n\r\nbase_map_imp(color_intensity = 1, resolution = 250, globe = TRUE,\r\n             legend_x = .17, legend_y = .12) + \r\n  geom_sf(data = sf_metro, shape = 21, fill = \"white\", \r\n          stroke = .4, size = 4) +\r\n  ggtitle(\"Metro Stations in Berlin\") + \r\n  theme(plot.title = element_text(size = 30, hjust = .5, family = \"Bangers\"),\r\n        panel.grid.major = element_line(color = \"white\", linewidth = .3),\r\n        axis.text = element_text(color = \"black\", size = 8),\r\n        plot.background = element_rect(fill = \"#fff0de\", color = NA),\r\n        plot.margin = margin(rep(20, 4)))\r\n\r\n\r\n\r\nSave Map\r\nUnfortunately, the size of the text elements is fixed. The best aspect ratio to export the map is 12x9 and you can save it with ggsave() for example:\r\n\r\n\r\nggsave(\"metro_map.pdf\", width = 12, height = 9, device = cairo_pdf)\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "https://raw.githubusercontent.com/EcoDynIZW/EcoDynIZW.github.io/main/img/wiki/hex-d6berlin.png",
    "last_modified": "2023-10-19T12:55:42+02:00",
    "input_file": {}
  },
  {
    "path": "posts/d6package/",
    "title": "Simplify Workflows of D6 Research Projects",
    "description": "Learn how to use the {d6} package to follow the project workflow within the department “Ecological Dynamics” at the Leibniz Institute for Zoo and Wildlife Research. The package functionality allows you to set up a standardized folder structure, to use templates for standardized reports, and provides a corporate theme for ggplot2 and some helpful utility functions.",
    "author": [
      {
        "name": "Cedric Scherer",
        "url": "https://cedricscherer.com"
      }
    ],
    "date": "2020-12-09",
    "categories": [
      "tutorial",
      "rstats",
      "workflow",
      "data management"
    ],
    "contents": "\r\n\r\nContents\r\nInstallation\r\nCreate Project Directory\r\n\r\n\r\n\r\n\r\nInstall Common Packages\r\nCorporate ggplot2 Theme\r\nTypefaces\r\nAdditional Utility Arguments\r\n\r\nUse Custom Rmarkdown Templates(\r\nRender Rmarkdown Files to Reports\r\nInstall and Load a Set of Packages\r\nAcknowledgements:\r\n\r\n\r\n\r\nThe {d6} package aims to simplify workflows of our D6 research projects by providing a standardized folder structure incl. version control, Rmarkdown templates, and other utilities.\r\nThere are five main functionalities:\r\nCreate standardized project directories with new_project()\r\nInstall a set of common packages with install_d6_packages()\r\nCreate figures that match our lab identity via theme_d6()\r\nProvide custom Rmarkdown templates via File > New File > Rmarkdown... > From Template\r\nRender all your Rmarkdown documents to ./docs/report with render_all_reports() or render_report()\r\nThe function simple_load() is a utility function that is currently in an experimental state. It allows you to install (if not yet) and load a set of packages—even a combination of CRAN and GitHub packages—in a single step.\r\n\r\n\r\nInstallation\r\nThe package is not on CRAN and needs to be installed from GitHub. To do\r\nso, open Rstudio and run the following two lines in the console. In case\r\nthe {remotes} package is already installed, skip that step.\r\n\r\n\r\ninstall.packages(\"remotes\")\r\nremotes::install_github(\"EcoDynIZW/d6\")\r\n\r\n\r\n(Note: If you are asked if you want to update other packages either\r\npress “No” (option 3) and continue or update the packages before running\r\nthe install command again.)\r\n\r\n\r\nCreate Project Directory\r\nRun the function new_project() to create a new project. This will\r\ncreate a standardized directory with all the scaffolding we use for all\r\nprojects in our department. It also add several files needed for\r\ndocumentation of your project.\r\nTo start a new project in the current working directory, simply run:\r\n\r\n\r\nd6::new_project(\"unicornus_wl_sdm_smith_j\")\r\n\r\n\r\nPlease give your project a unique and descriptive name:\r\nspecies_country_topic_name\r\nFor example, when John Smith is developing a species distribution models\r\nfor unicorns in Wonderland, a descriptive title could be:\r\nunicornus_wl_sdm_smith_j. Please use underscores and the\r\ninternational Alpha-2 encoding for\r\ncountries.\r\nThe main folders created in the root folder (here\r\nunicornus_wl_sdm_smith_j) are the following:\r\n\r\n.\r\n└── unicornus_wl_sdm_smith_j\r\n    ├── data\r\n    ├── docs\r\n    ├── output\r\n    ├── plots\r\n    └── scripts\r\n\r\nThe full scaffolding structure including all sub directories and\r\nadditional files looks like this:\r\n\r\n. \r\n└── unicornus_wl_sdm_smith_j\r\n    ├── .Rproj.user          —  Rproject files\r\n    ├── data                 —  main folder data\r\n    │    ├── geo             —  main folder spatial data (if `geo = TRUE`)\r\n    │    │    ├── processed  —  processed spatial data files\r\n    │    │    └── raw        —  raw spatial data files\r\n    │    ├── processed       —  processed tabular data files\r\n    │    └── raw             —  raw tabular data files\r\n    ├── docs                 —  documents main folder\r\n    │   ├── admin            —  administrative docs, e.g. permits \r\n    │   ├── literature       —  literature used for parametrization + manuscript\r\n    │   ├── manuscript       —  manuscript drafts (main + supplement)\r\n    │   ├── presentations    —  talks and poster presentations\r\n    │   └── reports          —  rendered reports\r\n    ├── output               —  everything that is computed (except plots)\r\n    ├── plots                —  plot output\r\n    ├── scripts              —  script files (e.g. .R, .Rmd, .Qmd, .py, .nlogo)\r\n    │   ├── 00_start.R       —  first script to run after project setup\r\n    │   └── zz_submit.R      —  final script to run before submission\r\n    ├── .gitignore           —  contains which files to ignore for version control\r\n    ├── .Rbuildignore        —  contains which files to ignore for package builds\r\n    ├── DESCRIPTION          —  contains project details and package dependencies\r\n    ├── NAMESPACE            —  contains context for R objects\r\n    └── project.Rproj        —  Rproject file: use to start your project\r\n\r\n\r\nUse A Custom Root Directory\r\nYou don’t need to change the working directory first—you can also\r\nspecify a path to a custom root folder in which the new project folder\r\nis created:\r\n\r\n\r\n## both work:\r\nd6::new_project(\"unicornus_wl_sdm_smith_j\", path = \"absolute/path/to/the/root/folder\")\r\n## or:\r\nd6::new_project(\"unicornus_wl_sdm_smith_j\", path = \"absolute/path/to/the/root/folder/\")\r\n\r\n\r\nThe resulting final directory of your project would be\r\nabsolute/path/to/the/root/folder/unicornus_wl_sdm_smith_j.\r\n\r\nUse Version Control\r\nIf you want to create a GitHub repository for the project at the same\r\ntime, use instead:\r\n\r\n\r\nd6::new_project(\"unicornus_wl_sdm_smith_j\", github = TRUE)\r\n\r\n\r\nBy default, the visibility of the GitHub repository is set to “private”\r\nbut you can also change that:\r\n\r\n\r\nd6::new_project(\"unicornus_wl_sdm_smith_j\", github = TRUE, private_repo = FALSE)\r\n\r\n\r\nNote that to create a GitHub repo you will need to have configured your\r\nsystem as explained\r\nhere.\r\n\r\nSetup without Geo Directories\r\nIf your project does not (or will not) contain any spatial data, you can\r\nprevent the creation of the directories geo-raw and geo-proc by\r\nsetting geo to FALSE:\r\n\r\n\r\nd6::new_project(\"unicornus_wl_sdm_smith_j\", geo = FALSE)\r\n\r\n\r\n\r\nAdd Documentation to Your Project\r\nAfter you have set up your project directory, open the file 00_start.R\r\nin the R folder. Add the details of your project, fill in the readme,\r\nadd a MIT license (if needed) and add package dependencies.\r\n\r\n\r\nInstall Common Packages\r\nYou can install the packages that are most commonly used in our\r\ndepartment via install_d6_packages():\r\n\r\n\r\nd6::install_d6_packages()\r\n\r\n\r\nNote that this function is going to check pre-installed versions and will\r\nonly install packages that are not installed with your current R\r\nversion.\r\nAgain, there is an argument geo so you can decide if you want to\r\ninstall common geodata packages as well (which is the default). If you\r\nare not intending to process geodata, set geo to FALSE:\r\n\r\n\r\nd6::install_d6_packages(geo = FALSE)\r\n\r\n\r\nThe default packages that are going to be installed are:\r\n\r\ntidyverse (tibble, dplyr, tidyr, ggplot2, readr, forcats, stringr, purrr), lubridate, here, vroom, patchwork, remotes\r\n\r\nThe following packages will be installed in case you specify\r\ngeo = TRUE:\r\n\r\nsf, terra, stars, tmap\r\n\r\n\r\n\r\nCorporate ggplot2 Theme\r\nThe package provides a ggplot2 theme with sensible defaults and additional\r\nutilities to simplify the process of creating a good-looking, clean look.\r\nFurthermore, we aim to have a consistent look across all our figures shown\r\nin manuscripts, presentations, and posters.\r\nThe theme can be added to a ggplot object as usual:\r\n\r\n\r\nlibrary(ggplot2)\r\nggplot(mpg, aes(x = displ, y = cty)) +\r\n  geom_point() +\r\n  d6::theme_d6()\r\n\r\n\r\nOr set as the new global theme by overwriting the current default:\r\n\r\n\r\ntheme_set(d6::theme_d6())\r\n\r\n\r\nTypefaces\r\nThe D6 corporate theme uses the PT font super family and will inform you to\r\ninstall the relevant files in case they are missing on your machine:\r\nPT Sans\r\nPT Serif\r\nPT Mono\r\nBy default, the theme uses PT Sans. If you prefer serif fonts or such a\r\ntypeface is required, you can set serif = TRUE inside the theme:\r\n\r\n\r\nggplot(mpg, aes(x = displ, y = cty)) +\r\n  geom_point() +\r\n  d6::theme_d6(serif = TRUE)\r\n\r\n\r\nAdditional Utility Arguments\r\nIn addition to the common arguments to specify the base settings\r\n(base_family, base_size, base_line_size, and base_rect_size),\r\nwe have added the following utility settings to simplify the modification\r\nof the theme:\r\ngrid: remove or add major grid lines (\"xy\" by default)\r\nlegend: control legend position (\"bottom\" by default)\r\nmono: use a tabular, mono spaced font for numeric scales such as x, y, and legends (\"none\" by default)\r\nbg: define background color (\"transparent\" by default)\r\nserif: set main typeface (FALSE by default; see above)\r\n\r\n\r\nggplot(mpg, aes(x = class, y = hwy, color = factor(year))) + \r\n  geom_boxplot() +\r\n  d6::theme_d6(\r\n    grid = \"y\",\r\n    legend = \"top\",\r\n    mono = \"yl\",\r\n    bg = \"cornsilk\"\r\n  )\r\n\r\n\r\n\r\n\r\nUse Custom Rmarkdown Templates(\r\nThe package also provides several templates for your scripts. In\r\nRstudio, navigate to File > New File > RMarkdown... > Templates and\r\nchoose the template you want to use. All templates come with a\r\npre-formatted YAML header and chunks for the setup.\r\nThe following templates are available for now:\r\nEcoDynIZW Basic: Template for a basic Rmarkdown research report\r\nincluding bits of codes and comments to get started\r\nEcoDynIZW Minimal: Template for an Rmarkdown research report\r\n(almost empty)\r\n\r\n\r\nRender Rmarkdown Files to Reports\r\nThe render_*() functions take care of knitting your Rmarkdown files\r\ninto HTML reports. The functions assume that your .Rmd files are saved\r\nin the R directory or any sub directory, and will store the resulting\r\n.html files in the according directory, namely ./docs/reports/.\r\nYou can render all .Rmd files that are placed in the R directory and\r\nsub directories in one step:\r\n\r\n\r\nd6::render_all_reports()\r\n\r\n\r\nYou can also render single Rmarkdown documents via render_report():\r\n\r\n\r\nd6::render_report(\"my-report.Rmd\")\r\nd6::render_report(\"notsurewhybutIhaveasubfolder/my-report.Rmd\")\r\n\r\n\r\n\r\n\r\nInstall and Load a Set of Packages\r\nThe simple_load() function takes a vector of packages, checks if they\r\nare installed already, installs them if needed, and loads them via\r\nlibrary() afterward. You can provide both, CRAN and GitHub packages,\r\nat the same time. GitHub packages need to be specified as\r\n“owner/repository” without any spaces.\r\n\r\n\r\nd6::simple_load(pcks = c(\"dplyr\", \"ggplot2\", \"EcoDynIZW/d6berlin\"))\r\n\r\n\r\nYou can also force a re-installation of packages. CRAN and GitHub packages\r\nare controlled individually via update_cran and update_gh, respectively.\r\n\r\n\r\nd6::simple_load(pcks = c(\"dplyr\", \"ggplot2\", \"EcoDynIZW/d6berlin\"),\r\n                update_cran = TRUE, update_gh = TRUE)\r\n\r\n\r\n\r\n\r\nAcknowledgements:\r\nThis package would not exist without the work of many great people!\r\nThe code to create the project folder is based on the template\r\npackage by Francisco\r\nRodriguez-Sanchez (and\r\nreferences therein)\r\nThe 00_start script is inspired by the {golem}\r\npackage\r\nRstudio for the development of Rmarkdown and all the great things\r\nthat come with it (knitting, templates, themes, …)\r\nAttribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "https://raw.githubusercontent.com/EcoDynIZW/EcoDynIZW.github.io/main/img/wiki/hex-d6.png",
    "last_modified": "2023-12-21T08:22:03+01:00",
    "input_file": {}
  }
]
