[
  {
    "path": "posts/d6geodatapackage/",
    "title": "d6geodata - a package to manage spatial data in the Ecodyn Department of the IZW",
    "description": "Learn how to use the {d6geodata} R package that provides functions for accessing data from the Geodata archive of the d6 department. The two functions `geo_overview()` and `get_geodata()` are the main components for all members of the EcoDyn Department. Several other functions are within this package but only meant to be used by the Geodata Manager of the Department.",
    "author": [
      {
        "name": "Moritz Wenzler-Meya",
        "url": {}
      }
    ],
    "date": "2023-02-21",
    "categories": [
      "tutorial",
      "spatial"
    ],
    "contents": "\r\nThe {d6geodata} package aims to access the data from the Geodata archive of the EcoDyn Department for members only!\r\nThe two main functions are:\r\nget_overview()\r\nget_geodata()\r\n\r\n\r\n## devtools::install_github(\"EcoDynIZW/d6geodata\")\r\n## library(d6geodata)\r\n\r\n\r\nIf you want to get geodata we already have in our Geodata archive you have two options: go on the EcoDyn Website, click on wikis and select Geodata. There you’ll find several raster layers and vector data with plots and metadata. In the metadata section, you’ll find the folder_name. You can copy this and use this together with get_geodata() function to get the data from our PopDynCloud. Another option is the function called get_overview(). There you can select which data and from which location you want to have a list of data.\r\nIf you run the function get_overview you have to decide if you want to see the raw or processed data by typing 1 for raw and 2 for processed data. Afterwards, you have to decide if you want to see the main (type 1) folders (the regions or sub-regions we have data from) or the sub (type 2) folders (the actually data we have in each region).\r\nExample 1: main folder\r\n\r\nd6geodata::get_overview(path_to_cloud = \"E:/PopDynCloud\")\r\nRaw or processed data: \r\n\r\n1: raw\r\n2: processed\r\n\r\nAuswahl: 2\r\nchoose folder type: \r\n\r\n1: main\r\n2: sub\r\n\r\nAuswahl: 1\r\n[1] \"atlas\" \"BB_MV_B\" \"berlin\" \"europe\" \"germany\" \"world\"\r\n\r\nExample 2: sub folder\r\n\r\nd6geodata::get_overview(path_to_cloud = \"E:/PopDynCloud\")\r\nRaw or processed data: \r\n\r\n1: raw\r\n2: processed\r\n\r\nAuswahl: 2\r\nchoose folder type: \r\n\r\n1: main\r\n2: sub\r\n\r\nAuswahl: 2\r\n$atlas\r\n[1] \"distance-to-human-settlements_atlas_2009_1m_03035_tif\"\r\n[2] \"distance-to-kettleholes_atlas_2022_1m_03035_tif\"      \r\n[3] \"distance-to-rivers_atlas_2009_1m_03035_tif\"           \r\n[4] \"distance-to-streets_atlas_2022_1m_03035_tif\"          \r\n[5] \"landuse_atlas_2009_1m_03035_tif\"                      \r\n\r\n$BB_MV_B\r\n[1] \"_archive\" \"_old_not_verified\" \"dist_path_bb_agroscapelabs\"\r\n[4] \"scripts\"                   \r\n\r\n$berlin\r\n [1] \"_old_not_verified\"                            \r\n [2] \"corine_berlin_2015_20m_03035_tif\"            \r\n [3] \"distance-to-paths_berlin_2022_100m_03035_tif\" \r\n [4]  \"green-capacity_berlin_2020_10m_03035_tif\"    \r\n [5] \"imperviousness_berlin_2018_10m_03035_tif\"     \r\n [6]  \"light-pollution_berlin_2021_100m_03035_tif\"  \r\n [7] \"light-pollution_berlin_2021_10m_03035_tif\"    \r\n [8]  \"motorways_berlin_2022_100m_03035_tif\"        \r\n [9] \"noise-day-night_berlin_2017_10m_03035_tif\"    \r\n[10]  \"population-density_berlin_2019_10m_03035_tif\"\r\n[11] \"template-raster_berlin_2018_10m_03035_tif\"    \r\n[12] \"tree-cover-density_berlin_2018_10m_03035_tif\"\r\n\r\n$europe\r\n[1] \"imperviousness_europe_2018_10m_03035_tif\"\r\n\r\n$germany\r\n [1] \"_old_not_verified\"                                          \r\n [2] \"distance-to-motorway-rural-road_germany_2022_100m_03035_tif\"\r\n [3] \"distance-to-motorways_germany_2022_100m_03035_tif\"          \r\n [4] \"distance-to-paths_germany_2022_100m_03035_tif\"              \r\n [5] \"distance-to-roads-paths_germany_2022_100m_03035_tif\"        \r\n [6] \"distance-to-roads_germany_2022_100m_03035_tif\"              \r\n [7] \"distance_to_paths_germany_2022_100m_03035_tif\"              \r\n [8] \"motoroways_germany_2022_03035_osm_tif\"                      \r\n [9] \"motorway-rural-road_germany_2022_100m_03035_tif\"            \r\n[10] \"motorways_germany_2022_100m_03035_tif\"                      \r\n[11] \"paths_germany_2022_100m_03035_tif\"                          \r\n[12] \"Roads-germany_2022_100m_03035_tif\"                          \r\n[13] \"roads_germany_2022_100m_03035_tif\"                          \r\n[14] \"tree-cover-density_germany_2015_100m_03035_tif\"             \r\n\r\n$world\r\ncharacter(0)\r\n\r\nNow you can copy the name of one of the layers and paste it into the get_geodata function\r\n\r\n\r\ncorine <-\r\n  d6geodata::get_geodata(\r\n    data_name = \"corine_berlin_2015_20m_03035_tif\",\r\n    path_to_cloud = \"E:/PopDynCloud\",\r\n    download_data = FALSE\r\n  )\r\n\r\n\r\nIf you set download_data = TRUE the data will be download and copied to your data-raw folder. If the data-raw folder doesn’t exist, it will create one.\r\nAdditional functions\r\nThe three functions plot_binary_map(), plot_qualitative_map() and plot plot_quantitative_map() can be used to plot raster data with the respective color sceams we used for the Geodata wiki page, but for raster data only!\r\n\r\n\r\nplot_binary_map(tif = tif)\r\nplot_qualitative_map(tif = tif)\r\nplot_quantitative_map(tif = tif)\r\n\r\n\r\nExample plot\r\n\r\n\r\nlibrary(d6geodata)\r\nplot_qualitative_map(tif = corine)\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/d6geodatapackage/d6geodatapackage_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2023-02-21T10:50:11+01:00",
    "input_file": "d6geodatapackage.knit.md"
  },
  {
    "path": "posts/d6berlin-fisbroker/",
    "title": "Download Spatial Data from the FIS-Broker Data Base in R",
    "description": "Learn how to download WFS (vector) data and ATOM (raster) data from the fisbroker data base using the our custom functions `download_fisbroker_wfs()` and `download_fisbroker_atom()`.",
    "author": [
      {
        "name": "Moritz Wenzler-Meya",
        "url": {}
      }
    ],
    "date": "2023-02-15",
    "categories": [
      "tutorial",
      "spatial"
    ],
    "contents": "\r\n\r\nContents\r\nThe FIS-Broker database\r\nWFS Data\r\nATOM Data\r\n\r\nThe {d6berlin} package provides several functions for the members of the Ecological Department of the IZW. Now two functions are added to the package: download_fisbroker_wfs() and download_fisbroker_atom().\r\n\r\n\r\n## devtools::install_github(\"EcoDynIZW/d6berlin\")\r\n## #install.packages(\"rcartocolor\")\r\n#install.packages(\"stars\")\r\n\r\n\r\n\r\n\r\nlibrary(d6berlin)\r\nlibrary(dplyr)\r\nlibrary(ggplot2)\r\n\r\n\r\nThe FIS-Broker database\r\nThe FIS-Broker database is hosted by the Berlin Senate and provides several geographical data sets. The file formats differ and some data sets have just one of the file formats to offer. The file formats are WMS (Web Media Service: Just like a png or jpg), WFS (Web Feature Service: Shapefiles) and ATOM (xml format: Raster layers data). This function is only looking for WFS files (Shapefiles), because these are the polygons, lines or points that we are looking for.\r\nFor using these two functions you have to select the layer you aim to download from the online data base.\r\nWFS Data\r\nAs an example we will download the layer containing the districts of Berlin (“ALKIS Bezirke”):\r\n\r\n\r\n\r\nurl <- \"https://fbinter.stadt-berlin.de/fb/wfs/data/senstadt/s_wfs_alkis_bezirk\"\r\n\r\ndata_wfs <- d6berlin::download_fisbroker_wfs(link = url)\r\n\r\nReading layer `s_wfs_alkis_bezirk' from data source \r\n  `https://fbinter.stadt-berlin.de/fb/wfs/data/senstadt/s_wfs_alkis_bezirk?service=wfs&version=2.0.0&request=GetFeature&typenames=s_wfs_alkis_bezirk&srsName=EPSG%3A25833' \r\n  using driver `GML'\r\nSimple feature collection with 12 features and 6 fields\r\nGeometry type: MULTIPOLYGON\r\nDimension:     XY\r\nBounding box:  xmin: 370000.8 ymin: 5799521 xmax: 415786.6 ymax: 5837259\r\nProjected CRS: ETRS89 / UTM zone 33N\r\n\r\nglimpse(data_wfs)\r\n\r\nRows: 12\r\nColumns: 7\r\n$ gml_id <chr> \"s_wfs_alkis_bezirk.1\", \"s_wfs_alkis_bezirk.2\", \"s_wf~\r\n$ gem    <int> 3, 9, 7, 2, 6, 11, 4, 10, 5, 12, 1, 8\r\n$ namgem <chr> \"Pankow\", \"Treptow-Köpenick\", \"Tempelhof-Schöneberg\",~\r\n$ namlan <chr> \"Berlin\", \"Berlin\", \"Berlin\", \"Berlin\", \"Berlin\", \"Be~\r\n$ lan    <int> 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11\r\n$ name   <int> 11000003, 11000009, 11000007, 11000002, 11000006, 110~\r\n$ geom   <MULTIPOLYGON [m]> MULTIPOLYGON (((391281.1 58..., MULTIPOLYGON (((41323~\r\n\r\nggplot() +\r\n  geom_sf(data = data_wfs, aes(fill = namgem)) +\r\n  rcartocolor::scale_fill_carto_d(palette = \"Bold\")\r\n\r\n\r\n\r\nYou got a spatial layer which you can save to disk or to use it directly.\r\nATOM Data\r\nAs an example we will download a raster of vegetation heights (“Vegetationshöhen 2020 (Umweltatlas)”):\r\n\r\n\r\n\r\nurl <- \"https://fbinter.stadt-berlin.de/fb/atom/Vegetationshoehen/veghoehe_2020.zip\"\r\n\r\ndata_atom <-\r\n  d6berlin::download_fisbroker_atom(\r\n    zip_link = url,\r\n    path = \"_posts/d6geodatapackage/man\",\r\n    name = \"vegetation_heights\"\r\n  )\r\n\r\nglimpse(data_atom)\r\n\r\nS4 class 'SpatRaster' [package \"terra\"]\r\n\r\ndata_atom_10 <- terra::aggregate(data_atom, 10)\r\n\r\nggplot() +\r\n  stars::geom_stars(data = stars::st_as_stars(data_atom_10)) +\r\n  coord_sf(expand = FALSE) + \r\n  rcartocolor::scale_fill_carto_c(\r\n    palette = \"Emrld\", name = NULL, \r\n    guide =  guide_legend(label.position = \"bottom\")\r\n  ) + \r\n  theme_void()\r\n\r\n\r\n\r\nA shortcut to plot this kind of data is the plot_qualitative_map() function from our dedicated d6geodata package. You can install this package with devtools::install_github(“EcoDynIZW/d6geodata”).\r\n\r\n\r\nd6geodata::plot_quantitative_map(tif = data_atom_10)\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2023-02-17T08:26:52+01:00",
    "input_file": {}
  },
  {
    "path": "posts/coding-goodpractice/",
    "title": "Coding Basics 1: General Tips",
    "description": "A brief introduction into various coding basics for people who are beginning to use R or other programming languages. In this session, we will be looking at some general tips and information that helps creating good and understandable code. Specifically we are looking at naming conventions, indentations and commenting of code.",
    "author": [
      {
        "name": "Tobias Kürschner",
        "url": {}
      }
    ],
    "date": "2023-02-13",
    "categories": [
      "tutorial",
      "coding"
    ],
    "contents": "\r\n\r\nContents\r\nGeneral:\r\nVariable Naming Conventions\r\nSnakecase:\r\nCamelcase:\r\nPascalcase:\r\nHungarian Notation:\r\nSidenote: Namespaces and Function Names\r\n\r\nIndentation\r\nCommenting Your Code\r\nDon’t Repeat Yourself\r\n\r\nGeneral:\r\nWrite as few lines as possible.\r\nUse appropriate naming conventions.\r\nSegment blocks of code in the same section into paragraphs.\r\nUse indentation to marks the beginning and end of control structures.\r\nDon’t repeat yourself.\r\n\r\n\r\n\r\nVariable Naming Conventions\r\nVariable naming is an important aspect in making your code readable. Create variables that describe their function and follow a consistent theme throughout your code. Separate words in a variable name without the use of whitespace and do not repeat variable names (unless in certain circumstances such as temporary variable within loops).\r\nSnakecase:\r\nWords are delimited by an underscore.\r\n\r\n\r\nvariable_one <- 1\r\n\r\nvariable_two <- 2\r\n\r\n\r\nCamelcase:\r\nWords are delimited by capital letters, except the initial word.\r\n\r\n\r\nvariableOne <- 1\r\n\r\nvariableTwo <- 2\r\n\r\n\r\nPascalcase:\r\nWords are delimited by capital letters.\r\n\r\n\r\nVariableOne <- 1\r\n\r\nVariableTwo <- 2\r\n\r\n\r\nHungarian Notation:\r\nThis notation describes the variable type or purpose at the start of the variable name, followed by a descriptor that indicates the variable’s function. The Camelcase notation is used to delimit words.\r\n\r\n\r\niVariableOne   <- 1  # i - integer\r\n\r\nsVariableTwo   <- \"two\" # s - string\r\n\r\nlVariableThree <- list() # l - list\r\n\r\n\r\nSidenote: Namespaces and Function Names\r\nOften when using for example R we will use libraries and many of them at the same time. Quite a few of those libraries are using the same names for their functions. This can be a big issues and cause code to suddenly not run anymore even though the only difference my be the order in which libraries are loaded. The last library loaded with a certain function will be the default one used by R. A prominent example would be the ‘raster’ package and ‘tidyverse’ (dplyr) who share the name ‘select’ for a function. So, when in doubt use namespaces to make sure to link a function to their library.\r\n\r\nuse the select function from the dplyr library\r\ndplyr::select(.....)\r\n\r\nIndentation\r\nI assume you already know that your code should have some sort of indentation. However, it’s also worth noting that its a good idea to keep your indentation style consistent.\r\nAs a quick reminder:\r\nBad:\r\nLong lines of text with no separation:\r\n\r\n\r\nmydata <- iris %>% dplyr::filter(Species == \"virginica\") %>% summarise_at(.vars = c(\"Sepal.Length\", \"Sepal.Width\"),.funs = \"mean\")\r\n\r\nfor (i in 1:length(hmpSplit)){ tmp1 <- as.data.frame(hmpSplit[[i]]$pm); colnames(tmp1) <- c(paste0(\"tab_\", i), 'pxcor', 'pycor', 'agent', 'breed'); tmp1 <- tmp1 %>% dplyr::select(-c('pxcor', 'pycor', 'agent', 'breed')); tempDf <- cbind(tempDf, tmp1)}\r\n\r\n\r\nGood:\r\n\r\n\r\nmydata <- iris %>%\r\n  dplyr::filter(Species == \"virginica\") %>%\r\n  summarise_at(.vars = c(\"Sepal.Length\", \"Sepal.Width\"),\r\n               .funs = \"mean\")\r\n\r\n\r\nfor (i in 1:length(hmpSplit))\r\n{\r\n  tmp1 <- as.data.frame(hmpSplit[[i]]$pm)\r\n  \r\n  colnames(tmp1) <- c(paste0(\"tab_\", i), 'pxcor', 'pycor', 'agent', 'breed')\r\n  \r\n  tmp1 <- tmp1 %>% \r\n    dplyr::select(-c('pxcor', 'pycor', 'agent', 'breed'))\r\n  \r\n  tempDf <- cbind(tempDf, tmp1)\r\n}\r\n\r\n\r\nThe details on how to indent or use whitesapces are up to individual styles with some guidelines, such as avoiding long lines of text, to keep in mind. Also fine would be something like the following.\r\n\r\n\r\nfor (i in 1:length(hmpSplit))\r\n{\r\n  tmp1 <- as.data.frame(hmpSplit[[i]]$pm)\r\n  colnames(tmp1) <- c(paste0(\"tab_\", i), 'pxcor', 'pycor', 'agent', 'breed')\r\n  tmp1 <- tmp1 %>% dplyr::select(-c('pxcor', 'pycor', 'agent', 'breed'))\r\n  tempDf <- cbind(tempDf, tmp1)\r\n}\r\n\r\n\r\nCommenting Your Code\r\nCommenting your code is fantastic but it can be overdone or just be plain redundant. Comments should add information or explanations to make your code understandable for people who didn’t write it or yourself in a year from now:\r\n\r\n\r\n# comment that adds information:\r\n\r\nwrite_simoutput(nl) #having nlrx write the output into a file - only works without patch or turtle metrics\r\n\r\n\r\n# redundant comments:\r\n\r\nif (col == \"blue\") # if colour is blue\r\n{\r\n  print('colour is blue') # print that the colour is blue\r\n}\r\n\r\n\r\nA better solution (if a comment is absolutely necessary) would be:\r\n\r\n\r\n# display selected colour\r\nif (colour == \"blue\")\r\n{\r\n  print('colour is blue')\r\n}\r\n\r\n\r\nDon’t Repeat Yourself\r\nAs a rule of thumb, if you have to do the same task multiple times in your code: automate it. A while back, I was decomposing many timeseries and I needed only part of the output, in this case the ‘trend’. Instead of running the same lines of code that remove all other components for each timeseries individually, a short function reduced the amount of needed code substantially.\r\n\r\n\r\nDecompTrend <- function(ts){\r\n  \r\n  temp1<-stats::decompose(ts)\r\n  return(temp1$trend)\r\n}\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2023-02-17T08:26:52+01:00",
    "input_file": {}
  },
  {
    "path": "posts/NLRX/",
    "title": "Run NetLogo Simulations in R",
    "description": "Learn how to use the {NLRS} R package to run NetLogo models/experiments with the NLRX library. This guide gives a brief overview of the basic functionality, how to quickly apply it to a model and shows some example (visual) outputs.",
    "author": [
      {
        "name": "Tobias Kürschner",
        "url": {}
      }
    ],
    "date": "2022-11-23",
    "categories": [
      "coding",
      "tutorial",
      "NetLogo",
      "R"
    ],
    "contents": "\r\n\r\nContents\r\nLibraries and Folders\r\nSetting Up the Library to Work with Your NetLogo Model\r\nCreating an Experiment\r\nCreating a Simulation\r\nRunning the Experiment\r\nNLRX output handling\r\n\r\nCleaning the data\r\nExample plots\r\n\r\nThe NLRX package provides tools to setup and execute NetLogo simulations from R developed by Salecker et al. 2019. NetLogo is a free, open-source and cross-platform modelling environment for simulating natural and social phenomena.\r\nLibraries and Folders\r\n\r\n\r\n# libraries\r\n\r\nfor (pckg in c('tidyverse', 'viridis', 'scico', 'nlrx'))\r\n{\r\n  if (!require(pckg, character.only = TRUE))\r\n    install.packages(pckg, dependencies = TRUE)\r\n  require(pckg, character.only = TRUE)\r\n}\r\n\r\n# output folder\r\n\r\ncurrentDate <- gsub(\"-\", \"\", Sys.Date())\r\n# todaysFolder <- paste(\"Output\", currentDate, sep = \"_\")\r\n# dir.create(todaysFolder) #in case output folder is wanted\r\n\r\n#virtual-ram if needed\r\n#memory.limit(85000)\r\n\r\n\r\nSetting Up the Library to Work with Your NetLogo Model\r\n\r\n\r\n#NetLogo path\r\nnetlogoPath <- file.path(\"C:/Program Files/NetLogo 6.2.2/\")\r\n\r\n#Model location\r\nmodelPath <- file.path(\"./Model/myModel.nlogo\")\r\n\r\n#Output location (hardly ever used)\r\noutPath <- file.path(\"./\")\r\n\r\n#Java\r\nSys.setenv(JAVA_HOME=\"C:/Program Files/Java/jre1.8.0_331\") \r\n\r\n\r\nConfigure the NetLogo object:\r\n\r\n\r\nnl <- nlrx::nl(\r\n  nlversion = \"6.2.2\",\r\n  nlpath = netlogoPath,\r\n  modelpath = modelPath,\r\n  jvmmem = 1024 # Java virtual machine memory capacity\r\n)\r\n\r\n\r\nCreating an Experiment\r\n\r\n\r\nnl@experiment <- nlrx::experiment(\r\n  expname = 'Exp_1_1',\r\n  # name\r\n  outpath = outPath,\r\n  # outpath\r\n  repetition = 1,\r\n  # number of times the experiment is repeated with the !!!SAME!!! random seed\r\n  tickmetrics = 'true',\r\n  # record metrics at every step\r\n  idsetup = 'setup',\r\n  # in-code setup function\r\n  idgo = 'go',\r\n  # in-code go function\r\n  runtime = 200,\r\n  # soft runtime-cap\r\n  metrics = c('population',  # global variables to be recorded, can also use NetLogos 'count' e.g. count turtles\r\n              'susceptible', # but requires escaped quotation marks for longer commands when strings are involved\r\n              'infected',    # functions similar for both patch and turtle variables (below)\r\n              'immune'),\r\n  metrics.patches = c('totalINfectionsHere',\r\n                      'pxcor',\r\n                      'pycor'),\r\n # metrics.turtles = list(\r\n #   \"turtles\" = c(\"xcor\", \"ycor\")\r\n # ),\r\n  constants = list(  # model parameters that are fixed. In theory all the constant values set in the UI before saving are the ones used\r\n    'duration' = 20, # however, I would always make sure to 'fix\" them though the constant input\r\n    'turtle-shape' = \"\\\"circle\\\"\",\r\n    'runtime' = 5\r\n  ),\r\n  variables = list( # model parameters you want to 'test' have several ways to be set\r\n    'number-people' = list(values = c(150)),     # simple list of values\r\n    'infectiousness' = list(                      #stepwise value change\r\n      min = 50,\r\n      max = 100,\r\n      step = 25,\r\n      qfun = 'qunif'\r\n    ),\r\n    # string based inputs such as used in NetLogo's 'choosers' or inputs require escaped quotation marks\r\n    # \"turtle-shape\" = list(values = c(\"\\\"circle\\\"\",\"\\\"person\\\"\")), \r\n    'chance-recover' = list(values = c(50, 75, 95))\r\n  )\r\n)\r\n\r\n\r\nCreating a Simulation\r\nHere we have several choices of simulation types. The full factorial simdesign used below creates a full-factorial parameter matrix with all possible combinations of parameter values. There are however, other options to choose from if needed and the vignette provides a good initial overview.\r\nA bit counter intuitive but nseeds used in the simdesign is actually the number of repeats we want to use for our simulation. In case we set nseed = 10 and have set the the repetitions above to 1, we would run each parameter combination with 10 different random seeds i.e. 10 times per combination. If we would have set the repetitions to 2 we would run each random seed 2 times i.e. 20 times in total but twice for each seed so 2 results should be identical.\r\nHowever, if your model handles seeds internally such as setting a random seed every time the model is setup, repetitions could be used instead.\r\n\r\n\r\nnl@simdesign <- nlrx::simdesign_ff(nl=nl, nseeds=1) \r\n\r\nprint(nl)\r\n\r\nnlrx::eval_variables_constants(nl)\r\n\r\n\r\nRunning the Experiment\r\nSince the simulations are executed in a nested loop where the outer loop iterates over the random seeds of the simdesign, and the inner loop iterates over the rows of the parameter matrix. These loops can be executed in parallel by setting up an appropriate plan from the future package which is built into nlrx.\r\n\r\n\r\n#plan(multisession, workers = 12) # one worker represents one CPU thread\r\n\r\nresults<- nlrx::run_nl_all(nl = nl)\r\n\r\n\r\nto speed up this this tutorial we load a pre-generated simulation result\r\n\r\n\r\nresults <- readr::read_rds(\"example1.rds\")\r\n\r\ndplyr::glimpse(results)\r\n\r\nRows: 1,809\r\nColumns: 15\r\n$ `[run number]`   <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1~\r\n$ `number-people`  <dbl> 150, 150, 150, 150, 150, 150, 150, 150, 150~\r\n$ infectiousness   <dbl> 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,~\r\n$ `chance-recover` <dbl> 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,~\r\n$ duration         <dbl> 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,~\r\n$ `turtle-shape`   <chr> \"circle\", \"circle\", \"circle\", \"circle\", \"ci~\r\n$ runtime          <dbl> 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5~\r\n$ `random-seed`    <dbl> -662923684, -662923684, -662923684, -662923~\r\n$ `[step]`         <dbl> 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1~\r\n$ population       <dbl> 0, 153, 155, 156, 156, 157, 158, 159, 160, ~\r\n$ susceptible      <dbl> 0, 10, 10, 10, 11, 11, 13, 13, 16, 16, 17, ~\r\n$ infected         <dbl> 0, 143, 145, 146, 145, 146, 145, 146, 144, ~\r\n$ immune           <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~\r\n$ metrics.patches  <list> [<tbl_df[1225 x 5]>], [<tbl_df[1225 x 5]>]~\r\n$ siminputrow      <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1~\r\n\r\nNLRX output handling\r\nThere are several ways to use / analyze the output directly via nlrx (although I have not used them personally) detailed information can be found here: https://cran.r-project.org/web/packages/nlrx/nlrx.pdf\r\n\r\n\r\nnlrx::setsim(nl, \"simoutput\") <- results # attaching simpout to our NetLogo object\r\n\r\n#write_simoutput(nl) # having nlrx write the output into a file only works without patch or turtle metrics\r\n\r\nnlrx::analyze_nl(nl, metrics = nlrx::getexp(nl, \"metrics\"), funs = list(mean = mean))\r\n\r\n\r\nCleaning the data\r\nNetLogo really dislikes outputting nice and readable variable names so some renaming is in order:\r\n\r\n\r\nraw1 <- results\r\n\r\nraw2 <-\r\n  raw1 %>% dplyr::rename(\r\n    run = `[run number]`,\r\n    recoveryChance = `chance-recover`,\r\n    StartingPop = `number-people`,\r\n    t = `[step]`\r\n  )\r\n\r\n\r\nIn case of very large datasets we want to separate the patch (or turtle) specific data from the global data to speed up the analysis of the global data\r\n\r\n\r\nglobalData <- raw2 %>% dplyr::select(!metrics.patches)#, !metrics.turtles)\r\n\r\n\r\nand summarise like any other dataset\r\n\r\n\r\nsum_1 <-\r\n  globalData %>%\r\n  dplyr::group_by(t, recoveryChance, infectiousness) %>%\r\n  dplyr::summarise(\r\n    across(\r\n      c('infected',\r\n        'immune',\r\n        'susceptible',\r\n        'population'),\r\n      mean\r\n    )\r\n  ) %>%\r\n  dplyr::filter(t < 201) %>%\r\n  tidyr::pivot_longer(cols = -c(recoveryChance, infectiousness, t),\r\n                      names_to = \"EpiStat\") %>%\r\n  dplyr::ungroup()\r\n\r\n\r\nExample plots\r\nWe use the global data to get an overview of the simulated SIR dynamics:\r\n\r\n\r\nggplot(sum_1) +\r\n  geom_line(aes(x = t, y = value, colour = EpiStat), size = 1) +\r\n  facet_grid(infectiousness ~ recoveryChance, labeller = label_both) +\r\n  scale_colour_scico_d(\r\n    palette = \"lajolla\",\r\n    begin = 0.1,\r\n    end = 0.9,\r\n    direction = 1\r\n  ) +\r\n  theme_bw()\r\n\r\n\r\n\r\n… and use the patch specific data to create a heatmap to visualize where most infections have happened in one specific scenario:\r\n\r\n\r\nhmpRaw <- raw2 %>%\r\n  dplyr::filter(infectiousness == 75 & recoveryChance == 75) %>%\r\n  dplyr::select(metrics.patches, t) %>%\r\n  dplyr::rename(pm = metrics.patches)\r\n\r\nhmpSplit <- hmpRaw %>% split(hmpRaw$t)\r\ntempDf <- hmpSplit[[1]]$pm \r\ntempDf <- tempDf %>% as.data.frame() %>%\r\n  dplyr::select(totalINfectionsHere, pxcor, pycor)\r\n\r\nfor (i in 1:length(hmpSplit))\r\n{\r\n  tmp1 <- as.data.frame(hmpSplit[[i]]$pm)\r\n  colnames(tmp1) <- c(paste0(\"tab_\", i), 'pxcor', 'pycor', 'agent', 'breed')\r\n  tmp1 <- tmp1 %>% \r\n    dplyr::select(-c('pxcor', 'pycor', 'agent', 'breed'))\r\n  tempDf <- cbind(tempDf, tmp1)\r\n}\r\n\r\ntempDf2 <- tempDf %>% \r\n  dplyr::select(-c(pxcor, pycor)) \r\n\r\ntempDf$summ <- rowSums(tempDf2)\r\nhmpData <- tempDf %>% dplyr::select(summ, pxcor, pycor)\r\n\r\nggplot(hmpData)+\r\n  geom_tile(aes(x = pxcor, y = pycor, fill = summ)) +\r\n  scale_fill_scico(palette = 'roma', direction = -1, name= 'N infected') +\r\n  ggtitle(\"N infections per cell\") +\r\n  theme_bw()\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2023-02-17T08:26:52+01:00",
    "input_file": {}
  },
  {
    "path": "posts/coding-basics/",
    "title": "Coding Basics 2: Loops and Functions",
    "description": "A brief introduction into various coding basics for people who are beginning to use R or other programming languages. In this session, we will be looking at both functions and loops in R, with examples from NetLogo and C++ and go over use and basic functionality.",
    "author": [
      {
        "name": "Tobias Kürschner",
        "url": {}
      }
    ],
    "date": "2022-11-14",
    "categories": [
      "tutorial",
      "coding"
    ],
    "contents": "\r\n\r\nContents\r\nFunctions and Loops\r\nWhat are they for?\r\nFunctions\r\nLoops\r\nA Classic, the ‘for’ Loop\r\n‘while’ Loops\r\n\r\nBonus Round: Conditionals\r\n\r\n\r\n\r\n\r\n\r\nFunctions and Loops\r\nWhat are they for?\r\nIn short: making you life easier. They are used to automate certain steps in your code to make the execution faster.\r\nFunctions\r\nA simple example: you have multiple data sets with a measurement in inches. To continue working with that data you need to convert it meter, but doing that manually would take hours. Solution: a function!!\r\nIn R:\r\n\r\n\r\ninch_to_meter <-   #function name\r\n  function(inch) #function input parameter(s)\r\n  { # function body\r\n    inM <- (inch * 0.0254) # in our case: transformation\r\n    \r\n    return(inM) # function reporter\r\n  }\r\n\r\n\r\nNow lets apply that function to some random example data:\r\n\r\n\r\nmyOldData <- c(15,98,102,5,17)\r\n\r\nmyNewData <- inch_to_meter(myOldData)\r\n\r\n\r\nOur function is applied to all elements of the old data, giving us the converted measurements.\r\n\r\n\r\nmyOldData\r\n\r\n[1]  15  98 102   5  17\r\n\r\nmyNewData\r\n\r\n[1] 0.3810 2.4892 2.5908 0.1270 0.4318\r\n\r\nFor a more general approach we can use the following template for simple functions:\r\n\r\n\r\nmyFunction <-\r\n  function(input1, input2, input3)\r\n  {\r\n    output <- input1 * input3 / input2 # example operation\r\n    \r\n    return(output)\r\n  }\r\n\r\n\r\nIn C++:\r\n\r\n# the viableCell function of the class Grid returns a bool and takes two inputs\r\n# (x and y coordinates)\r\nbool Grid::viableCell(CellCount_t x, CellCount_t y) \r\n{\r\n    return !m_grid.empty() && x < m_grid.cbegin() -> size() && y < m_grid.size();\r\n}\r\n\r\nIn NetLogo, the closest thing we have to functions are reporters:\r\n\r\nto-report Male_Alpha_Alive \r\n\r\n  ifelse (any? turtles-here with [isAlpha = true AND isFemale = false])\r\n  [report true]\r\n  [report false] #else\r\n\r\nend\r\n\r\nLoops\r\nWhat is a loop? It a simply a piece of code we want to repeat. But wait, isn’t that exactly what a function does? Well, yes and no. A function (after it is declared) is an embodiment of a piece of code that we can run anytime just by calling it. A loop is a local repetition of code.\r\nLets stay in R and look at some examples:\r\nA Classic, the ‘for’ Loop\r\nIf you auto fill “for” in R, it will give you the following structure:\r\n\r\n\r\nfor (variable in vector)\r\n{\r\n  #body\r\n}\r\n\r\n\r\nSo what is variable and what is vector? In this case the variable, you could also call iterator (often you will see the letter i used) is simply put a counter that tells our loop how many times it has repeated itself. The vector is determined by us and tells the loop how many repetitions we want before it stops.\r\n\r\n\r\nfor (i in 1:10)\r\n{\r\n  #body\r\n}\r\n\r\n\r\nThe loop above will now repeat exactly 10 times and then stop. R is helpful in the sense that it automatically increments i after each repetition. Other language like C++ for example need a manual increment of i:\r\n\r\nfor (i = 0, i <= 10, ++i) #C++ \r\n{\r\n  #body\r\n}\r\n\r\nWith the basics out of the way lets look at an example:\r\n\r\n\r\nmyData <- rnorm(30) # random numbers\r\nmyResults <- 0 # initializing myResults\r\n\r\nfor(i in 1:10)\r\n{\r\n  # i-th element of `myData` squared into `i`-th position of `myResults`\r\n  myResults[i] <- myData[i] * myData[i]  \r\n  print(i)\r\n}\r\n\r\n[1] 1\r\n[1] 2\r\n[1] 3\r\n[1] 4\r\n[1] 5\r\n[1] 6\r\n[1] 7\r\n[1] 8\r\n[1] 9\r\n[1] 10\r\n\r\nmyResults\r\n\r\n [1] 0.0001410016 0.6997562533 0.0092729159 0.1070337352 0.1392032186\r\n [6] 1.8664878519 0.2213922328 0.6309227824 1.5626483108 0.1790988143\r\n\r\nThis was of course a very simple loop and there is pretty much no limit to the level of complexity those loops can have and how many loops could be nested. But be warned there can be some pitfalls with loops. Exercise: Would the following loop work?\r\n\r\n\r\nfor (i in 1:length(summIdentSplit))\r\n{\r\n  tmpSumm <- summIdentSplit[[i]]\r\n  tmpName <- SummIdentVector[[i]]\r\n  \r\n  if (base::grepl(\"HD\", tmpName) == TRUE)\r\n  {\r\n    clearName <- \"Habitat-driven movement\"\r\n    cn <- \"HD\"\r\n  }\r\n  \r\n  combiMelt <- tmpSumm %>%\r\n    dplyr::ungroup() %>%\r\n    dplyr::select(t, Ninfected_mean, Nimmune_mean) %>%\r\n    dplyr::rename(Infected = Ninfected_mean, Immune = Nimmune_mean)\r\n  \r\n  combiMelt_sd <- tmpSumm %>%\r\n    dplyr::ungroup() %>%\r\n    dplyr::select(t, Ninfected_sd, Nimmune_sd) %>%\r\n    dplyr::rename(Inf_sd = Ninfected_sd, Imm_sd = Nimmune_sd)\r\n  \r\n  combiMelt_c <- dplyr::left_join(combiMelt, combiMelt_sd , by = \"t\")\r\n  \r\n  q <- 0\r\n  combiMelt_c$quarter <- 0\r\n  \r\n  for (i in 1:nrow(combiMelt_c))\r\n  {\r\n    if (i %% 13 == 0)\r\n    {\r\n      q <- q + 1\r\n    }\r\n    combiMelt_c$quarter[i] <- q\r\n  }\r\n}\r\n\r\n\r\nThe answer is no. There are two loops involved where one is nested inside the other. So far, that would not be an issue, however, both loops use the iterator (variable) i. To see what would happen:\r\n\r\nFirst iteration:\r\n  i = 1\r\n\r\nfor (i in 1:length(summIdentSplit))\r\n{\r\n  tmpSumm <- summIdentSplit[[i]]\r\n  tmpName <- SummIdentVector[[i]]\r\n  \r\n  if (base::grepl(\"HD\", tmpName) == TRUE)\r\n  {\r\n    clearName <- \"Habitat-driven movement\"\r\n    cn <- \"HD\"\r\n  }\r\n  \r\n  combiMelt <- tmpSumm %>%\r\n    dplyr::ungroup() %>%\r\n    dplyr::select(t, Ninfected_mean, Nimmune_mean) %>%\r\n    dplyr::rename(Infected = Ninfected_mean, Immune = Nimmune_mean)\r\n  \r\n  combiMelt_sd <- tmpSumm %>%\r\n    dplyr::ungroup() %>%\r\n    dplyr::select(t, Ninfected_sd, Nimmune_sd) %>%\r\n    dplyr::rename(Inf_sd = Ninfected_sd, Imm_sd = Nimmune_sd)\r\n  \r\n  combiMelt_c <- dplyr::left_join(combiMelt, combiMelt_sd , by = \"t\")\r\n  \r\n  q <- 0\r\n  combiMelt_c$quarter <- 0\r\n\r\nAt this point i is still 1 and lets say nrow(combiMelt_c) is also 10 (like in our outer loop)\r\n\r\n  for (i in 1:nrow(combiMelt_c))\r\n  {\r\n    if (i %% 13 == 0)\r\n    {\r\n      q <- q + 1\r\n    }\r\n    combiMelt_c$quarter[i] <- q\r\n\r\nat this point i is iterated within the inner loop\r\n\r\n  }\r\n\r\nonce we reach this point i is 10, so the for the next iteration of the outer\r\nloop, i = 10 so most iterations of the outer loop will be skipped!\r\n\r\n}\r\n\r\nSolution: make sure to use different iterators in nested loops. For example the outer loop uses i, the inner loop uses j and maybe that loop has also a nested loop which then uses k as its iterator.\r\n‘while’ Loops\r\nSimilar to for loops, while loops also repeat a certain block of code. The difference here is, that while loop repeat until a certain condition is fulfilled, potentially forever. R’s auto fill provides us the following code snippet:\r\n\r\n\r\nwhile (condition)\r\n{\r\n  \r\n}\r\n\r\n\r\nA simple example:\r\n\r\n\r\nn <- 0\r\n\r\nwhile(n < 100)\r\n{\r\n  n = n + 1\r\n}\r\n\r\nprint(n)\r\n\r\n[1] 100\r\n\r\nAs long as n is below 100 we increment n every repeat. Take note: if the condition is never fulfilled, the loop will run forever and the software might crash. In complex loops you could run a “escape timer” such as in this example plucked from an IBM:\r\n\r\n\r\nwhile(!viableCell())\r\n{\r\n   turn(2 * (atan (( (1 - 0.5) / (1 + 0.9)) * tan ((randomDouble(1) - 0.5)) * 180)) + 1) \r\n}\r\n\r\n\r\nThis little loop runs on an individual and check’s the cell in front of the individual for its viability to move into. As long as viableCell() reports false, the individual turns. However, there are cases when they are no viable cells around so the individual would turn in a circle forever. Introducing a random timer (note, this is not an optimal solution just a quick fix).\r\n\r\nint timer = 0\r\n\r\nwhile(!viableCell() && timer < 50 )\r\n{\r\n   turn(2 * (atan (( (1 - 0.5) / (1 + 0.9)) * tan ((randomDouble(1) - 0.5)) * 180)) + 1) \r\n  \r\n  ++timer #increment the timer\r\n}\r\n\r\nNow the individual turns a maximum of 50 times before the loop ends. Another option would be conditionals:\r\nBonus Round: Conditionals\r\nMost people already know what a conditional is. Basically, when a certain condition is fulfilled something happens (usually done via if and/or else).\r\n\r\nint timer = 0\r\n\r\nwhile(!viableCell())\r\n{\r\n   turn(2 * (atan (( (1 - 0.5) / (1 + 0.9)) * tan ((randomDouble(1) - 0.5)) * 180)) + 1) \r\n  \r\n  ++timer #increment the timer\r\n  \r\n  if(timer == 50)\r\n  {\r\n    break # break is a function that for example ends a loop\r\n  }\r\n}\r\n\r\nConditionals can be used in a variety of way within and outside of loops but have the advantage that they can be used stop loops under certain conditions. Lets say you are running a complex construct of multiple nested for loops to find a certain value in your data. You don’t need to always iterate through all the data if you can create certain logical stop conditions.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2023-02-17T08:26:52+01:00",
    "input_file": {}
  },
  {
    "path": "posts/netlogo-weibull/",
    "title": "Turning Equations into Functions in NetLogo",
    "description": "While some programming languages, such as R, offer many native functions, others (e.g., NetLogo) offer fewer built-in options. However, users can create their own functions easily. Here, we will show the example of a Weibull density distribution function & associated cumulative density distribution function—both not yet implemented in NetLogo or NetLogo extensions.",
    "author": [
      {
        "name": "Marius Grabow",
        "url": {}
      }
    ],
    "date": "2022-11-11",
    "categories": [
      "NetLogo",
      "rstats",
      "distributions"
    ],
    "contents": "\r\n\r\nContents\r\nDensity distribution function\r\nR\r\nNetLogo\r\n\r\nCumulative density function\r\nR\r\nNetLogo\r\n\r\n\r\nDensity distribution\r\nfunction\r\nLet’s first take a look at the Weibull density distribution\r\nfunction:\r\n\\[\\begin{equation}\r\n\r\nf(x) = \\frac{\\gamma} {\\alpha} (\\frac{x-\\mu}\r\n{\\alpha})^{(\\gamma - 1)}\\exp{(-((x-\\mu)/\\alpha)^{\\gamma})}\r\n\\hspace{.3in}  x \\ge \\mu; \\gamma, \\alpha > 0\r\n\r\n\\end{equation}\\]\r\nR\r\nIn R, this is already implemented:\r\n\r\n\r\nscale <- 3\r\nshape <- 1\r\n\r\ndweibull(scale, shape = shape)\r\n\r\n\r\n[1] 0.04978707\r\n\r\nNetLogo\r\nIn Netlogo we can simply translate the mathematical equation into a\r\nfunction:\r\nto-report weibull [a_scale a_shape x]\r\n\r\n  let Wei (a_shape / a_scale ) * ((x / a_scale)^(a_shape - 1)) * exp( - ((x / a_scale)^ a_shape))\r\n\r\n  report Wei\r\n\r\nend\r\nCumulative density function\r\nLet’s also take a look at the Weibull cumulative density\r\nfunction:\r\n\\[\\begin{equation}\r\n\r\nF(x) = 1 - e^{-(x^{\\gamma})} \\hspace{.3in}  x \\ge 0; \\gamma > 0\r\n\r\n\\end{equation}\\]\r\nR\r\nAgain, fully implemented in R already:\r\n\r\n\r\nscale <- 3\r\nshape <- 1\r\nx <- 5\r\n\r\npweibull(q = x, scale = scale, shape = shape)\r\n\r\n\r\n[1] 0.8111244\r\n\r\nNetLogo\r\nIn NetLogo we can simply translate the mathematical equation into a\r\nfunction:\r\nto-report weibull_cumulative [a_scale a_shape x]\r\n\r\n  let Wei_cumu 1 - exp( - ((x / a_scale)^ a_shape))\r\n\r\n  report Wei_cumu\r\n\r\nend\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2023-02-17T08:26:53+01:00",
    "input_file": {}
  },
  {
    "path": "posts/imageseg/",
    "title": "imageseg",
    "description": "R package for deep learning image segmentation using the U-Net model architecture by Ronneberger (2015), implemented in Keras and TensorFlow. It provides pre-trained models for forest structural metrics (canopy density and understory vegetation density) and a workflow to apply these on custom images",
    "author": [
      {
        "name": "Jürgen Niedballa",
        "url": {}
      }
    ],
    "date": "2022-09-12",
    "categories": [],
    "contents": "\r\nimageseg\r\nR package for deep learning image segmentation using the U-Net model architecture by Ronneberger (2015), implemented in Keras and TensorFlow. It provides pre-trained models for forest structural metrics (canopy density and understory vegetation density) and a workflow to apply these on custom images.\r\nIn addition, it provides a workflow for easily creating model input and model architectures for general-purpose image segmentation based on the U-net architecture. Model can be trained on grayscale or color images, and can provide binary or multi-class image segmentation as output.\r\nThe package can be found on CRAN.\r\nThe preprint of the paper describing the package is available on bioRxiv.\r\nInstallation\r\nFirst, install the R package “R.rsp” which enables the static vignettes.\r\n\r\n\r\n\r\nInstall the imageseg package from CRAN via:\r\n\r\n\r\n\r\nAlternatively you can install from GitHub (requires remotes package and R.rsp):\r\n\r\n\r\n\r\nUsing imageseg requires Keras and TensorFlow. See the vignette for information about installation and initial setup:\r\nTutorial\r\nSee the vignette for an introduction and tutorial to imageseg.\r\n\r\n\r\n\r\nThe vignette covers:\r\n- Installation and setup\r\n- Sample workflow for canopy density assessments\r\n- Training new models\r\n- Continued training of existing models\r\n- Multi-class image segmentation models\r\n- Image segmentation based on grayscale images\r\nForest structure model download\r\nThe models, example predictions, training data and R script for model training for both the canopy and understory model are available from Dryad as a single download.\r\nSee the “Usage Notes” section for details on the dataset.\r\nThe models and script (without the training data) are also hosted on Zenodo and can be downloaded individually from zenodo.\r\nThe pre-trained models for forest canopy density and understory vegetation density are available for download. The zip files contain the model (as .hdf5 files) and example classifications to give an impression of model performance and output:\r\nCanopy model\r\nUnderstory model\r\nPlease see the vignette for further information on how to use these models.\r\nTraining data download\r\nTraining data for both the canopy and understory model are included in the Dryad dataset download in the zip files:\r\nimageseg_canopy_training_data.zip\r\nimageseg_understory_training_data.zip\r\nFor details, please see the Usage Notes and the info.txt files contained in the zip files.\r\nThe training data are not required for users who only wish to use the pre-trained models on their own images.\r\n\r\n\r\n\r\n",
    "preview": "https://raw.githubusercontent.com/EcoDynIZW/EcoDynIZW.github.io/main/img/hex-imageseg.png",
    "last_modified": "2022-09-21T12:26:47+02:00",
    "input_file": {}
  },
  {
    "path": "posts/isorix/",
    "title": "Isorix",
    "description": "This is the development repository of IsoriX, an R package aiming at building isoscapes using mixed models and inferring the geographic origin of organisms based on their isotopic ratios",
    "author": [
      {
        "name": "Alexandre Courtiol",
        "url": {}
      }
    ],
    "date": "2022-09-12",
    "categories": [],
    "contents": "\r\nHow to download and install IsoriX?\r\nYou can download and install the stable version of IsoriX directly from within R by typing:\r\n\r\n\r\ninstall.packages(\"IsoriX\", dependencies = TRUE)\r\n\r\n\r\nNote: if you get into troubles due to gmp, magick, maps, maptools, RandomFields, rgeos, or rgl, retry using simply:\r\n\r\n\r\ninstall.packages(\"IsoriX\")\r\n\r\n\r\nThese packages offer additional functionalities but some of them are particularly difficult to install on some systems.\r\nIf you want the development version of IsoriX, you can download and install it by typing:\r\n\r\n\r\nremotes::install_github(\"courtiol/IsoriX/IsoriX\")\r\n\r\n\r\nMind that you need the R package remotes to be installed for that to work. Mind also that the development version, being under development, can sometimes be broken. So before downloading it make sure that the current build status is build passing. The current built status is provided at the top of this readme document.\r\nAlso, if you access the network via a proxy, you may experience troubles with install_github. In such case try something like:\r\n\r\n\r\nlibrary(httr)\r\nwith_config(use_proxy(\"192.168.2.2:3128\"), devtools::install_github(\"courtiol/IsoriX/IsoriX\"))\r\n\r\n\r\nOff course, unless you are in the same institute than some of us, replace the numbers with your proxy settings!\r\nWhere to learn about IsoriX?\r\nYou can start by reading our bookdown!\r\nThen, if may not be a bad idea to also have a look at our papers: here and there.\r\nAnother great source of help is our mailing list. First register for free (using your Google account) and then feel free to send us questions.\r\nFor specific help on IsoriX functions and objects, you should also check the documentation embedded in the package:\r\n\r\n\r\nhelp(package = \"IsoriX\")\r\n\r\n\r\nin R after having installed and attached (= loaded) the package.\r\nHow can you contribute?\r\nThere are plenty way you can contribute! If you are fluent in R programming, you can improve the code and develop new functions. If you are not so fluent, you can still edit the documentation files to make them more complete and clearer, write new vignettes, report bugs or make feature requests.\r\nSome useful links\r\n\r\n\r\n\r\n",
    "preview": "https://raw.githubusercontent.com/EcoDynIZW/EcoDynIZW.github.io/main/img/hex-isorix.png",
    "last_modified": "2022-09-21T12:26:47+02:00",
    "input_file": {}
  },
  {
    "path": "posts/camtrapr/",
    "title": "Manage Camera Trap Data in R",
    "description": "Learn how to use the {camtrapR} R package for streamlined and flexible camera trap data management. It should be most useful to researchers and practitioners who regularly handle large amounts of camera trapping data.",
    "author": [
      {
        "name": "Jürgen Niedballa",
        "url": {}
      }
    ],
    "date": "2022-05-23",
    "categories": [
      "tutorial",
      "rstats",
      "camera traping",
      "data management"
    ],
    "contents": "\r\n\r\nContents\r\nInstallation\r\nExiftool\r\n\r\nThe {camtrapR} R\r\npackage simplifies camera trap data management in R.\r\nInstallation\r\nYou can install the release version of {camtrapR} from\r\nCRAN:\r\n\r\n\r\ninstall.packages(\"camtrapR\")\r\n\r\n\r\n\r\nExiftool\r\nNumerous important {camtrapR} functions read EXIF\r\nmetadata from JPG images (and videos). This is done via\r\nExiftool, a free and open-source sofware tool developed by\r\nPhil Harvey and available for Windows, MacOS and Linux.\r\nTo make full use of {camtrapR}, you will need\r\nExiftool on your system. You can download it from the\r\nExiftool homepage and follow the installation instruction\r\nin vignette 1.\r\nYou may not need Exiftool if you do not work with image\r\nfiles, but only use {camtrapR} to create input for\r\noccupancy or spatial capture-recapture models from existing record\r\ntables.\r\nSee the article\r\nin Methods in Ecology and Evolution for an overview of the package.\r\nThe five vignettes provide examples for the entire workflow.\r\nCitation\r\n\r\n\r\n\r\n",
    "preview": "https://raw.githubusercontent.com/EcoDynIZW/EcoDynIZW.github.io/main/img/hex-camtrapr.png",
    "last_modified": "2022-11-14T10:21:25+01:00",
    "input_file": {}
  },
  {
    "path": "posts/nlmr/",
    "title": "Simulate Neutral Landscape Models",
    "description": "Learn how to use the {NLMR} R package to simulate neutral landscape models (NLM), a common set of various null models for spatial analysis and as input for spatially-explicit, generic models.",
    "author": [
      {
        "name": "Cédric Scherer",
        "url": "htttps://cedricscherer.com"
      }
    ],
    "date": "2021-08-06",
    "categories": [
      "tutorial",
      "rstats",
      "spatial",
      "modelling"
    ],
    "contents": "\r\nThe {NLMR} R package to simulate neutral landscape models (NLM). Designed to be a generic framework like NLMpy, it leverages the ability to simulate the most common NLM that are described in the ecological literature.\r\nIf you want to learn more about the {NLMR} package and the accompanying {landscapetools} package, check the publication Sciaini, Fritsch, Scherer 6 Simpkins (2019) Methods in Ecology and Evolution.\r\nInstallation\r\nInstall the release version from CRAN:\r\n\r\n\r\ninstall.packages(\"NLMR\")\r\n\r\n\r\nUsage\r\nEach neutral landscape models is simulated with a single function (all starting with nlm_) in NLMR, e.g.:\r\n\r\n\r\nrandom_cluster <- NLMR::nlm_randomcluster(\r\n  nrow = 100,\r\n  ncol = 100,\r\n  p    = 0.5,\r\n  ai   = c(0.3, 0.6, 0.1),\r\n  rescale = FALSE\r\n)\r\n\r\nrandom_curdling <- NLMR::nlm_curds(\r\n  curds = c(0.5, 0.3, 0.6),\r\n  recursion_steps = c(32, 6, 2)\r\n)\r\n\r\nmidpoint_displacement <- NLMR::nlm_mpd(\r\n  ncol = 100,\r\n  nrow = 100,\r\n  roughness = 0.61\r\n)\r\n\r\n\r\n{NLMR} supplies 15 NLM algorithms, with several options to simulate derivatives of them. The algorithms differ from each other in spatial auto-correlation, from no auto-correlation (random NLM) to a constant gradient (planar gradients).\r\nThe package builds on the advantages of the raster package and returns all simulation as RasterLayer objects, thus ensuring a direct compatibility to common GIS tasks and a flexible and simple usage:\r\n\r\n\r\nclass(random_cluster)\r\n\r\n[1] \"RasterLayer\"\r\nattr(,\"package\")\r\n[1] \"raster\"\r\n\r\nrandom_cluster\r\n\r\nclass      : RasterLayer \r\ndimensions : 100, 100, 10000  (nrow, ncol, ncell)\r\nresolution : 1, 1  (x, y)\r\nextent     : 0, 100, 0, 100  (xmin, xmax, ymin, ymax)\r\ncrs        : NA \r\nsource     : memory\r\nnames      : clumps \r\nvalues     : 1, 3  (min, max)\r\n\r\nVisualization\r\nThe {landscapetools} package provides a function show_landscape that was developed to plot raster objects and help users to adhere to some standards concerning color scales and typography. This means for example that by default the viridis color scale is applied (and you can pick others from the {viridis} package, too).\r\n\r\n\r\n#install.packages(\"landscapetools\")\r\n\r\n## plotting continuous values\r\nlandscapetools::show_landscape(random_cluster)\r\n\r\n\r\n## plotting discrete values\r\nlandscapetools::show_landscape(random_curdling, discrete = TRUE)\r\n\r\n\r\n## using another viridis palette\r\nlandscapetools::show_landscape(midpoint_displacement, viridis_scale = \"magma\")\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "https://raw.githubusercontent.com/EcoDynIZW/EcoDynIZW.github.io/main/img/hex-nlmr.png",
    "last_modified": "2023-02-17T08:26:53+01:00",
    "input_file": {}
  },
  {
    "path": "posts/d6berlinpackage/",
    "title": "Spatial Datasets and Template Maps for Berlin",
    "description": "Learn how to use the {d6berlin} R package that provides spatial data sets and template maps for Berlin with carefully chosen and aesthetically pleasing defaults. The template maps include Berlin districts, green spaces, imperviousness levels, water bodies, district borders, roads, metro stations, and railways. Templates maps are created with a {ggplot2} wrapper function with the utility to add an inset globe with a locator pin, a scalebar, and a caption stating the data sources.",
    "author": [
      {
        "name": "Cédric Scherer",
        "url": "htttps://cedricscherer.com"
      }
    ],
    "date": "2021-03-18",
    "categories": [
      "tutorial",
      "rstats",
      "spatial",
      "Berlin",
      "ggplot2"
    ],
    "contents": "\r\n\r\nContents\r\nInstallation\r\nA Basic Template Map of Imperviousness\r\nBerlin Data Sets {datasets}\r\nAdding Locations to the Map\r\nCustom Styling\r\nSave Map\r\n\r\nThe {d6berlin} package aims provide template maps for Berlin with carefully chosen and aesthetically pleasing defaults. Template maps include green spaces, imperviousness levels, water bodies, district borders, roads, and railways, plus the utility to add a globe with locator pin, a scalebar, and a caption to include the data sources.\r\nThere are two main functionalities:\r\nCreate a template map with imperviousness and green spaces with base_map_imp()\r\nProvide various ready-to-use Berlin data sets with sf_*\r\nFurthermore, the package provides utility to add a globe with locator pin, a scalebar, and a caption to include the data sources.\r\nInstallation\r\nYou can install the {d6berlin} package from GitHub:\r\n\r\n\r\n## install.packages(\"devtools\")\r\n## devtools::install_github(\"EcoDynIZW/d6berlin\")\r\nlibrary(d6berlin)\r\n\r\n\r\n(Note: If you are asked if you want to update other packages either press “No” (option 3) and continue or update the packages before running the install command again.)\r\nA Basic Template Map of Imperviousness\r\nThe basic template map shows levels of imperviousness and green areas in Berlin. The imperviousness raster data was derived from Geoportal Berlin (FIS-Broker) with a resolution of 10m. The vector data on green spaces was collected from data provided by the OpenStreetMap Contributors. The green spaces consist of a mixture of land use and natural categories (namely “forest”, “grass”, “meadow”, “nature_reserve”, “scrub”, “heath”, “beach”, “cliff”).\r\nThe map is projected in EPSG 4326 (WGS84).\r\n\r\n\r\nd6berlin::base_map_imp()\r\n\r\n#> Aggregating raster data.\r\n#> Plotting basic map.\r\n#> Styling map.\r\n\r\n\r\nYou can also customize the arguments, e.g. change the color intensity, add a globe with a locator pin, change the resolution of the raster, and move the legend to a custom position:\r\n\r\n\r\nbase_map_imp(color_intensity = 1, globe = TRUE, resolution = 500,\r\n             legend_x = .17, legend_y = .12)\r\n\r\n\r\n\r\nIf you think the legend is not need, there is also an option called \"none\". (The default is \"bottom\". You can also use of the predefined setting \"top\" as illustrated below or a custom position as shown in the previous example.)\r\nBerlin Data Sets {datasets}\r\nThe package contains several data sets for Berlin. All of them start with sf_, e.g. d6berlin::sf_roads. Here is a full overview of the data sets that are available:\r\n\r\n\r\n\r\nAdding Locations to the Map\r\nLet’s assume you have recorded some animal locations or you want to plot another information on to of this plot. For example, let’s visualize the Berlin metro stations by adding geom_sf(data = x) to the template map:\r\n\r\n\r\nlibrary(ggplot2)\r\nlibrary(sf)\r\n\r\nmap <- base_map_imp(color_intensity = .3, resolution = 500, legend = \"top\")\r\n\r\nmap + geom_sf(data = sf_metro) ## sf_metro is contained in the {d6berlin} package\r\n\r\n\r\n\r\nNote: Since the template map contains many filled areas, we recommend to add geometries with variables mapped to color|colour|col to the template maps.\r\nYou can, of course, style the appearance of the points as usual:\r\n\r\n\r\nmap + geom_sf(data = sf_metro, shape = 8, color = \"red\", size = 2)\r\n\r\n\r\n\r\nIt is also possible to filter the data inside the geom_sf function — no need to use subset:\r\n\r\n\r\nlibrary(dplyr) ## for filtering\r\nlibrary(stringr) ## for filtering based on name\r\n\r\nmap + \r\n  geom_sf(data = filter(sf_metro, str_detect(name, \"^U\")), \r\n          shape = 21, fill = \"dodgerblue\", size = 2) +\r\n  geom_sf(data = filter(sf_metro, str_detect(name, \"^S\")), \r\n          shape = 21, fill = \"forestgreen\", size = 2)\r\n\r\n\r\n\r\nYou can also use the mapping functionality of ggplot2 to address variables from your data set:\r\n\r\n\r\nmap + \r\n  geom_sf(data = sf_metro, aes(color = type), size = 2) +\r\n  scale_color_discrete(type = c(\"dodgerblue\", \"forestgreen\"), \r\n                       name = NULL) +\r\n  guides(color = guide_legend(direction = \"horizontal\",\r\n                              title.position = \"top\", \r\n                              title.hjust = .5))\r\n\r\n\r\n\r\n(It looks better if you style the legend in the same horizontal layout.)\r\nCustom Styling\r\nSince the output is a ggplot object, you can manipulate the result as you like (but don’t apply a new theme, this will mess up the legend design):\r\n\r\n\r\nlibrary(systemfonts) ## for title font\r\n\r\nbase_map_imp(color_intensity = 1, resolution = 250, globe = TRUE,\r\n             legend_x = .17, legend_y = .12) + \r\n  geom_sf(data = sf_metro, shape = 21, fill = \"white\", \r\n          stroke = .4, size = 4) +\r\n  ggtitle(\"Metro Stations in Berlin\") + \r\n  theme(plot.title = element_text(size = 30, hjust = .5, family = \"Bangers\"),\r\n        panel.grid.major = element_line(color = \"white\", size = .3),\r\n        axis.text = element_text(color = \"black\", size = 8),\r\n        plot.background = element_rect(fill = \"#fff0de\", color = NA),\r\n        plot.margin = margin(rep(20, 4)))\r\n\r\n\r\n\r\nSave Map\r\nUnfortunately, the size of the text elements is fixed. The best aspect ratio to export the map is 12x9 and you can save it with ggsave() for example:\r\n\r\n\r\nggsave(\"metro_map.pdf\", width = 12, height = 9, device = cairo_pdf)\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "https://raw.githubusercontent.com/EcoDynIZW/EcoDynIZW.github.io/main/img/hex-d6berlin.png",
    "last_modified": "2023-02-17T08:26:52+01:00",
    "input_file": {}
  },
  {
    "path": "posts/d6package/",
    "title": "Simplify Workflows of D6 Research Projects",
    "description": "Learn how to use the {d6} package to follow the project workflow within the department “Ecological Dynamics” at the Leibniz Institute for Zoo and Wildlife Research. The package functionality allows you to set up a standardized folder structure, to use templates for standardized reports and provides some helpful utility functions.",
    "author": [
      {
        "name": "Cédric Scherer",
        "url": "htttps://cedricscherer.com"
      }
    ],
    "date": "2020-09-12",
    "categories": [
      "tutorial",
      "rstats",
      "workflow",
      "data management"
    ],
    "contents": "\r\n\r\nContents\r\nInstallation\r\nCreate Project Directory\r\nUse A Custom Root\r\nDirectory\r\nUse\r\nVersion Control\r\nSetup without Geo\r\nDirectories\r\nAdd Documentation to Your\r\nProject\r\n\r\nInstall Common Packages\r\nUse Custom Rmarkdown\r\nTemplates\r\nRender Rmarkdown Files to\r\nReports\r\n\r\nThe {d6}\r\npackage aims to simplify workflows of our D6 research projects by\r\nproviding a standardized folder structure incl. version control,\r\nRmarkdown templates, and other utilities.\r\nThere are four main functionalities:\r\nCreate standardized project\r\ndirectories with new_project()\r\nInstall a set of common packages\r\nwith install_d6_packages()\r\nProvide custom Rmarkdown\r\ntemplates via\r\nFile > New File > Rmarkdown... > From Template\r\nRender all Rmarkdown\r\ndocuments to ./docs/report with render_all_reports() or\r\nrender_report()\r\nInstallation\r\nThe package is not on CRAN and needs to be installed from GitHub. To\r\ndo so, open Rstudio and run the following two lines in the console. In\r\ncase the {devtools} package is already installed, skip that\r\nstep.\r\n\r\n\r\ninstall.packages(\"devtools\")\r\ndevtools::install_github(\"EcoDynIZW/d6\")\r\n\r\n\r\n\r\n(Note: If you are asked if you want to update other packages either\r\npress “No” (option 3) and continue or update the packages before running\r\nthe install command again.)\r\nCreate Project Directory\r\nRun the function new_project() to create a new project.\r\nThis will create a standardized directory with all the scaffolding we\r\nuse for all projects in our department. It also add several files needed\r\nfor documentation of your project.\r\nTo start a new project in the current working directory, simply\r\nrun:\r\n\r\n\r\nd6::new_project(\"unicornus_wl_sdm_smith_j\")\r\n\r\n\r\n\r\nPlease give your project a unique and descriptive name:\r\nspecies_country_topic_name\r\nFor example, when John Smith is developing a species distribution\r\nmodels for unicorns in Wonderland, a descriptive title could be:\r\nunicornus_wl_sdm_smith_j. Please use underscores and the international Alpha-2 encoding\r\nfor countries.\r\nThe full scaffolding structure created in the root folder (here\r\nunicornus_wl_sdm_smith_j) is the following:\r\n\r\n. \r\n└── unicornus_wl_sdm_smith_j\r\n    ├── .Rproj.user         —  Rproject files\r\n    ├── data-raw            —  raw data (tabular data in root folder)\r\n    │    └── geo-raw        —  raw spatial data\r\n    ├── docs                —  documents\r\n    │   ├── admin           —  adminstrative docs, e.g. permits \r\n    │   ├── literature      —  literature used for parameterization + ms\r\n    │   ├── manuscript      —  manuscript drafts (main + supplement)\r\n    │   ├── presentations   —  talks and poster presentations\r\n    │   └── reports         —  rendered reports\r\n    ├── output              —  everything that is computed (except plots)\r\n    │   ├── data-proc       —  processed tabular data\r\n    │   └── geo-proc        —  processed spatial data\r\n    ├── plots               —  plot output\r\n    ├── R                   —  scripts\r\n    │   ├── 00_start.R      —  first script to run\r\n    │   └── XX_submit.R     —  final script to run\r\n    ├── .gitignore          —  contains which files to ignore for version control\r\n    ├── .Rbuildignore       —  contains which files to ignore for package builds\r\n    ├── DESCRIPTION         —  contains project details and package dependencies\r\n    ├── NAMESPACE           —  contains context for R objects\r\n    └── project.Rproj       —  Rproject file: use to start your project\r\n\r\nUse A Custom Root Directory\r\nYou don’t need to change the working directory first—you can also\r\nspecify a path to a custom root folder in which the new project folder\r\nis created:\r\n\r\n\r\n## both work:\r\nd6::new_project(\"unicornus_wl_sdm_smith_j\", path = \"absolute/path/to/the/root/folder\")\r\n## or:\r\nd6::new_project(\"unicornus_wl_sdm_smith_j\", path = \"absolute/path/to/the/root/folder/\")\r\n\r\n\r\n\r\nThe resulting final directory of your project would be\r\nabsolute/path/to/the/root/folder/unicornus_wl_sdm_smith_j.\r\nUse Version Control\r\nIf you want to create a GitHub repository for the project at the same\r\ntime, use instead:\r\n\r\n\r\nd6::new_project(\"unicornus_wl_sdm_smith_j\", github = TRUE)\r\n\r\n\r\n\r\nBy default, the visibility of the GitHub repository is set to\r\n“private” but you can also change that:\r\n\r\n\r\nd6::new_project(\"unicornus_wl_sdm_smith_j\", github = TRUE, private_repo = FALSE)\r\n\r\n\r\n\r\nNote that to create a GitHub repo you will need to have configured\r\nyour system as explained here.\r\nSetup without Geo\r\nDirectories\r\nIf your project does not (or will not) contain any spatial data, you\r\ncan prevent the creation of the directories geo-raw and\r\ngeo-proc by setting geo to\r\nFALSE:\r\n\r\n\r\nd6::new_project(\"unicornus_wl_sdm_smith_j\", geo = FALSE)\r\n\r\n\r\n\r\nAdd Documentation to Your\r\nProject\r\nAfter you have set up your project directory, open the file\r\n00_start.R in the R folder. Add the details of\r\nyour project, fill in the readme, add a MIT license (if needed) and add\r\npackage dependencies.\r\nInstall Common Packages\r\nYou can install the packages that are most commonly used in our\r\ndepartment via install_d6_packages():\r\n\r\n\r\nd6::install_d6_packages()\r\n\r\n\r\n\r\nNote that this function is going to check preinstalled versions and\r\nwill only install packages that are not installed with your current R\r\nversion.\r\nAgain, there is an arguement geo so you can decide if\r\nyou want to install common geodata packages as well (which is the\r\ndefault). If you are not intending to process geodata, set\r\ngeo to FALSE:\r\n\r\n\r\nd6::install_d6_packages(geo = FALSE)\r\n\r\n\r\n\r\nThe default packages that are going to be installed are:\r\n\r\ntidyverse (tibble, dplyr, tidyr, ggplot2, readr, forecats, stringr, purrr), lubridate, here, vroom, patchwork, usethis\r\n\r\nThe following packages will be installed in case you specify\r\ngeo = TRUE:\r\n\r\nrgdal, geos, raster, sp, sf, tmap\r\n\r\nUse Custom Rmarkdown\r\nTemplates\r\nThe package also provides several templates for your scripts. In\r\nRstudio, navigate to\r\nFile > New File > RMarkdown... > Templates and\r\nchoose the template you want to use. All templates come with a\r\npreformatted YAML header and chunks for the setup.\r\nThe following templates are available for now:\r\nEcoDynIZW Basic: Template for a basic Rmarkdown research\r\nreport including bits of codes and comments to get started\r\nEcoDynIZW Minimal: Template for an Rmarkdown research\r\nreport (almost empty)\r\nRender Rmarkdown Files to\r\nReports\r\nThe render_*() functions take care of knitting your\r\nRmarkdown files into HTML reports. The functions assume that your .Rmd\r\nfiles are saved in the R directory or any subdirectory, and\r\nwill store the resulting .html files in the according directory, namely\r\n./docs/reports/.\r\nYou can render all .Rmd files that are placed in the R\r\ndirectory and subdirectories in one step:\r\n\r\n\r\nd6::render_all_reports()\r\n\r\n\r\n\r\nYou can also render single Rmarkdown documents via\r\nrender_report():\r\n\r\n\r\nd6::render_report(\"my-report.Rmd\")\r\nd6::render_report(\"notsurewhybutIhaveasubfolder/my-report.Rmd\")\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "https://raw.githubusercontent.com/EcoDynIZW/EcoDynIZW.github.io/main/img/hex-d6.png",
    "last_modified": "2022-11-14T10:21:25+01:00",
    "input_file": {}
  }
]
