[
  {
    "path": "posts/clippingmaskberlin/",
    "title": "Clipping a Raster to a Specific Area",
    "description": "Learn how to use a clipping mask to extract a portion of a raster, based on a template extent. In this example we show you how to clip a raster map to the extent and borders of Berlin boundaries.",
    "author": [
      {
        "name": "Moritz Wenzler-Meya",
        "url": {}
      }
    ],
    "date": "2023-06-26",
    "categories": [
      "tutorial",
      "spatial",
      "Berlin"
    ],
    "contents": "\n\nContents\nExample PopDynCloud\nExample geoboundaries\n\nSometimes you will get a dataset that is larger than your study area and you want to clip it to your specific extent or boundaries. There are two ways to do that:\nthe crop function of the {terra} package: this will crop the dataset to the extent of the cropping mask\nthe mask function of the {terra} package: this will crop the dataset to the extent of the cropping mask and set everything outside of the mask boundaries to NA (or to a custom set value)\nIn this tutorial only the mask function is covered because the crop function is straightforward to use. The mask function gives the opportunity to only get the raster cells that are covered by another raster or spatial object.\nHere are two examples showing how to use data from the PopDynCloud and one with data from the geoboundaries website/package:\n\n\nlibrary(d6geodata)\nlibrary(sf)\nlibrary(terra)\nlibrary(dplyr)\n\n\nExample PopDynCloud\nIf you have access to the PopDynCloud you can use the districts_berlin_layer like this:\n\n\nberlin_mask <- get_geodata(data_name =  \"districs_berlin_2022_poly_03035_gpkg\",\n                           path_to_cloud = \"E:/PopDynCloud\") # get_geodata function from the d6geodata package\n\nReading layer `districs_berlin_2022_poly_03035' from data source \n  `E:\\PopDynCloud\\GeoData\\data-raw\\berlin\\districs_berlin_2022_poly_03035_gpkg\\districs_berlin_2022_poly_03035.gpkg' \n  using driver `GPKG'\nSimple feature collection with 97 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 4531043 ymin: 3253864 xmax: 4576654 ymax: 3290795\nProjected CRS: ETRS89-extended / LAEA Europe\n\nrast_example <- get_geodata(data_name =  \"tree-cover-density_berlin_2018_10m_03035_tif\",\n                           path_to_cloud = \"E:/PopDynCloud\")\n\n\nplot_quantitative_map(tif = rast_example) # plot not masked layer\n\n\nrast_example_masked <- mask(rast_example, # input raster \n                            berlin_mask) # mask to be clipped on \n\nplot_quantitative_map(tif = rast_example_masked) # plot masked layer\n\n\n\nExample geoboundaries\nIf not, you can use the data from the geoboundaries website or using the rgeoboundaries package from github:\n\n\nremotes::install_github(\"dickoa/rgeoboundaries\")\n\n\n\n\nlibrary(rgeoboundaries)\nrgeob_mask_berlin <- rgeoboundaries::gb_adm2(\"Germany\") %>% # set Country name(s)\n  filter(shapeName %in% \"Berlin\") %>% # filter for Berlin\n  st_transform(3035) # reproject to 3035 (or desired crs) \n\nrast_example_rgeob_masked <- mask(rast_example, # input raster \n                                  rgeob_mask_berlin) # mask to be clipped on\n\nplot_quantitative_map(tif = rast_example_rgeob_masked) # plot with function from d6geodata package\n\n\n\n\n\n\n",
    "preview": "posts/clippingmaskberlin/clippingmaskberlin_files/figure-html5/example-1.png",
    "last_modified": "2023-07-03T16:29:06+02:00",
    "input_file": {}
  },
  {
    "path": "posts/howtopost/",
    "title": "How to Make a Blogpost: a Brief Introduction",
    "description": "Learn how to contribute to the wiki section of the Ecological Dynamics Department webpage. If you think you have an important information, script, package or just a piece of code that might be interesting for your colleagues, consider to turn it into a blogpost for our webpage.",
    "author": [
      {
        "name": "Moritz Wenzler-Meya",
        "url": {}
      },
      {
        "name": "C√©dric Scherer",
        "url": {}
      }
    ],
    "date": "2023-06-26",
    "categories": [
      "tutorial",
      "workflow"
    ],
    "contents": "\n\nContents\nCreate a Blogpost\nStep 0 ‚Äî Install the {d6} Package\nStep 1 ‚Äî Use the D6 Blogpost Template\nStep 2 ‚Äî Fill the YAML Header of the Script\nStep 3 ‚Äî Add the Content\nStep 4 ‚Äî Render the Blogpost\nStep 5 ‚Äî Review Process\nStep 6 ‚Äî Published!\n\nGeneral Tips for Posting\n\nYou have a nice piece of code? You have developed a cool package? You have something to share within (or even outside) our department?\nYou can share it with a blogpost on our EcoDynIZW Website by following these simple steps!\nCreate a Blogpost\nStep 0 ‚Äî Install the {d6} Package\nIf not yet installed, install the {d6} package. It provides several functions for our department along with Rmarkdown templates, including the D6 blogpost template.\n\n\n# install.packages(remotes)\nremotes::install_github(\"EcoDynIZW/d6\")\n\n\nStep 1 ‚Äî Use the D6 Blogpost Template\nIn RStudio, navigate to File > New File... > R Markdown... .\nIn the From Template section, choose the D6 Blogpost Template from the list.\n\n\nIf the template is not listed, please make sure that the latest version of {d6} is installed.\nStep 2 ‚Äî Fill the YAML Header of the Script\nFill in a proper name, please use uppercase for the name.\nWrite a short description. Please have in mind that this description will appear on the blogpost listing page as well as in the post. Usually, we start these descriptions with ‚ÄúLearn how to learn x to do y.‚Äù.\nAdd some categories that relate to your article. For examples, browse through other posts featured in the wiki section of our web page.\nEnter your name and the date of the post in the given format.\nStep 3 ‚Äî Add the Content\nDescribe briefly what you will show us and describe each chunk separately.\nLeave a line of space between text and chunk for separating text and chunk output.\nPlease use the chunk options to name chunks, hide them or ignore them in the knitting process. This will help later to find possible errors.\nStep 4 ‚Äî Render the Blogpost\nKnit the post and check if the knitted document looks as desired.\nStep 5 ‚Äî Review Process\nSend the Rmd file to the data manager for review.\nWait for feedback by the data manager. If changes are requested, update the article accordingly and send the corrected script.\nStep 6 ‚Äî Published!\nThe data manager will publish your post as soon as possible on the website.\nGeneral Tips for Posting\nPlease use the spell check before knitting and pushing the post.\nPlease check if the code is running (in a reasonable time) as we have to rebuild the page from time to time.\nUse styling like `plot()` for function names in the text and `{pckg}` package names.\nPick examples that are simple enough in terms of file size and performance so that they can be easily reproduced within the script.\nMake sure that your code can be run by anybody reading the post. If you need data inputs, generate made-up data examples, use data sets from packages, or link to the data source (make sure it is publicly available as readers are not necessarily part of the department!)\n\n\n\n",
    "preview": {},
    "last_modified": "2023-07-03T16:29:16+02:00",
    "input_file": {}
  },
  {
    "path": "posts/ggplot-workflow/",
    "title": "ggplot2 Workflow Tips",
    "description": "Learn how to efficiently use the powerful graphics library {ggplot2} by avoiding repeated code and manual adjustments. You will learn how to define your themes globally and how to use the {patchwork} package to create multi-panel plots with automated numbering and combined legends.",
    "author": [
      {
        "name": "Cedric Scherer",
        "url": "https://cedricscherer.com"
      }
    ],
    "date": "2023-06-22",
    "categories": [
      "rstats",
      "ggplot2",
      "workflow",
      "tutorial"
    ],
    "contents": "\n\nContents\nSetup\nTheming\nSet Themes Globally\nAdjust\nTheme Base Settings\nUpdate Theme\nElements\nCustom Local\nModifications\nSummary\n\nMulti-Panel Figures\nAdjust\nLayout\nAdd White\nSpace\nNested\nLayouts\nMerge\nLegends\nAutomate Plot\nTags\nInset\nPlots\nSummary\n\n\nSetup\nLet‚Äôs load the {ggplot2} library and create two basic\nggplots, stored as g1 (scatter plot) and g2\n(box-and-whisker plot) that can be used later.\n\n\nlibrary(ggplot2)\n\n\n\n\n\ng1 <- ggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class))\n\ng1\n\n\n\ng2 <- ggplot(mpg, aes(x = class, y = hwy)) +\n  geom_boxplot()\n\ng2\n\n\n\n\n\n\nTheming\nThe resulting plots use the default gray theme:\ntheme_gray() or theme_grey().\nWe can change the default theme by adding a complete theme, starting\nwith theme_*(), and/or customizing single elements of the\ndefault theme via theme():\n\n\ng1 + \n  ## apply light complete theme\n  theme_light() + \n  ## remove minor grid + modify typeface\n  theme(panel.grid.minor = element_blank(),\n        text = element_text(family = \"PT Sans\"))\n\n\n\ng2 + \n  theme_light() + \n  theme(panel.grid.minor = element_blank(), \n        text = element_text(family = \"PT Sans\"))\n\n\n\n\n\n\nThis procedure involves a lot of copy-and-paste‚Äôing, which makes it a\ntedious procedure especially in case you decide to make some general\nstyling changes at a later point. It is also prone to mistakes as you\nmight forget to set specific adjustments for single plots.\nSet Themes Globally\nInstead of repeating the same code to change the appearance of your\nplots, it is more efficient and beneficial to overwrite the default\nglobal theme:\n\n\ntheme_set(theme_light())\n\n\n\nAfter setting the new theme, all plots created within the\nsame environment are styled accordingly:\n\n\ng1\n\n\n\ng2\n\n\n\n\n\n\nAdjust Theme Base Settings\nComplete themes allow for some general modifications, no matter if\nadded locally to your plot or if set globally. The setting include the\ntypeface used for all text elements (base_family), the\ngeneral base size (base_size) as well as dedicated relative\nsizes for line elements (base_line_size) and rect elements\n(base_rect_size).\n\n\ng1 + \n  theme_light(\n    base_family = \"PT Serif\", ## default: depends on OS\n    base_size = 18,           ## default: 11\n    base_line_size = 3,       ## default: base_size/22 -> 0.5\n    base_rect_size = 10       ## default: base_size/22 -> 0.5\n  )\n\n\n\n\nKnowing of this feature, we can already adjust the general size\n(which tends to be too small by default) as well as the typeface of our\ncustom global theme:\n\n\ntheme_set(theme_light(base_size = 15, base_family = \"PT Sans\")) \n\n\n\n‚Ä¶ which is then used for all following plots:\n\n\ng1\n\n\n\ng2\n\n\n\n\n\n\nUpdate Theme Elements\nComplete themes are great but in most circumstances we likely want to\nadjust a few things. Usually, you do that by adding the\ntheme() function to your ggplot (as shown in the\nbeginning). However, similarly to theme_set() we can apply\nthe modifications globally:\n\n\ntheme_update(\n  panel.grid.minor = element_blank(),\n  axis.title = element_text(face = \"bold\"),\n  legend.title = element_text(face = \"bold\")\n)\n\n\n\n\n\ng1\n\n\n\ng2\n\n\n\n\n\n\nCustom Local Modifications\nOf course, you can still either overwrite the global theme as before\nor modify specific elements for a single plot if needed:\n\n\ng1 + \n  ## new complete theme\n  theme_classic(base_family = \"PT Serif\", base_size = 15) + \n  ## add grid lines\n  theme(panel.grid.major = element_line(color = \"grey90\"))\n\n\n\ng2 + \n  ## remove vertical grid lines + overwrite axis title styling\n  theme(panel.grid.major.x = element_blank(), \n           axis.title = element_text(color = \"red\", face = \"italic\")) \n\n\n\n\n\n\nSummary\nSetting and updating ggplot themes globally is efficient and avoids\npotential mistakes.\nAs a workflow routine, add a chunk that loads {ggplot2}\nand afterwards sets and updates your theme at the beginning of a\nscript rather than adding the same code to each plot\nindividually.\nMulti-Panel Figures\nWe often use multi-panel visualizations, i.e.¬†several plots layed out\nin a single graphic. Instead of combining single figures manually, we\nmake use of a coding-first approach.\nThere are many packages to combine ggplots such as\n{gridExtra}, {cowplot}, and\n{ggarrange}. The most recent, and IMHO the best in terms of\nfunctionality and simplicity, is the {patchwok}\npackage by Thomas Lin Pedersen. For simple multi-panel graphics,\nmathematical operators can be used ‚Äì easy to use and remember.\n\n\nlibrary(patchwork)\n\n\n\n\n\ng1 + g2\n\n\n\n\n\n\ng1 / g2\n\n\n\n\nAdjust Layout\nBy default, both plots take the same space. In case you want to\nadjust how the plots are laid out, use plot_layout() in\ncombination with either widths or heights.\nThese arguments take a vector with the relative width or height for each\nplot, respectively.\n\n\ng1 + g2 + \n  plot_layout(widths = c(.5, 1))\n\n\n\n\nAdd White Space\n{patchwok} comes with a placeholder to add empty space\nbetween plots. Once can add a plot_spacer() similar to a\nregular plot:\n\n\ng1 + plot_spacer() + g2 + \n  plot_layout(widths = c(.5, .1, 1))\n\n\n\n\nNested Layouts\nAlso more complex layouts can be created:\n\n\n\n\n\n(g1 + g3 + g4) / (g2 + g5)\n\n\n\n\nCode to create g3, g4, and g5\n\n\ng3 <- ggplot(mpg, aes(x = cyl, y = hwy)) +\n  geom_point(aes(color = class))\n\ng4 <- ggplot(mpg, aes(x = cty, y = hwy)) +\n  geom_point(aes(color = class))\n\ng5 <- ggplot(mpg, aes(x = class, y = hwy)) +\n  stat_summary(fun.data = \"mean_sdl\", fun.args = list(mult = 1))\n\n\n\n\n\n\nAlternatively, you can also create a design layout to have full\ncontrol:\n\n\nlayout <- \n  \"\n  ABBCCC#\n  DDDD#EE\n  \"\n\ng1 + g3 + g4 + g2 + g5 + \n  plot_layout(design = layout)\n\n\n\n\nThe letters refer to the single plots (in the order you combine them\nlater) and a hash # indicates empty space, similar to\nplot_spacer().\nMerge Legends\nDisplaying the legend three times makes no sense.\n{patchwork} offers the utility to ‚Äúcollect your guides‚Äù\ninside the plot_layout() function:\n\n\n(g1 + g3 + g4) / (g2 + g5) + \n  plot_layout(guides = \"collect\")\n\n\n\n\nNow we may want to move it to the top so it is shown next to the\nrelevant colored scatter plots, not in the middle. We can adjust the\ntheme for all plots inside another {patchwork} function\ncalled plot_annotation()‚Äîor by updating your global theme üòâ\n\n\n((g1 + g3 + g4) / (g2 + g5)) + \n  plot_layout(guides = \"collect\") +\n  plot_annotation(theme = theme(legend.justification = \"top\"))\n\n\n\n\nAutomate Plot Tags\nWhen preparing such multi-panel figures for publications, we usually\nwant to add tags to be able to refer to subplots in the figure caption\nor main text. Again, we can do this inside R instead of adding them\nafterwards by hand (which either takes very long or results in\nirregularly aligned labels).\n\n\n((g1 + g3 + g4) / (g2 + g5)) + \n  plot_layout(guides = \"collect\") +\n  plot_annotation(tag_levels = \"A\", \n                  theme = theme(legend.justification = \"top\"))\n\n\n\n\n{patchwork} understands a range of numbering formats\nsuch as a for lowercase letters, 1 for\nnumbers, or i and I for lowercase and\nuppercase Roman numerals, respectively. Furthermore we can style the tag\nby defining a pre- and/or suffix:\n\n\n((g1 + g3 + g4) / (g2 + g5)) + \n  plot_layout(guides = \"collect\") +\n  plot_annotation(tag_levels = \"i\", tag_prefix = \"(\", tag_suffix = \")\",\n                  theme = theme(legend.justification = \"top\"))\n\n\n\n\nInset Plots\nSimilar to other arrangement packages, we can use\n{patchwork} also to add inset plots. Inside the\ninside_element() function, we specify the plot to draw and\nthen the outer bounds (left, bottom, right top).\n\n\ng4 + inset_element(g1 + guides(color = \"none\"), .5, 0, 1, .5)\n\n\n\n\nBy default, the inset plot is aligned with the panel of the main\nplot. If you want to modify the behavior, overwrite the default input of\nalign_to.\n\n\ng4 + inset_element(g1 + guides(color = \"none\"), .5, 0, 1, .5, align_to = \"plot\")\ng4 + inset_element(g1 + guides(color = \"none\"), .5, 0, 1, .5, align_to = \"full\")\n\n\n\n\n\n\nSummary\n{patchwork} offers some great functionality to create\nbasic and pretty complex layouts, add inset plots, merge repeated\nlegends, and automate tag numbering. This makes it a powerful tool as\nyou do not need to adjust tag labels, legends, and more for the\nindividual ggplot.\n\n\n\n",
    "preview": "https://raw.githubusercontent.com/EcoDynIZW/EcoDynIZW.github.io/main/img/preview-ggplot-workflow.png",
    "last_modified": "2023-07-03T16:29:14+02:00",
    "input_file": {}
  },
  {
    "path": "posts/netlogoturtlespatialprojection/",
    "title": "Netlogo Turtle Spatial Projection",
    "description": "Learn how to transform the relative coordinates of the individuals from Netlogo into coordinates from real maps. This code is especially designed for spatially explicit netlogo models that were set to store the individual coordinates (xcor, ycor) in the output for the turtle data.",
    "author": [
      {
        "name": "Aimara Planillo",
        "url": {}
      }
    ],
    "date": "2023-06-15",
    "categories": [
      "NetLogo",
      "spatial",
      "tutorial"
    ],
    "contents": "\n\nContents\nHow to project Netlogo Turtle coordinates into a real map\n0. Load libraries\n1. Create data\n2. Get reference coordinates\n3. Transform turtle coordinates into map coordinates\n4. Quick plot with tmap\n5. Plot with ggplot2\n\n\nHow to project Netlogo Turtle coordinates into a real map\nWhen using spatial data in Netlogo, the coordinates of a raster get transformed to relative coordinates. This means, the cell in the bottom left gets coordinate (1,1), the one on top of it is (1,2), and so on.\nAfter running a model, usually we want to reproject the output back to the spatial data coordinates used, either for post-simulation analyses or for plotting.\nThis code shows how to project the turtles‚Äô coordinates back into a map, when a raster was\nused to create the Netlogo landscape.\nFor this we need:\n- The raster used as netlogo input\n- The turtle coordinates in the output\n0. Load libraries\n\n\nlibrary(terra)\nlibrary(dplyr)\nlibrary(sf)\nlibrary(tmap)\nlibrary(ggplot2)\nlibrary(ggspatial)\n\n\n1. Create data\nIn this example we create a raster and some turtle data to use.\nWith real data, you will load your raster here and make sure it has a PROJECTED coordinate system.\nTurtle data will have different formats depending how it was created, the basic data we need for this is the identity of the turtle and the coordinates.\n\n\n## Create raster with 100 cells for the example\nmyraster <- rast(nrows = 100, ncols = 100, \n                 xmin = 4541100, xmax = 4542100, \n                 ymin = 3265800, ymax = 3266800)\n## give random values to the raster\nmyraster <- init(myraster, sample(1:1000))\n\n## assign projection\ncrs(myraster) <- \"epsg:3035\"\nplot(myraster)\n\n\n## turtle data - example data\nwho <- seq(1,10)\nxcoord <- sample(1:100, 10) # create random integers for x coordinate\nycoord <- sample(1:100, 10) # create random integers for y coordinate\n\nturtle_variables <- cbind.data.frame(who, xcoord, ycoord)\nhead(turtle_variables)\n\n  who xcoord ycoord\n1   1      8     23\n2   2     51     97\n3   3     87     74\n4   4     11     24\n5   5     69     26\n6   6     68     81\n\nNow that we have our data, let‚Äôs extract the map coordinates as reference and transform the turtle ones. This process will work with any PROJECTED coordinate system.\n2. Get reference coordinates\nWe need the bottom left corner of the map as a reference point and the resolution of the map\n\n\n# we are going to trasnform the cell relative numbering to real coordinates, starting left down as this is where netlogo starts numbering the cells\nstart_left <- xmin(myraster)\nstart_down <- ymin(myraster)\nmy_res <- res(myraster)[1]\n\n\n3. Transform turtle coordinates into map coordinates\nNow we use the reference point to transform our coordinates into the projected coordinates and the resolution to correct for the size of the cells\n\n\nturtle_spatial <- turtle_variables %>% \n  mutate(spatial_xcoord = start_left + ((xcoord * my_res) + my_res/2), #divided by 2 to locate in the center of the cell\n         spatial_ycoord = start_down + ((ycoord * my_res) + my_res/2))\n\n\n## make spatial points\nturtle_sf <- st_as_sf(turtle_spatial, \n                      coords = c(\"spatial_xcoord\", \"spatial_ycoord\"), \n                      crs = crs(myraster))\n\n\n4. Quick plot with tmap\n\n\n\n5. Plot with ggplot2\n\n        x       y  hs\n1 4541105 3266795 902\n2 4541115 3266795 698\n3 4541125 3266795 500\n4 4541135 3266795 492\n5 4541145 3266795 405\n6 4541155 3266795 151\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-07-03T16:29:24+02:00",
    "input_file": {}
  },
  {
    "path": "posts/r-spatial-data/",
    "title": "Spatial Data Sources in R",
    "description": "Learn how to use a suite of R packages to download various spatial data sources. This guide illustrates how to download data of countries, physical objects, and cultural properties as vector or raster objects that can be assessed from within R.",
    "author": [
      {
        "name": "Cedric Scherer",
        "url": "htttps://cedricscherer.com"
      }
    ],
    "date": "2023-05-25",
    "categories": [
      "tutorial",
      "rstats",
      "workflow",
      "spatial",
      "data sources"
    ],
    "contents": "\n\nContents\nPreparation\n{rnaturalearth}\nCountry Data\nPhysical Data Sources\nCultural Data Sources\nRelief Data\n\n{rgeoboundaries}\n{osmdata}\n{elevatr}\n\nPreparation\nTo visualize the data sets, we use the {ggplot2}\npackage. We will also use the {sf} and the\n{terra} packages to work and plot spatial data‚Äìvector and\nraster, respectively‚Äìin R. Make sure all packages are installed when\nrunning the code snippets.\n\n\n# install.packages(\"ggplot2\")\n# install.packages(\"sf\")\n# install.packages(\"terra\")\n\nlibrary(ggplot2)\n\n### set \"empty\" theme with centered titles for ggplot output\ntheme_set(theme_void())\ntheme_update(plot.title = element_text(face = \"bold\", hjust = .5))\n\n\n\n\n{rnaturalearth}\nNaturalEarth is a\npublic domain map data set that features vector and raster data of\nphysical and cultural properties. It is available at 1:10m, 1:50m, and\n1:110 million scales.\n{rnaturalearth}\nis an R package to hold and facilitate interaction with NaturalEarth map\ndata via dedicated ne_* functions. After loading the\npackage, you can for example quickly access shapefiles of all\ncountries‚Äìthe resulting spatial object contains vector data that is\nalready projected and can be stored as either sp or\nsf format:\n\n\n## install development version of {rnaturalearth} as currently the \n## download doesn't work in the CRAN package version\n# install.packages(\"remotes\")\n# remotes::install_github(\"ropensci/rnaturalearth\")\n\n## for high resolution data, also install {rnaturalearthhires}\n# remotes::install_github(\"ropensci/rnaturalearthhires\")\n\nlibrary(rnaturalearth)\n\n## store as sp object (SpatialPolygonsDataFrame)\nworld <- ne_countries() ## `returnclass = \"sp\"` by default\nclass(world)\n\n\n[1] \"SpatialPolygonsDataFrame\"\nattr(,\"package\")\n[1] \"sp\"\n\n## store as sf object (simple features)\nworld <- ne_countries(returnclass = \"sf\")\nclass(world)\n\n\n[1] \"sf\"         \"data.frame\"\n\nsf::st_crs(world)[1]\n\n\n$input\n[1] \"+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0\"\n\nCountry Data\nThis country data set (which is actually not downloaded but stored\nlocally by installing the package) already contains several useful\nvariables, mostly referring to different naming conventions (helpful\nwhen joining with other data sets), to identify continents and regions,\nand also some information on population size, GDP, and economy:\n\n\nnames(world)\n\n\n [1] \"featurecla\" \"scalerank\"  \"labelrank\"  \"sovereignt\" \"sov_a3\"    \n [6] \"adm0_dif\"   \"level\"      \"type\"       \"admin\"      \"adm0_a3\"   \n[11] \"geou_dif\"   \"geounit\"    \"gu_a3\"      \"su_dif\"     \"subunit\"   \n[16] \"su_a3\"      \"brk_diff\"   \"name\"       \"name_long\"  \"brk_a3\"    \n[21] \"brk_name\"   \"brk_group\"  \"abbrev\"     \"postal\"     \"formal_en\" \n[26] \"formal_fr\"  \"name_ciawf\" \"note_adm0\"  \"note_brk\"   \"name_sort\" \n[31] \"name_alt\"   \"mapcolor7\"  \"mapcolor8\"  \"mapcolor9\"  \"mapcolor13\"\n[36] \"pop_est\"    \"pop_rank\"   \"gdp_md_est\" \"pop_year\"   \"lastcensus\"\n[41] \"gdp_year\"   \"economy\"    \"income_grp\" \"wikipedia\"  \"fips_10_\"  \n[46] \"iso_a2\"     \"iso_a3\"     \"iso_a3_eh\"  \"iso_n3\"     \"un_a3\"     \n[51] \"wb_a2\"      \"wb_a3\"      \"woe_id\"     \"woe_id_eh\"  \"woe_note\"  \n[56] \"adm0_a3_is\" \"adm0_a3_us\" \"adm0_a3_un\" \"adm0_a3_wb\" \"continent\" \n[61] \"region_un\"  \"subregion\"  \"region_wb\"  \"name_len\"   \"long_len\"  \n[66] \"abbrev_len\" \"tiny\"       \"homepart\"   \"min_zoom\"   \"min_label\" \n[71] \"max_label\"  \"ne_id\"      \"wikidataid\" \"name_ar\"    \"name_bn\"   \n[76] \"name_de\"    \"name_en\"    \"name_es\"    \"name_fr\"    \"name_el\"   \n[81] \"name_hi\"    \"name_hu\"    \"name_id\"    \"name_it\"    \"name_ja\"   \n[86] \"name_ko\"    \"name_nl\"    \"name_pl\"    \"name_pt\"    \"name_ru\"   \n[91] \"name_sv\"    \"name_tr\"    \"name_vi\"    \"name_zh\"    \"geometry\"  \n\nWe can quickly plot it:\n\n\nggplot(world) + \n  geom_sf(aes(fill = economy)) + \n  coord_sf(crs = \"+proj=eqearth\")\n\n\n\n\nNOTE: Unfortunately, NaturalEarth is using weird\nde-facto and on-the-ground rules to define country\nborders which do not follow the borders the UN and most countries agree\non. For correct and official borders, please use the\n{rgeoboundaries} package (see below).\nPhysical Data Sources\nYou can specify the scale, category, and type you want as in the\nexamples below.\n\n\nglacier_small <- ne_download(type = \"glaciated_areas\", category = \"physical\", \n                             scale = \"small\", returnclass = \"sf\")\n\nglacier_large <- ne_download(type = \"glaciated_areas\", category = \"physical\", \n                             scale = \"large\", returnclass = \"sf\")\n\n\n\nNow we can compare the impact of different scales specified‚Äìthere is\na notable difference in detail here (and also in size of the object with\n11 versus 1886 observations).\n\n\nggplot() + \n  geom_sf(data = world, color = \"grey85\", fill = \"grey85\") +\n  geom_sf(data = glacier_small, color = \"grey40\", fill = \"grey40\") + \n  coord_sf(crs = \"+proj=eqearth\")\n\nggplot() +\n  geom_sf(data = world, color = \"grey85\", fill = \"grey85\") +\n  geom_sf(data = glacier_large, color = \"grey40\", fill = \"grey40\") +\n  coord_sf(crs = \"+proj=eqearth\")\n\n\n\n\n\nlibrary(patchwork)\n\nsmall <- ggplot() + \n  geom_sf(data = world, color = \"grey85\", fill = \"grey85\", lwd = .05) +\n  geom_sf(data = glacier_small, color = \"grey40\", fill = \"grey40\") + \n  coord_sf(crs = \"+proj=eqearth\") +\n  labs(title = 'scale = \"small\"')\n\nlarge <- ggplot() +\n  geom_sf(data = world, color = \"grey85\", fill = \"grey85\", lwd = .05) +\n  geom_sf(data = glacier_large, color = \"grey40\", fill = \"grey40\") +\n  coord_sf(crs = \"+proj=eqearth\") + \n  labs(title = 'scale = \"large\"')\n\nsmall + large * theme(plot.margin = margin(0, -20, 0, -20))\n\n\n\n\nCultural Data Sources\nNaturalEarth also provides several cultural data sets, such as\nairports, roads, disputed areas. Let‚Äôs have a look at the urban areas\nacross the world:\n\n\nurban <- ne_download(type = \"urban_areas\", category = \"cultural\", \n                     scale = \"medium\", returnclass = \"sf\")\n\nggplot() + \n  geom_sf(data = world, color = \"grey90\", fill = \"grey90\") +\n  geom_sf(data = urban, color = \"firebrick\", fill = \"firebrick\") + \n  coord_sf(crs = \"+proj=eqearth\")\n\n\n\n\nRelief Data\nThe physical and cultural data sets showcased above are all vector\ndata. NaturalEarth also provides raster data, namely gridded relief\ndata:\n\n\nrelief <- ne_download(type = \"MSR_50M\", category = \"raster\",\n                      scale = 50, returnclass = \"sf\")\n\nterra::plot(relief)\n\n\n\n\n\n{rgeoboundaries}\nThe {rgeoboundaries}\npackage uses the Global Database of Political Administrative\nBoundaries that provide generally accepted political borders. The data\nare licensed openly.\n\n\n## install package from GitHub as it is not featured on CRAN yet\n# install.packages(\"remotes\")\n# remotes::install_github(\"wmgeolab/rgeoboundaries\")\n\nlibrary(rgeoboundaries)\n\ngb_adm0()\n\n\nSimple feature collection with 198 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -180 ymin: -89.99893 xmax: 180 ymax: 83.58869\nGeodetic CRS:  WGS 84\nFirst 10 features:\n      shapeName shapeISO           shapeID shapeGroup shapeType\n1          Cuba      CUB CUB-ADM0-3_0_0-B1        CUB      ADM0\n2       Denmark      DNK DNK-ADM0-3_0_0-B1        DNK      ADM0\n3  Saudi Arabia      SAU SAU-ADM0-3_0_0-B1        SAU      ADM0\n4         Yemen      YEM YEM-ADM0-3_0_0-B1        YEM      ADM0\n5         Italy      ITA ITA-ADM0-3_0_0-B1        ITA      ADM0\n6       Comoros      COM COM-ADM0-3_0_0-B1        COM      ADM0\n7         Gabon      GAB GAB-ADM0-3_0_0-B1        GAB      ADM0\n8        Norway      NOR NOR-ADM0-3_0_0-B1        NOR      ADM0\n9    Kyrgyzstan      KGZ KGZ-ADM0-3_0_0-B1        KGZ      ADM0\n10        Tonga      TON TON-ADM0-3_0_0-B1        TON      ADM0\n                         geometry\n1  MULTIPOLYGON (((-78.38432 2...\n2  MULTIPOLYGON (((11.19134 54...\n3  MULTIPOLYGON (((41.75838 16...\n4  MULTIPOLYGON (((53.28007 12...\n5  MULTIPOLYGON (((8.48607 44....\n6  MULTIPOLYGON (((43.65672 -1...\n7  MULTIPOLYGON (((11.72096 2....\n8  MULTIPOLYGON (((7.26726 58....\n9  MULTIPOLYGON (((69.26495 39...\n10 MULTIPOLYGON (((-174.9152 -...\n\nggplot(gb_adm0()) + \n  geom_sf(color = \"grey40\", lwd = .2) + \n  coord_sf(crs = \"+proj=eqearth\") \n\n\n\n\nLower administrative levels are available as well, e.g.¬†in Germany\nadm1 represents federal states (‚ÄúBundesl√§nder‚Äù), adm2\ndistricts (‚ÄúKreise‚Äù) and so on.\nLet‚Äôs plot the admin 1 levels for the DACH countries:\n\n\ndach <- gb_adm1(c(\"germany\", \"switzerland\", \"austria\"), type = \"sscgs\")\n\nggplot(dach) +\n  geom_sf(aes(fill = shapeGroup)) +\n  scale_fill_brewer(palette = \"Set2\")\n\n\n\n\n{osmdata}\nOpenStreetMap (https://www.openstreetmap.org) is a collaborative\nproject to create a free editable geographic database of the world. The\ngeodata underlying the maps is considered the primary output of the\nproject and is accessible from R via the {osmdata}\npackage.\nWe first need to define our query and limit it to a region. You can\nexplore the features and tags (also available as information via\nOpenStreetMap directly).\n\n\n## install package\n# install.packages(\"osmdata\")\n\nlibrary(osmdata)\n\n## explore features + tags\nhead(available_features())\n\n\n[1] \"4wd_only\"  \"abandoned\" \"abutters\"  \"access\"    \"addr\"     \n[6] \"addr:city\"\n\nhead(available_tags(\"craft\"))\n\n\n[1] \"agricultural_engines\" \"atelier\"             \n[3] \"bakery\"               \"basket_maker\"        \n[5] \"beekeeper\"            \"blacksmith\"          \n\n## building the query, e.g. beekeepers\nbeekeeper_query <- \n  ## you can automatically retrieve a boudning box (pr specify one manually)\n  getbb(\"Berlin\") %>%\n  ## build an Overpass query\n  opq(timeout = 999) %>%\n  ## access particular feature\n  add_osm_feature(\"craft\", \"beekeeper\")\n  \n## download data\nsf_beekeepers <- osmdata_sf(beekeeper_query)\n\n\n\nNow we can investigate beekeepers in Berlin:\n\n\nnames(sf_beekeepers)\n\n\n[1] \"bbox\"              \"overpass_call\"     \"meta\"             \n[4] \"osm_points\"        \"osm_lines\"         \"osm_polygons\"     \n[7] \"osm_multilines\"    \"osm_multipolygons\"\n\nhead(sf_beekeepers$osm_points)\n\n\nSimple feature collection with 6 features and 25 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 13.24443 ymin: 52.35861 xmax: 13.69093 ymax: 52.573\nGeodetic CRS:  WGS 84\n             osm_id name addr.city addr.country addr.housenumber\n358407135 358407135 <NA>      <NA>         <NA>             <NA>\n358407138 358407138 <NA>      <NA>         <NA>             <NA>\n417509803 417509803 <NA>      <NA>         <NA>             <NA>\n417509805 417509805 <NA>      <NA>         <NA>             <NA>\n597668310 597668310 <NA>      <NA>         <NA>             <NA>\n597668311 597668311 <NA>      <NA>         <NA>             <NA>\n          addr.postcode addr.street addr.suburb contact.phone\n358407135          <NA>        <NA>        <NA>          <NA>\n358407138          <NA>        <NA>        <NA>          <NA>\n417509803          <NA>        <NA>        <NA>          <NA>\n417509805          <NA>        <NA>        <NA>          <NA>\n597668310          <NA>        <NA>        <NA>          <NA>\n597668311          <NA>        <NA>        <NA>          <NA>\n          contact.website craft email facebook instagram man_made\n358407135            <NA>  <NA>  <NA>     <NA>      <NA>     <NA>\n358407138            <NA>  <NA>  <NA>     <NA>      <NA>     <NA>\n417509803            <NA>  <NA>  <NA>     <NA>      <NA>     <NA>\n417509805            <NA>  <NA>  <NA>     <NA>      <NA>     <NA>\n597668310            <NA>  <NA>  <NA>     <NA>      <NA>     <NA>\n597668311            <NA>  <NA>  <NA>     <NA>      <NA>     <NA>\n          opening_hours operator organic phone product shop source\n358407135          <NA>     <NA>    <NA>  <NA>    <NA> <NA>   <NA>\n358407138          <NA>     <NA>    <NA>  <NA>    <NA> <NA>   <NA>\n417509803          <NA>     <NA>    <NA>  <NA>    <NA> <NA>   <NA>\n417509805          <NA>     <NA>    <NA>  <NA>    <NA> <NA>   <NA>\n597668310          <NA>     <NA>    <NA>  <NA>    <NA> <NA>   <NA>\n597668311          <NA>     <NA>    <NA>  <NA>    <NA> <NA>   <NA>\n          website wheelchair works                  geometry\n358407135    <NA>       <NA>  <NA> POINT (13.69068 52.35918)\n358407138    <NA>       <NA>  <NA> POINT (13.69093 52.35894)\n417509803    <NA>       <NA>  <NA> POINT (13.68991 52.35888)\n417509805    <NA>       <NA>  <NA>  POINT (13.6902 52.35861)\n597668310    <NA>       <NA>  <NA>   POINT (13.24445 52.573)\n597668311    <NA>       <NA>  <NA> POINT (13.24443 52.57295)\n\nbeekeper_locations <- sf_beekeepers$osm_points\n\n## Berlin borders via {geoboundaries}\nsf_berlin <- gb_adm1(c(\"germany\"), type = \"sscgs\")[6,] # the sixth element is Berlin\n\n## Berlin border incl. district borders via our {d6berlin}\n# remotes::install_github(\"EcoDynIZW/d6berlin\")\nsf_berlin <- d6berlin::sf_districts\n\nggplot(beekeper_locations) + \n  geom_sf(data = sf_berlin, fill = \"grey10\", color = \"grey30\") +\n  geom_sf(size = 4, color = \"#FFB000\", alpha = .3) +\n  labs(title = \"Beekeepers in Berlin\",\n       caption = \"¬© OpenStreetMap contributors\")\n\n\n\n\n{elevatr}\nThe {elevatr}\n(https://github.com/jhollist/elevatr/) is an R package\nthat provides access to elevation data from AWS Open Data Terrain Tiles\nand the Open Topography Global data sets API for raster digital\nelevation models (DEMs).\nWe first need to define a location or bounding box for our elevation\ndata. This can either be a data frame or a spatial object. We use an\nsf object which holds the projection to be used when\nassessing the elevation data:\n\n\n## install package\n# install.packages(\"elevatr\")\n\nlibrary(elevatr)\n\n## manually specify corners of the bounding box of the US\nbbox_usa <- data.frame(x = c(-125.0011, -66.9326), \n                       y = c(24.9493, 49.5904))\n\n## turn into spatial, projected bounding box\nsf_bbox_usa <- sf::st_as_sf(bbox_usa, coords = c(\"x\", \"y\"), crs = 4326)\n\n\n\nNow we can download the elevation data with a specified resolution z\n(ranging from 1 to 14 with 1 being very coarse and 14 being very\nfine).\n\n\nelev_usa <- get_elev_raster(locations = sf_bbox_usa, z = 5)\n\nterra::plot(elev_usa)\n\n\n\n\n\n\n\n",
    "preview": "https://raw.githubusercontent.com/EcoDynIZW/EcoDynIZW.github.io/main/img/preview-rspatial-data.png",
    "last_modified": "2023-07-03T16:29:28+02:00",
    "input_file": {}
  },
  {
    "path": "posts/d6geodatapackage/",
    "title": "Manage Spatial Data from Our Geodata Archive",
    "description": "Learn how to use the {d6geodata} R package that provides functions for accessing data from the Geodata archive of the Department of Ecological Dynamics. The two functions `geo_overview()` and `get_geodata()` are the main components for all members of our Department. Several other functions are within this package but only meant to be used by the Geodata Manager of the Department.",
    "author": [
      {
        "name": "Moritz Wenzler-Meya",
        "url": {}
      }
    ],
    "date": "2023-03-03",
    "categories": [
      "tutorial",
      "spatial",
      "rstats"
    ],
    "contents": "\nThe {d6geodata} package aims to access the data from the Geodata archive of the EcoDyn Department for members only!\nThe two main functions are:\ngeo_overview()\nget_geodata()\n\n\n## remotes::install_github(\"EcoDynIZW/d6geodata\")\n## library(d6geodata)\n\n\nIf you want to work with geodata that is already stored in our Geodata archive you have two options:\nGo to the EcoDynIZW Website, click on wikis and select Geodata. There you find several spatial data sets with respective metadaat and visualizations. In the metadata section, you‚Äôll find the information . To donwload the data, cope the folder_name information provided in the metadata and use it as an input in the get_geodata() function from our {d6geodata} to get the data from our PopDynCloud. Another option is the function called geo_overview(). There you can select which data and from which location you want to have a list of data.\nIf you run the function geo_overview you have to decide if you want to see the raw or processed data by typing 1 for raw and 2 for processed data. Afterwards, you have to decide if you want to see the main (type 1) folders (the regions or sub-regions we have data from) or the sub (type 2) folders (the actually data we have in each region).\nExample 1: Main Folder\n\nd6geodata::geo_overview(path_to_cloud = \"E:/PopDynCloud\")\nRaw or processed data: \n\n1: raw\n2: processed\n\nAuswahl: 2\nchoose folder type: \n\n1: main\n2: sub\n\nAuswahl: 1\n[1] \"atlas\" \"BB_MV_B\" \"berlin\" \"europe\" \"germany\" \"world\"\n\nExample 2: Sub Folder\n\nd6geodata::geo_overview(path_to_cloud = \"E:/PopDynCloud\")\nRaw or processed data: \n\n1: raw\n2: processed\n\nAuswahl: 2\nchoose folder type: \n\n1: main\n2: sub\n\nAuswahl: 2\n$atlas\n[1] \"distance-to-human-settlements_atlas_2009_1m_03035_tif\"\n[2] \"distance-to-kettleholes_atlas_2022_1m_03035_tif\"      \n[3] \"distance-to-rivers_atlas_2009_1m_03035_tif\"           \n[4] \"distance-to-streets_atlas_2022_1m_03035_tif\"          \n[5] \"landuse_atlas_2009_1m_03035_tif\"                      \n\n$BB_MV_B\n[1] \"_archive\" \"_old_not_verified\" \"dist_path_bb_agroscapelabs\"\n[4] \"scripts\"                   \n\n$berlin\n [1] \"_old_not_verified\"                            \n [2] \"corine_berlin_2015_20m_03035_tif\"            \n [3] \"distance-to-paths_berlin_2022_100m_03035_tif\" \n [4]  \"green-capacity_berlin_2020_10m_03035_tif\"    \n [5] \"imperviousness_berlin_2018_10m_03035_tif\"     \n [6]  \"light-pollution_berlin_2021_100m_03035_tif\"  \n [7] \"light-pollution_berlin_2021_10m_03035_tif\"    \n [8]  \"motorways_berlin_2022_100m_03035_tif\"        \n [9] \"noise-day-night_berlin_2017_10m_03035_tif\"    \n[10]  \"population-density_berlin_2019_10m_03035_tif\"\n[11] \"template-raster_berlin_2018_10m_03035_tif\"    \n[12] \"tree-cover-density_berlin_2018_10m_03035_tif\"\n\n$europe\n[1] \"imperviousness_europe_2018_10m_03035_tif\"\n\n$germany\n [1] \"_old_not_verified\"                                          \n [2] \"distance-to-motorway-rural-road_germany_2022_100m_03035_tif\"\n [3] \"distance-to-motorways_germany_2022_100m_03035_tif\"          \n [4] \"distance-to-paths_germany_2022_100m_03035_tif\"              \n [5] \"distance-to-roads-paths_germany_2022_100m_03035_tif\"        \n [6] \"distance-to-roads_germany_2022_100m_03035_tif\"              \n [7] \"distance_to_paths_germany_2022_100m_03035_tif\"              \n [8] \"motoroways_germany_2022_03035_osm_tif\"                      \n [9] \"motorway-rural-road_germany_2022_100m_03035_tif\"            \n[10] \"motorways_germany_2022_100m_03035_tif\"                      \n[11] \"paths_germany_2022_100m_03035_tif\"                          \n[12] \"Roads-germany_2022_100m_03035_tif\"                          \n[13] \"roads_germany_2022_100m_03035_tif\"                          \n[14] \"tree-cover-density_germany_2015_100m_03035_tif\"             \n\n$world\ncharacter(0)\n\nNow you can copy the name of one of the layers and paste it into the get_geodata() function\n\n\ncorine <-\n  d6geodata::get_geodata(\n    data_name = \"corine_berlin_2018_20m_03035_tif\",\n    path_to_cloud = \"E:/PopDynCloud\",\n    download_data = FALSE\n  )\n\n\nIf you set download_data = TRUE the data will be download and copied to your data-raw folder. If the data-raw folder doesn‚Äôt exist, it will be created.\nIf you want to download more than one file, you can simply use lapply() and add multiple file names like this:\n\n\ndata_list <-\n  lapply(\n    c(\n      \"corine_berlin_2018_20m_03035_tif\",\n      \"motorways_berlin_2022_100m_03035_tif\"\n    ),\n    FUN = function(x) {\n      d6geodata::get_geodata(\n        data_name = x,\n        path_to_cloud = \"E:/PopDynCloud\",\n        download_data = FALSE\n      )})\n\n\nAdditional functions\nThe three functions plot_binary_map(), plot_qualitative_map() and plot plot_quantitative_map() can be used to plot raster data with the respective color schemes we used for the Geodata wiki page (note that this function works only for raster data).\n\n\nplot_binary_map(tif = tif)\nplot_qualitative_map(tif = tif)\nplot_quantitative_map(tif = tif)\n\n\nExample plot\n\n\nlibrary(d6geodata)\nplot_qualitative_map(tif = corine)\n\n\n\n\n\n\n",
    "preview": "posts/d6geodatapackage/d6geodatapackage_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2023-07-03T16:29:12+02:00",
    "input_file": {}
  },
  {
    "path": "posts/d6berlin-fisbroker/",
    "title": "Download Spatial Data from the FIS-Broker Data Base in R",
    "description": "Learn how to download WFS (vector) data and ATOM (raster) data from the fisbroker data base using the our custom functions `download_fisbroker_wfs()` and `download_fisbroker_atom()`.",
    "author": [
      {
        "name": "Moritz Wenzler-Meya",
        "url": {}
      }
    ],
    "date": "2023-02-15",
    "categories": [
      "tutorial",
      "spatial"
    ],
    "contents": "\n\nContents\nThe FIS-Broker database\nWFS Data\nATOM Data\n\nThe {d6berlin} package provides several functions for the members of the Ecological Department of the IZW. Now two functions are added to the package: download_fisbroker_wfs() and download_fisbroker_atom().\n\n\n## devtools::install_github(\"EcoDynIZW/d6berlin\")\n## #install.packages(\"rcartocolor\")\n#install.packages(\"stars\")\n\n\n\n\nlibrary(d6berlin)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n\nThe FIS-Broker database\nThe FIS-Broker database is hosted by the Berlin Senate and provides several geographical data sets. The file formats differ and some data sets have just one of the file formats to offer. The file formats are WMS (Web Media Service: Just like a png or jpg), WFS (Web Feature Service: Shapefiles) and ATOM (xml format: Raster layers data). This function is only looking for WFS files (Shapefiles), because these are the polygons, lines or points that we are looking for.\nFor using these two functions you have to select the layer you aim to download from the online data base.\nWFS Data\nAs an example we will download the layer containing the districts of Berlin (‚ÄúALKIS Bezirke‚Äù):\n\n\n\nurl <- \"https://fbinter.stadt-berlin.de/fb/wfs/data/senstadt/s_wfs_alkis_bezirk\"\n\ndata_wfs <- d6berlin::download_fisbroker_wfs(link = url)\n\nReading layer `s_wfs_alkis_bezirk' from data source \n  `https://fbinter.stadt-berlin.de/fb/wfs/data/senstadt/s_wfs_alkis_bezirk?service=wfs&version=2.0.0&request=GetFeature&typenames=s_wfs_alkis_bezirk&srsName=EPSG%3A25833' \n  using driver `GML'\nSimple feature collection with 12 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 370000.8 ymin: 5799521 xmax: 415786.6 ymax: 5837259\nProjected CRS: ETRS89 / UTM zone 33N\n\nglimpse(data_wfs)\n\nRows: 12\nColumns: 7\n$ gml_id <chr> \"s_wfs_alkis_bezirk.1\", \"s_wfs_alkis_bezirk.2\", \"s_wf~\n$ gem    <int> 3, 9, 7, 2, 6, 11, 4, 10, 5, 12, 1, 8\n$ namgem <chr> \"Pankow\", \"Treptow-K√∂penick\", \"Tempelhof-Sch√∂neberg\",~\n$ namlan <chr> \"Berlin\", \"Berlin\", \"Berlin\", \"Berlin\", \"Berlin\", \"Be~\n$ lan    <int> 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11\n$ name   <int> 11000003, 11000009, 11000007, 11000002, 11000006, 110~\n$ geom   <MULTIPOLYGON [m]> MULTIPOLYGON (((391281.1 58..., MULTIPOLYGON (((41323~\n\nggplot() +\n  geom_sf(data = data_wfs, aes(fill = namgem)) +\n  rcartocolor::scale_fill_carto_d(palette = \"Bold\")\n\n\n\nYou got a spatial layer which you can save to disk or to use it directly.\nATOM Data\nAs an example we will download a raster of vegetation heights (‚ÄúVegetationsh√∂hen 2020 (Umweltatlas)‚Äù):\n\n\n\nurl <- \"https://fbinter.stadt-berlin.de/fb/atom/Vegetationshoehen/veghoehe_2020.zip\"\n\ndata_atom <-\n  d6berlin::download_fisbroker_atom(\n    zip_link = url,\n    path = \"_posts/d6berlin-fisbroker/man\",\n    name = \"vegetation_heights\"\n  )\n\nglimpse(data_atom)\n\nS4 class 'SpatRaster' [package \"terra\"]\n\ndata_atom_10 <- terra::aggregate(data_atom, 10)\n\nggplot() +\n  stars::geom_stars(data = stars::st_as_stars(data_atom_10)) +\n  coord_sf(expand = FALSE) + \n  rcartocolor::scale_fill_carto_c(\n    palette = \"Emrld\", name = NULL, \n    guide =  guide_legend(label.position = \"bottom\")\n  ) + \n  theme_void()\n\n\n\nA shortcut to plot this kind of data is the plot_qualitative_map() function from our dedicated d6geodata package. You can install this package with devtools::install_github(‚ÄúEcoDynIZW/d6geodata‚Äù).\n\n\nd6geodata::plot_quantitative_map(tif = data_atom_10)\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-07-03T16:29:09+02:00",
    "input_file": {}
  },
  {
    "path": "posts/NLRX/",
    "title": "Run NetLogo Simulations in R",
    "description": "Learn how to use the {NLRX} R package to run NetLogo models/experiments. This guide gives a brief overview of the basic functionality, how to quickly apply it to a model and shows some example (visual) outputs.",
    "author": [
      {
        "name": "Tobias K√ºrschner",
        "url": {}
      }
    ],
    "date": "2022-11-23",
    "categories": [
      "tutorial",
      "NetLogo",
      "rstats",
      "modelling"
    ],
    "contents": "\n\nContents\nLibraries and Folders\nSetting Up the Library to Work with Your NetLogo Model\nCreating an Experiment\nCreating a Simulation\nRunning the Experiment\nNLRX output handling\n\nCleaning the data\nExample plots\n\nThe NLRX package provides tools to setup and execute NetLogo simulations from R developed by Salecker et al.¬†2019. NetLogo is a free, open-source and cross-platform modelling environment for simulating natural and social phenomena.\nLibraries and Folders\n\n\n# libraries\n\nfor (pckg in c('tidyverse', 'viridis', 'scico', 'nlrx'))\n{\n  if (!require(pckg, character.only = TRUE))\n    install.packages(pckg, dependencies = TRUE)\n  require(pckg, character.only = TRUE)\n}\n\n# output folder\n\ncurrentDate <- gsub(\"-\", \"\", Sys.Date())\n# todaysFolder <- paste(\"Output\", currentDate, sep = \"_\")\n# dir.create(todaysFolder) #in case output folder is wanted\n\n#virtual-ram if needed\n#memory.limit(85000)\n\n\nSetting Up the Library to Work with Your NetLogo Model\n\n\n#NetLogo path\nnetlogoPath <- file.path(\"C:/Program Files/NetLogo 6.2.2/\")\n\n#Model location\nmodelPath <- file.path(\"./Model/myModel.nlogo\")\n\n#Output location (hardly ever used)\noutPath <- file.path(\"./\")\n\n#Java\nSys.setenv(JAVA_HOME=\"C:/Program Files/Java/jre1.8.0_331\") \n\n\nConfigure the NetLogo object:\n\n\nnl <- nlrx::nl(\n  nlversion = \"6.2.2\",\n  nlpath = netlogoPath,\n  modelpath = modelPath,\n  jvmmem = 1024 # Java virtual machine memory capacity\n)\n\n\nCreating an Experiment\n\n\nnl@experiment <- nlrx::experiment(\n  expname = 'Exp_1_1',\n  # name\n  outpath = outPath,\n  # outpath\n  repetition = 1,\n  # number of times the experiment is repeated with the !!!SAME!!! random seed\n  tickmetrics = 'true',\n  # record metrics at every step\n  idsetup = 'setup',\n  # in-code setup function\n  idgo = 'go',\n  # in-code go function\n  runtime = 200,\n  # soft runtime-cap\n  metrics = c('population',  # global variables to be recorded, can also use NetLogos 'count' e.g. count turtles\n              'susceptible', # but requires escaped quotation marks for longer commands when strings are involved\n              'infected',    # functions similar for both patch and turtle variables (below)\n              'immune'),\n  metrics.patches = c('totalINfectionsHere',\n                      'pxcor',\n                      'pycor'),\n # metrics.turtles = list(\n #   \"turtles\" = c(\"xcor\", \"ycor\")\n # ),\n  constants = list(  # model parameters that are fixed. In theory all the constant values set in the UI before saving are the ones used\n    'duration' = 20, # however, I would always make sure to 'fix\" them though the constant input\n    'turtle-shape' = \"\\\"circle\\\"\",\n    'runtime' = 5\n  ),\n  variables = list( # model parameters you want to 'test' have several ways to be set\n    'number-people' = list(values = c(150)),     # simple list of values\n    'infectiousness' = list(                      #stepwise value change\n      min = 50,\n      max = 100,\n      step = 25,\n      qfun = 'qunif'\n    ),\n    # string based inputs such as used in NetLogo's 'choosers' or inputs require escaped quotation marks\n    # \"turtle-shape\" = list(values = c(\"\\\"circle\\\"\",\"\\\"person\\\"\")), \n    'chance-recover' = list(values = c(50, 75, 95))\n  )\n)\n\n\nCreating a Simulation\nHere we have several choices of simulation types. The full factorial simdesign used below creates a full-factorial parameter matrix with all possible combinations of parameter values. There are however, other options to choose from if needed and the vignette provides a good initial overview.\nA bit counter intuitive but nseeds used in the simdesign is actually the number of repeats we want to use for our simulation. In case we set nseed = 10 and have set the the repetitions above to 1, we would run each parameter combination with 10 different random seeds i.e.¬†10 times per combination. If we would have set the repetitions to 2 we would run each random seed 2 times i.e.¬†20 times in total but twice for each seed so 2 results should be identical.\nHowever, if your model handles seeds internally such as setting a random seed every time the model is setup, repetitions could be used instead.\n\n\nnl@simdesign <- nlrx::simdesign_ff(nl=nl, nseeds=1) \n\nprint(nl)\n\nnlrx::eval_variables_constants(nl)\n\n\nRunning the Experiment\nSince the simulations are executed in a nested loop where the outer loop iterates over the random seeds of the simdesign, and the inner loop iterates over the rows of the parameter matrix. These loops can be executed in parallel by setting up an appropriate plan from the future package which is built into nlrx.\n\n\n#plan(multisession, workers = 12) # one worker represents one CPU thread\n\nresults<- nlrx::run_nl_all(nl = nl)\n\n\nto speed up this this tutorial we load a pre-generated simulation result\n\n\nresults <- readr::read_rds(\"example1.rds\")\n\ndplyr::glimpse(results)\n\nRows: 1,809\nColumns: 15\n$ `[run number]`   <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1~\n$ `number-people`  <dbl> 150, 150, 150, 150, 150, 150, 150, 150, 150~\n$ infectiousness   <dbl> 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,~\n$ `chance-recover` <dbl> 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,~\n$ duration         <dbl> 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,~\n$ `turtle-shape`   <chr> \"circle\", \"circle\", \"circle\", \"circle\", \"ci~\n$ runtime          <dbl> 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5~\n$ `random-seed`    <dbl> -662923684, -662923684, -662923684, -662923~\n$ `[step]`         <dbl> 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1~\n$ population       <dbl> 0, 153, 155, 156, 156, 157, 158, 159, 160, ~\n$ susceptible      <dbl> 0, 10, 10, 10, 11, 11, 13, 13, 16, 16, 17, ~\n$ infected         <dbl> 0, 143, 145, 146, 145, 146, 145, 146, 144, ~\n$ immune           <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~\n$ metrics.patches  <list> [<tbl_df[1225 x 5]>], [<tbl_df[1225 x 5]>]~\n$ siminputrow      <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1~\n\nNLRX output handling\nThere are several ways to use / analyze the output directly via nlrx (although I have not used them personally) detailed information can be found here: https://cran.r-project.org/web/packages/nlrx/nlrx.pdf\n\n\nnlrx::setsim(nl, \"simoutput\") <- results # attaching simpout to our NetLogo object\n\n#write_simoutput(nl) # having nlrx write the output into a file only works without patch or turtle metrics\n\nnlrx::analyze_nl(nl, metrics = nlrx::getexp(nl, \"metrics\"), funs = list(mean = mean))\n\n\nCleaning the data\nNetLogo really dislikes outputting nice and readable variable names so some renaming is in order:\n\n\nraw1 <- results\n\nraw2 <-\n  raw1 %>% dplyr::rename(\n    run = `[run number]`,\n    recoveryChance = `chance-recover`,\n    StartingPop = `number-people`,\n    t = `[step]`\n  )\n\n\nIn case of very large datasets we want to separate the patch (or turtle) specific data from the global data to speed up the analysis of the global data\n\n\nglobalData <- raw2 %>% dplyr::select(!metrics.patches)#, !metrics.turtles)\n\n\nand summarise like any other dataset\n\n\nsum_1 <-\n  globalData %>%\n  dplyr::group_by(t, recoveryChance, infectiousness) %>%\n  dplyr::summarise(\n    across(\n      c('infected',\n        'immune',\n        'susceptible',\n        'population'),\n      mean\n    )\n  ) %>%\n  dplyr::filter(t < 201) %>%\n  tidyr::pivot_longer(cols = -c(recoveryChance, infectiousness, t),\n                      names_to = \"EpiStat\") %>%\n  dplyr::ungroup()\n\n\nExample plots\nWe use the global data to get an overview of the simulated SIR dynamics:\n\n\nggplot(sum_1) +\n  geom_line(aes(x = t, y = value, colour = EpiStat), size = 1) +\n  facet_grid(infectiousness ~ recoveryChance, labeller = label_both) +\n  scale_colour_scico_d(\n    palette = \"lajolla\",\n    begin = 0.1,\n    end = 0.9,\n    direction = 1\n  ) +\n  theme_bw()\n\n\n\n‚Ä¶ and use the patch specific data to create a heatmap to visualize where most infections have happened in one specific scenario:\n\n\nhmpRaw <- raw2 %>%\n  dplyr::filter(infectiousness == 75 & recoveryChance == 75) %>%\n  dplyr::select(metrics.patches, t) %>%\n  dplyr::rename(pm = metrics.patches)\n\nhmpSplit <- hmpRaw %>% split(hmpRaw$t)\ntempDf <- hmpSplit[[1]]$pm \ntempDf <- tempDf %>% as.data.frame() %>%\n  dplyr::select(totalINfectionsHere, pxcor, pycor)\n\nfor (i in 1:length(hmpSplit))\n{\n  tmp1 <- as.data.frame(hmpSplit[[i]]$pm)\n  colnames(tmp1) <- c(paste0(\"tab_\", i), 'pxcor', 'pycor', 'agent', 'breed')\n  tmp1 <- tmp1 %>% \n    dplyr::select(-c('pxcor', 'pycor', 'agent', 'breed'))\n  tempDf <- cbind(tempDf, tmp1)\n}\n\ntempDf2 <- tempDf %>% \n  dplyr::select(-c(pxcor, pycor)) \n\ntempDf$summ <- rowSums(tempDf2)\nhmpData <- tempDf %>% dplyr::select(summ, pxcor, pycor)\n\nggplot(hmpData)+\n  geom_tile(aes(x = pxcor, y = pycor, fill = summ)) +\n  scale_fill_scico(palette = 'roma', direction = -1, name= 'N infected') +\n  ggtitle(\"N infections per cell\") +\n  theme_bw()\n\n\n\n\n\n\n",
    "preview": "posts/NLRX/nlrxpackage_files/figure-html5/global plot-1.png",
    "last_modified": "2023-07-03T16:29:04+02:00",
    "input_file": {}
  },
  {
    "path": "posts/coding-basics/",
    "title": "Coding Basics 2: Loops and Functions",
    "description": "A brief introduction into various coding basics for people who are beginning to use R or other programming languages. In this session, we will be looking at both functions and loops in R, with examples from NetLogo and C++ and go over use and basic functionality.",
    "author": [
      {
        "name": "Tobias K√ºrschner",
        "url": {}
      }
    ],
    "date": "2022-11-14",
    "categories": [
      "tutorial",
      "workflow",
      "rstats",
      "NetLogo"
    ],
    "contents": "\n\nContents\nFunctions and Loops\nWhat are they for?\nFunctions\nLoops\nA Classic, the ‚Äòfor‚Äô Loop\n‚Äòwhile‚Äô Loops\n\nBonus Round: Conditionals\n\n\n\n\n\nFunctions and Loops\nWhat are they for?\nIn short: making you life easier. They are used to automate certain steps in your code to make the execution faster.\nFunctions\nA simple example: you have multiple data sets with a measurement in inches. To continue working with that data you need to convert it meter, but doing that manually would take hours. Solution: a function!!\nIn R:\n\n\ninch_to_meter <-   #function name\n  function(inch) #function input parameter(s)\n  { # function body\n    inM <- (inch * 0.0254) # in our case: transformation\n    \n    return(inM) # function reporter\n  }\n\n\nNow lets apply that function to some random example data:\n\n\nmyOldData <- c(15,98,102,5,17)\n\nmyNewData <- inch_to_meter(myOldData)\n\n\nOur function is applied to all elements of the old data, giving us the converted measurements.\n\n\nmyOldData\n\n[1]  15  98 102   5  17\n\nmyNewData\n\n[1] 0.3810 2.4892 2.5908 0.1270 0.4318\n\nFor a more general approach we can use the following template for simple functions:\n\n\nmyFunction <-\n  function(input1, input2, input3)\n  {\n    output <- input1 * input3 / input2 # example operation\n    \n    return(output)\n  }\n\n\nIn C++:\n\n# the viableCell function of the class Grid returns a bool and takes two inputs\n# (x and y coordinates)\nbool Grid::viableCell(CellCount_t x, CellCount_t y) \n{\n    return !m_grid.empty() && x < m_grid.cbegin() -> size() && y < m_grid.size();\n}\n\nIn NetLogo, the closest thing we have to functions are reporters:\n\nto-report Male_Alpha_Alive \n\n  ifelse (any? turtles-here with [isAlpha = true AND isFemale = false])\n  [report true]\n  [report false] #else\n\nend\n\nLoops\nWhat is a loop? It a simply a piece of code we want to repeat. But wait, isn‚Äôt that exactly what a function does? Well, yes and no. A function (after it is declared) is an embodiment of a piece of code that we can run anytime just by calling it. A loop is a local repetition of code.\nLets stay in R and look at some examples:\nA Classic, the ‚Äòfor‚Äô Loop\nIf you auto fill ‚Äúfor‚Äù in R, it will give you the following structure:\n\n\nfor (variable in vector)\n{\n  #body\n}\n\n\nSo what is variable and what is vector? In this case the variable, you could also call iterator (often you will see the letter i used) is simply put a counter that tells our loop how many times it has repeated itself. The vector is determined by us and tells the loop how many repetitions we want before it stops.\n\n\nfor (i in 1:10)\n{\n  #body\n}\n\n\nThe loop above will now repeat exactly 10 times and then stop. R is helpful in the sense that it automatically increments i after each repetition. Other language like C++ for example need a manual increment of i:\n\nfor (i = 0, i <= 10, ++i) #C++ \n{\n  #body\n}\n\nWith the basics out of the way lets look at an example:\n\n\nmyData <- rnorm(30) # random numbers\nmyResults <- 0 # initializing myResults\n\nfor(i in 1:10)\n{\n  # i-th element of `myData` squared into `i`-th position of `myResults`\n  myResults[i] <- myData[i] * myData[i]  \n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n\nmyResults\n\n [1] 2.348470e+00 8.045558e-02 9.254717e+00 6.298287e-01 5.587004e-02\n [6] 5.496863e-01 2.161664e+00 1.326536e-05 2.026844e+00 2.065682e-01\n\nThis was of course a very simple loop and there is pretty much no limit to the level of complexity those loops can have and how many loops could be nested. But be warned there can be some pitfalls with loops. Exercise: Would the following loop work?\n\n\nfor (i in 1:length(summIdentSplit))\n{\n  tmpSumm <- summIdentSplit[[i]]\n  tmpName <- SummIdentVector[[i]]\n  \n  if (base::grepl(\"HD\", tmpName) == TRUE)\n  {\n    clearName <- \"Habitat-driven movement\"\n    cn <- \"HD\"\n  }\n  \n  combiMelt <- tmpSumm %>%\n    dplyr::ungroup() %>%\n    dplyr::select(t, Ninfected_mean, Nimmune_mean) %>%\n    dplyr::rename(Infected = Ninfected_mean, Immune = Nimmune_mean)\n  \n  combiMelt_sd <- tmpSumm %>%\n    dplyr::ungroup() %>%\n    dplyr::select(t, Ninfected_sd, Nimmune_sd) %>%\n    dplyr::rename(Inf_sd = Ninfected_sd, Imm_sd = Nimmune_sd)\n  \n  combiMelt_c <- dplyr::left_join(combiMelt, combiMelt_sd , by = \"t\")\n  \n  q <- 0\n  combiMelt_c$quarter <- 0\n  \n  for (i in 1:nrow(combiMelt_c))\n  {\n    if (i %% 13 == 0)\n    {\n      q <- q + 1\n    }\n    combiMelt_c$quarter[i] <- q\n  }\n}\n\n\nThe answer is no. There are two loops involved where one is nested inside the other. So far, that would not be an issue, however, both loops use the iterator (variable) i. To see what would happen:\n\nFirst iteration:\n  i = 1\n\nfor (i in 1:length(summIdentSplit))\n{\n  tmpSumm <- summIdentSplit[[i]]\n  tmpName <- SummIdentVector[[i]]\n  \n  if (base::grepl(\"HD\", tmpName) == TRUE)\n  {\n    clearName <- \"Habitat-driven movement\"\n    cn <- \"HD\"\n  }\n  \n  combiMelt <- tmpSumm %>%\n    dplyr::ungroup() %>%\n    dplyr::select(t, Ninfected_mean, Nimmune_mean) %>%\n    dplyr::rename(Infected = Ninfected_mean, Immune = Nimmune_mean)\n  \n  combiMelt_sd <- tmpSumm %>%\n    dplyr::ungroup() %>%\n    dplyr::select(t, Ninfected_sd, Nimmune_sd) %>%\n    dplyr::rename(Inf_sd = Ninfected_sd, Imm_sd = Nimmune_sd)\n  \n  combiMelt_c <- dplyr::left_join(combiMelt, combiMelt_sd , by = \"t\")\n  \n  q <- 0\n  combiMelt_c$quarter <- 0\n\nAt this point i is still 1 and lets say nrow(combiMelt_c) is also 10 (like in our outer loop)\n\n  for (i in 1:nrow(combiMelt_c))\n  {\n    if (i %% 13 == 0)\n    {\n      q <- q + 1\n    }\n    combiMelt_c$quarter[i] <- q\n\nat this point i is iterated within the inner loop\n\n  }\n\nonce we reach this point i is 10, so the for the next iteration of the outer\nloop, i = 10 so most iterations of the outer loop will be skipped!\n\n}\n\nSolution: make sure to use different iterators in nested loops. For example the outer loop uses i, the inner loop uses j and maybe that loop has also a nested loop which then uses k as its iterator.\n‚Äòwhile‚Äô Loops\nSimilar to for loops, while loops also repeat a certain block of code. The difference here is, that while loop repeat until a certain condition is fulfilled, potentially forever. R‚Äôs auto fill provides us the following code snippet:\n\n\nwhile (condition)\n{\n  \n}\n\n\nA simple example:\n\n\nn <- 0\n\nwhile(n < 100)\n{\n  n = n + 1\n}\n\nprint(n)\n\n[1] 100\n\nAs long as n is below 100 we increment n every repeat. Take note: if the condition is never fulfilled, the loop will run forever and the software might crash. In complex loops you could run a ‚Äúescape timer‚Äù such as in this example plucked from an IBM:\n\n\nwhile(!viableCell())\n{\n   turn(2 * (atan (( (1 - 0.5) / (1 + 0.9)) * tan ((randomDouble(1) - 0.5)) * 180)) + 1) \n}\n\n\nThis little loop runs on an individual and check‚Äôs the cell in front of the individual for its viability to move into. As long as viableCell() reports false, the individual turns. However, there are cases when they are no viable cells around so the individual would turn in a circle forever. Introducing a random timer (note, this is not an optimal solution just a quick fix).\n\nint timer = 0\n\nwhile(!viableCell() && timer < 50 )\n{\n   turn(2 * (atan (( (1 - 0.5) / (1 + 0.9)) * tan ((randomDouble(1) - 0.5)) * 180)) + 1) \n  \n  ++timer #increment the timer\n}\n\nNow the individual turns a maximum of 50 times before the loop ends. Another option would be conditionals:\nBonus Round: Conditionals\nMost people already know what a conditional is. Basically, when a certain condition is fulfilled something happens (usually done via if and/or else).\n\nint timer = 0\n\nwhile(!viableCell())\n{\n   turn(2 * (atan (( (1 - 0.5) / (1 + 0.9)) * tan ((randomDouble(1) - 0.5)) * 180)) + 1) \n  \n  ++timer #increment the timer\n  \n  if(timer == 50)\n  {\n    break # break is a function that for example ends a loop\n  }\n}\n\nConditionals can be used in a variety of way within and outside of loops but have the advantage that they can be used stop loops under certain conditions. Lets say you are running a complex construct of multiple nested for loops to find a certain value in your data. You don‚Äôt need to always iterate through all the data if you can create certain logical stop conditions.\n\n\n\n",
    "preview": {},
    "last_modified": "2023-07-03T16:29:07+02:00",
    "input_file": {}
  },
  {
    "path": "posts/coding-goodpractice/",
    "title": "Coding Basics 1: General Tips",
    "description": "A brief introduction into various coding basics for people who are beginning to use R or other programming languages. In this session, we will be looking at some general tips and information that helps creating good and understandable code. Specifically we are looking at naming conventions, indentations and commenting of code.",
    "author": [
      {
        "name": "Tobias K√ºrschner",
        "url": {}
      }
    ],
    "date": "2022-11-13",
    "categories": [
      "tutorial",
      "workflow",
      "rstats"
    ],
    "contents": "\n\nContents\nGeneral:\nVariable Naming Conventions\nSnakecase:\nCamelcase:\nPascalcase:\nHungarian Notation:\nSidenote: Namespaces and Function Names\n\nIndentation\nCommenting Your Code\nDon‚Äôt Repeat Yourself\n\nGeneral:\nWrite as few lines as possible.\nUse appropriate naming conventions.\nSegment blocks of code in the same section into paragraphs.\nUse indentation to marks the beginning and end of control structures.\nDon‚Äôt repeat yourself.\n\n\n\nVariable Naming Conventions\nVariable naming is an important aspect in making your code readable. Create variables that describe their function and follow a consistent theme throughout your code. Separate words in a variable name without the use of whitespace and do not repeat variable names (unless in certain circumstances such as temporary variable within loops).\nSnakecase:\nWords are delimited by an underscore.\n\n\nvariable_one <- 1\n\nvariable_two <- 2\n\n\nCamelcase:\nWords are delimited by capital letters, except the initial word.\n\n\nvariableOne <- 1\n\nvariableTwo <- 2\n\n\nPascalcase:\nWords are delimited by capital letters.\n\n\nVariableOne <- 1\n\nVariableTwo <- 2\n\n\nHungarian Notation:\nThis notation describes the variable type or purpose at the start of the variable name, followed by a descriptor that indicates the variable‚Äôs function. The Camelcase notation is used to delimit words.\n\n\niVariableOne   <- 1  # i - integer\n\nsVariableTwo   <- \"two\" # s - string\n\nlVariableThree <- list() # l - list\n\n\nSidenote: Namespaces and Function Names\nOften when using for example R we will use libraries and many of them at the same time. Quite a few of those libraries are using the same names for their functions. This can be a big issues and cause code to suddenly not run anymore even though the only difference my be the order in which libraries are loaded. The last library loaded with a certain function will be the default one used by R. A prominent example would be the ‚Äòraster‚Äô package and ‚Äòtidyverse‚Äô (dplyr) who share the name ‚Äòselect‚Äô for a function. So, when in doubt use namespaces to make sure to link a function to their library.\n\nuse the select function from the dplyr library\ndplyr::select(.....)\n\nIndentation\nI assume you already know that your code should have some sort of indentation. However, it‚Äôs also worth noting that its a good idea to keep your indentation style consistent.\nAs a quick reminder:\nBad:\nLong lines of text with no separation:\n\n\nmydata <- iris %>% dplyr::filter(Species == \"virginica\") %>% summarise_at(.vars = c(\"Sepal.Length\", \"Sepal.Width\"),.funs = \"mean\")\n\nfor (i in 1:length(hmpSplit)){ tmp1 <- as.data.frame(hmpSplit[[i]]$pm); colnames(tmp1) <- c(paste0(\"tab_\", i), 'pxcor', 'pycor', 'agent', 'breed'); tmp1 <- tmp1 %>% dplyr::select(-c('pxcor', 'pycor', 'agent', 'breed')); tempDf <- cbind(tempDf, tmp1)}\n\n\nGood:\n\n\nmydata <- iris %>%\n  dplyr::filter(Species == \"virginica\") %>%\n  summarise_at(.vars = c(\"Sepal.Length\", \"Sepal.Width\"),\n               .funs = \"mean\")\n\n\nfor (i in 1:length(hmpSplit))\n{\n  tmp1 <- as.data.frame(hmpSplit[[i]]$pm)\n  \n  colnames(tmp1) <- c(paste0(\"tab_\", i), 'pxcor', 'pycor', 'agent', 'breed')\n  \n  tmp1 <- tmp1 %>% \n    dplyr::select(-c('pxcor', 'pycor', 'agent', 'breed'))\n  \n  tempDf <- cbind(tempDf, tmp1)\n}\n\n\nThe details on how to indent or use white spaces are up to individual styles with some guidelines, such as avoiding long lines of text, to keep in mind. Also fine would be something like the following.\n\n\nfor (i in 1:length(hmpSplit))\n{\n  tmp1 <- as.data.frame(hmpSplit[[i]]$pm)\n  colnames(tmp1) <- c(paste0(\"tab_\", i), 'pxcor', 'pycor', 'agent', 'breed')\n  tmp1 <- tmp1 %>% dplyr::select(-c('pxcor', 'pycor', 'agent', 'breed'))\n  tempDf <- cbind(tempDf, tmp1)\n}\n\n\nCommenting Your Code\nCommenting your code is fantastic but it can be overdone or just be plain redundant. Comments should add information or explanations to make your code understandable for people who didn‚Äôt write it or yourself in a year from now:\n\n\n# comment that adds information:\n\nwrite_simoutput(nl) # having nlrx write the output into a file - only works without patch or turtle metrics\n\n\n# redundant comments:\n\nif (col == \"blue\") # if colour is blue\n{\n  print('colour is blue') # print that the colour is blue\n}\n\n\nA better solution (if a comment is absolutely necessary) would be:\n\n\n# display selected colour\nif (colour == \"blue\")\n{\n  print('colour is blue')\n}\n\n\nDon‚Äôt Repeat Yourself\nAs a rule of thumb, if you have to do the same task multiple times in your code: automate it. A while back, I was decomposing many time series and I needed only part of the output, in this case the ‚Äòtrend‚Äô. Instead of running the same lines of code that remove all other components for each time series individually, a short function reduced the amount of needed code substantially.\n\n\nDecompTrend <- function(ts){\n  \n  temp1<-stats::decompose(ts)\n  return(temp1$trend)\n}\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-07-03T16:29:07+02:00",
    "input_file": {}
  },
  {
    "path": "posts/netlogo-weibull/",
    "title": "Turning Equations into Functions in NetLogo",
    "description": "While some programming languages, such as R, offer many native functions, others (e.g., NetLogo) offer fewer built-in options. However, users can create their own functions easily. Here, we will show the example of a Weibull density distribution function & associated cumulative density distribution function‚Äîboth not yet implemented in NetLogo or NetLogo extensions.",
    "author": [
      {
        "name": "Marius Grabow",
        "url": {}
      }
    ],
    "date": "2022-11-11",
    "categories": [
      "NetLogo",
      "rstats",
      "distributions"
    ],
    "contents": "\n\nContents\nDensity distribution function\nR\nNetLogo\n\nCumulative density function\nR\nNetLogo\n\n\nDensity distribution\nfunction\nLet‚Äôs first take a look at the Weibull density distribution\nfunction:\n\\[\\begin{equation}\n\nf(x) = \\frac{\\gamma} {\\alpha} (\\frac{x-\\mu}\n{\\alpha})^{(\\gamma - 1)}\\exp{(-((x-\\mu)/\\alpha)^{\\gamma})}\n\\hspace{.3in}  x \\ge \\mu; \\gamma, \\alpha > 0\n\n\\end{equation}\\]\nR\nIn R, this is already implemented:\n\n\nscale <- 3\nshape <- 1\n\ndweibull(scale, shape = shape)\n\n\n[1] 0.04978707\n\nNetLogo\nIn Netlogo we can simply translate the mathematical equation into a\nfunction:\nto-report weibull [a_scale a_shape x]\n\n  let Wei (a_shape / a_scale ) * ((x / a_scale)^(a_shape - 1)) * exp( - ((x / a_scale)^ a_shape))\n\n  report Wei\n\nend\nCumulative density function\nLet‚Äôs also take a look at the Weibull cumulative density\nfunction:\n\\[\\begin{equation}\n\nF(x) = 1 - e^{-(x^{\\gamma})} \\hspace{.3in}  x \\ge 0; \\gamma > 0\n\n\\end{equation}\\]\nR\nAgain, fully implemented in R already:\n\n\nscale <- 3\nshape <- 1\nx <- 5\n\npweibull(q = x, scale = scale, shape = shape)\n\n\n[1] 0.8111244\n\nNetLogo\nIn NetLogo we can simply translate the mathematical equation into a\nfunction:\nto-report weibull_cumulative [a_scale a_shape x]\n\n  let Wei_cumu 1 - exp( - ((x / a_scale)^ a_shape))\n\n  report Wei_cumu\n\nend\n\n\n\n",
    "preview": {},
    "last_modified": "2023-07-03T16:29:23+02:00",
    "input_file": {}
  },
  {
    "path": "posts/imageseg/",
    "title": "imageseg",
    "description": "R package for deep learning image segmentation using the U-Net model architecture by Ronneberger (2015), implemented in Keras and TensorFlow. It provides pre-trained models for forest structural metrics (canopy density and understory vegetation density) and a workflow to apply these on custom images",
    "author": [
      {
        "name": "J√ºrgen Niedballa",
        "url": {}
      }
    ],
    "date": "2022-09-12",
    "categories": [],
    "contents": "\nimageseg\nR package for deep learning image segmentation using the U-Net model architecture by Ronneberger (2015), implemented in Keras and TensorFlow. It provides pre-trained models for forest structural metrics (canopy density and understory vegetation density) and a workflow to apply these on custom images.\nIn addition, it provides a workflow for easily creating model input and model architectures for general-purpose image segmentation based on the U-net architecture. Model can be trained on grayscale or color images, and can provide binary or multi-class image segmentation as output.\nThe package can be found on CRAN.\nThe preprint of the paper describing the package is available on bioRxiv.\nInstallation\nFirst, install the R package ‚ÄúR.rsp‚Äù which enables the static vignettes.\n\n\n\nInstall the imageseg package from CRAN via:\n\n\n\nAlternatively you can install from GitHub (requires remotes package and R.rsp):\n\n\n\nUsing imageseg requires Keras and TensorFlow. See the vignette for information about installation and initial setup:\nTutorial\nSee the vignette for an introduction and tutorial to imageseg.\n\n\n\nThe vignette covers:\n- Installation and setup\n- Sample workflow for canopy density assessments\n- Training new models\n- Continued training of existing models\n- Multi-class image segmentation models\n- Image segmentation based on grayscale images\nForest structure model download\nThe models, example predictions, training data and R script for model training for both the canopy and understory model are available from Dryad as a single download.\nSee the ‚ÄúUsage Notes‚Äù section for details on the dataset.\nThe models and script (without the training data) are also hosted on Zenodo and can be downloaded individually from zenodo.\nThe pre-trained models for forest canopy density and understory vegetation density are available for download. The zip files contain the model (as .hdf5 files) and example classifications to give an impression of model performance and output:\nCanopy model\nUnderstory model\nPlease see the vignette for further information on how to use these models.\nTraining data download\nTraining data for both the canopy and understory model are included in the Dryad dataset download in the zip files:\nimageseg_canopy_training_data.zip\nimageseg_understory_training_data.zip\nFor details, please see the Usage Notes and the info.txt files contained in the zip files.\nThe training data are not required for users who only wish to use the pre-trained models on their own images.\n\n\n\n",
    "preview": "https://raw.githubusercontent.com/EcoDynIZW/EcoDynIZW.github.io/main/img/hex-imageseg.png",
    "last_modified": "2023-07-03T16:29:17+02:00",
    "input_file": {}
  },
  {
    "path": "posts/isorix/",
    "title": "Isorix",
    "description": "This is the development repository of IsoriX, an R package aiming at building isoscapes using mixed models and inferring the geographic origin of organisms based on their isotopic ratios",
    "author": [
      {
        "name": "Alexandre Courtiol",
        "url": {}
      }
    ],
    "date": "2022-09-12",
    "categories": [],
    "contents": "\nHow to download and install IsoriX?\nYou can download and install the stable version of IsoriX directly from within R by typing:\n\n\ninstall.packages(\"IsoriX\", dependencies = TRUE)\n\n\nNote: if you get into troubles due to gmp, magick, maps, maptools, RandomFields, rgeos, or rgl, retry using simply:\n\n\ninstall.packages(\"IsoriX\")\n\n\nThese packages offer additional functionalities but some of them are particularly difficult to install on some systems.\nIf you want the development version of IsoriX, you can download and install it by typing:\n\n\nremotes::install_github(\"courtiol/IsoriX/IsoriX\")\n\n\nMind that you need the R package remotes to be installed for that to work. Mind also that the development version, being under development, can sometimes be broken. So before downloading it make sure that the current build status is build passing. The current built status is provided at the top of this readme document.\nAlso, if you access the network via a proxy, you may experience troubles with install_github. In such case try something like:\n\n\nlibrary(httr)\nwith_config(use_proxy(\"192.168.2.2:3128\"), devtools::install_github(\"courtiol/IsoriX/IsoriX\"))\n\n\nOff course, unless you are in the same institute than some of us, replace the numbers with your proxy settings!\nWhere to learn about IsoriX?\nYou can start by reading our bookdown!\nThen, if may not be a bad idea to also have a look at our papers: here and there.\nAnother great source of help is our mailing list. First register for free (using your Google account) and then feel free to send us questions.\nFor specific help on IsoriX functions and objects, you should also check the documentation embedded in the package:\n\n\nhelp(package = \"IsoriX\")\n\n\nin R after having installed and attached (= loaded) the package.\nHow can you contribute?\nThere are plenty way you can contribute! If you are fluent in R programming, you can improve the code and develop new functions. If you are not so fluent, you can still edit the documentation files to make them more complete and clearer, write new vignettes, report bugs or make feature requests.\nSome useful links\n\n\n\n",
    "preview": "https://raw.githubusercontent.com/EcoDynIZW/EcoDynIZW.github.io/main/img/hex-isorix.png",
    "last_modified": "2023-07-03T16:29:17+02:00",
    "input_file": {}
  },
  {
    "path": "posts/camtrapr/",
    "title": "Manage Camera Trap Data in R",
    "description": "Learn how to use the {camtrapR} R package for streamlined and flexible camera trap data management. It should be most useful to researchers and practitioners who regularly handle large amounts of camera trapping data.",
    "author": [
      {
        "name": "J√ºrgen Niedballa",
        "url": {}
      }
    ],
    "date": "2022-05-23",
    "categories": [
      "tutorial",
      "rstats",
      "camera traping",
      "data management"
    ],
    "contents": "\n\nContents\nInstallation\nExiftool\n\nThe {camtrapR} R\npackage simplifies camera trap data management in R.\nInstallation\nYou can install the release version of {camtrapR} from\nCRAN:\n\n\ninstall.packages(\"camtrapR\")\n\n\n\nExiftool\nNumerous important {camtrapR} functions read EXIF\nmetadata from JPG images (and videos). This is done via\nExiftool, a free and open-source sofware tool developed by\nPhil Harvey and available for Windows, MacOS and Linux.\nTo make full use of {camtrapR}, you will need\nExiftool on your system. You can download it from the\nExiftool homepage and follow the installation instruction\nin vignette 1.\nYou may not need Exiftool if you do not work with image\nfiles, but only use {camtrapR} to create input for\noccupancy or spatial capture-recapture models from existing record\ntables.\nSee the article\nin Methods in Ecology and Evolution for an overview of the package.\nThe five vignettes provide examples for the entire workflow.\nCitation\n\n\n\n",
    "preview": "https://raw.githubusercontent.com/EcoDynIZW/EcoDynIZW.github.io/main/img/hex-camtrapr.png",
    "last_modified": "2023-07-03T16:29:05+02:00",
    "input_file": {}
  },
  {
    "path": "posts/nlmr/",
    "title": "Simulate Neutral Landscape Models",
    "description": "Learn how to use the {NLMR} R package to simulate neutral landscape models (NLMs), a common set of various null models for spatial analysis and as input for spatially-explicit, generic models.",
    "author": [
      {
        "name": "Cedric Scherer",
        "url": "htttps://cedricscherer.com"
      }
    ],
    "date": "2021-08-06",
    "categories": [
      "tutorial",
      "rstats",
      "spatial",
      "modelling"
    ],
    "contents": "\nThe {NLMR} R\npackage to simulate neutral landscape models (NLM). Designed to be a\ngeneric framework like NLMpy, it leverages the\nability to simulate the most common NLM that are described in the\necological literature.\nIf you want to learn more about the {NLMR} package and\nthe accompanying {landscapetools} package, check the\npublication Sciaini,\nFritsch, Scherer 6 Simpkins (2019) Methods in Ecology and\nEvolution.\nInstallation\nInstall the release version from CRAN:\n\n\ninstall.packages(\"NLMR\")\n\n\n\nor the development version from Github, along with two packages that\nare needed for the generation of random fields:\n\n\ninstall.packages(\"remotes\")\nremotes::install_github(\"cran/RandomFieldsUtils\")\nremotes::install_github(\"cran/RandomFields\")\nremotes::install_github(\"ropensci/NLMR\")\n\n\n\nUsage\nEach neutral landscape models is simulated with a single function\n(all starting with nlm_) in NLMR, e.g.:\n\n\nrandom_cluster <- NLMR::nlm_randomcluster(\n  nrow = 100,\n  ncol = 100,\n  p    = 0.5,\n  ai   = c(0.3, 0.6, 0.1),\n  rescale = FALSE\n)\n\nrandom_curdling <- NLMR::nlm_curds(\n  curds = c(0.5, 0.3, 0.6),\n  recursion_steps = c(32, 6, 2)\n)\n\nmidpoint_displacement <- NLMR::nlm_mpd(\n  ncol = 100,\n  nrow = 100,\n  roughness = 0.61\n)\n\n\n\n{NLMR} supplies 15 NLM algorithms, with several options\nto simulate derivatives of them. The algorithms differ from each other\nin spatial auto-correlation, from no auto-correlation (random NLM) to a\nconstant gradient (planar gradients).\nThe package builds on the advantages of the raster package and\nreturns all simulation as RasterLayer objects, thus\nensuring a direct compatibility to common GIS tasks and a flexible and\nsimple usage:\n\n\nclass(random_cluster)\n\n\n[1] \"RasterLayer\"\nattr(,\"package\")\n[1] \"raster\"\n\nrandom_cluster\n\n\nclass      : RasterLayer \ndimensions : 100, 100, 10000  (nrow, ncol, ncell)\nresolution : 1, 1  (x, y)\nextent     : 0, 100, 0, 100  (xmin, xmax, ymin, ymax)\ncrs        : NA \nsource     : memory\nnames      : clumps \nvalues     : 1, 3  (min, max)\n\nVisualization\nThe {landscapetools}\npackage provides a function show_landscape that was\ndeveloped to plot raster objects and help users to adhere to some\nstandards concerning color scales and typography. This means for example\nthat by default the viridis color scale is applied (and you can pick\nothers from the {viridis} package, too).\n\n\n#install.packages(\"landscapetools\")\n\n## plotting continuous values\nlandscapetools::show_landscape(random_cluster)\n\n\n\n## plotting discrete values\nlandscapetools::show_landscape(random_curdling, discrete = TRUE)\n\n\n\n## using another viridis palette\nlandscapetools::show_landscape(midpoint_displacement, viridis_scale = \"magma\")\n\n\n\n\n\n\n\n",
    "preview": "https://raw.githubusercontent.com/EcoDynIZW/EcoDynIZW.github.io/main/img/hex-nlmr.png",
    "last_modified": "2023-07-03T16:29:26+02:00",
    "input_file": {}
  },
  {
    "path": "posts/d6berlinpackage/",
    "title": "Spatial Datasets and Template Maps for Berlin",
    "description": "Learn how to use the {d6berlin} R package that provides spatial data sets and template maps for Berlin with carefully chosen and aesthetically pleasing defaults. The template maps include Berlin districts, green spaces, imperviousness levels, water bodies, district borders, roads, metro stations, and railways. Templates maps are created with a {ggplot2} wrapper function with the utility to add an inset globe with a locator pin, a scalebar, and a caption stating the data sources.",
    "author": [
      {
        "name": "Cedric Scherer",
        "url": "htttps://cedricscherer.com"
      }
    ],
    "date": "2021-03-18",
    "categories": [
      "tutorial",
      "rstats",
      "spatial",
      "Berlin",
      "ggplot2",
      "data sources",
      "workflow"
    ],
    "contents": "\n\nContents\nInstallation\nA Basic Template Map of\nImperviousness\nBerlin Data Sets\nAdding Locations to the\nMap\nCustom\nStyling\nSave Map\n\nThe {d6berlin}\npackage aims provide template maps for Berlin with carefully chosen\nand aesthetically pleasing defaults. Template maps include green spaces,\nimperviousness levels, water bodies, district borders, roads, and\nrailways, plus the utility to add a globe with locator pin, a scalebar,\nand a caption to include the data sources.\nThere are two main functionalities:\nCreate a template map with imperviousness and\ngreen spaces with base_map_imp()\nProvide various ready-to-use Berlin data sets\nwith sf_*\nFurthermore, the package provides utility to add a globe with locator\npin, a scalebar, and a caption to include the data sources.\nInstallation\nYou can install the {d6berlin} package from GitHub:\n\n\n## install.packages(\"remotes\")\n## remotes::install_github(\"EcoDynIZW/d6berlin\")\nlibrary(d6berlin)\n\n\n\nNote: If you are asked if you want to update other packages either\npress ‚ÄúNo‚Äù (option 3) and continue or update the packages before running\nthe install command again.\nA Basic Template Map of Imperviousness\nThe basic template map shows levels of imperviousness and green areas\nin Berlin. The imperviousness raster data was derived from Geoportal Berlin (FIS-Broker) with a resolution of 10m. The\nvector data on green spaces was collected from data provided by the OpenStreetMap Contributors.\nThe green spaces consist of a mixture of land use and natural categories\n(namely ‚Äúforest‚Äù, ‚Äúgrass‚Äù, ‚Äúmeadow‚Äù, ‚Äúnature_reserve‚Äù, ‚Äúscrub‚Äù, ‚Äúheath‚Äù,\n‚Äúbeach‚Äù, ‚Äúcliff‚Äù).\nThe map is projected in EPSG 4326 (WGS84).\n\n\nd6berlin::base_map_imp()\n\n\n#> Aggregating raster data.\n#> Plotting basic map.\n#> Styling map.\n\n\nYou can also customize the arguments, e.g.¬†change the color\nintensity, add a globe with a locator pin, change the resolution of the\nraster, and move the legend to a custom position:\n\n\nbase_map_imp(color_intensity = 1, globe = TRUE, resolution = 500,\n             legend_x = .17, legend_y = .12)\n\n\n\n\nIf you think the legend is not need, there is also an option called\n\"none\". (The default is \"bottom\". You can also\nuse of the predefined setting \"top\" as illustrated below or\na custom position as shown in the previous example.)\nBerlin Data Sets\nThe package contains several data sets for Berlin. All of them start\nwith sf_, e.g.¬†d6berlin::sf_roads. Here is a\nfull overview of the data sets that are available:\n\n\n\nAdding Locations to the Map\nLet‚Äôs assume you have recorded some animal locations or you want to\nplot another information on to of this plot. For example, let‚Äôs\nvisualize the Berlin metro stations by adding\ngeom_sf(data = x) to the template map:\n\n\nlibrary(ggplot2)\nlibrary(sf)\n\nmap <- base_map_imp(color_intensity = .4, resolution = 500, legend = \"top\")\n\nmap + geom_sf(data = sf_metro) ## sf_metro is contained in the {d6berlin} package\n\n\n\n\nNote: Since the template map contains many filled\nareas, we recommend to add geometries with variables mapped to\ncolor|colour|col to the template maps.\nYou can, of course, style the appearance of the points as usual:\n\n\nmap + geom_sf(data = sf_metro, shape = 8, color = \"red\", size = 2)\n\n\n\n\nIt is also possible to filter the data inside the\ngeom_sf function ‚Äî no need to use subset:\n\n\nlibrary(dplyr) ## for filtering\nlibrary(stringr) ## for filtering based on name\n\nmap + \n  geom_sf(data = filter(sf_metro, str_detect(name, \"^U\")), \n          shape = 21, fill = \"dodgerblue\", size = 2) +\n  geom_sf(data = filter(sf_metro, str_detect(name, \"^S\")), \n          shape = 21, fill = \"forestgreen\", size = 2)\n\n\n\n\nYou can also use the mapping functionality of ggplot2 to\naddress variables from your data set.\n\n\nmap + \n  geom_sf(data = sf_metro, aes(color = type), size = 2) +\n  scale_color_discrete(type = c(\"forestgreen\", \"dodgerblue\"), \n                       name = NULL) +\n  guides(color = guide_legend(direction = \"horizontal\",\n                              title.position = \"top\", \n                              title.hjust = .5))\n\n\n\n\n(It looks better if you style the legend in the same horizontal\nlayout.)\nCustom Styling\nSince the output is a ggplot object, you can manipulate\nthe result as you like (but don‚Äôt apply a new theme, this will mess up\nthe legend design):\n\n\nlibrary(systemfonts) ## for title font\n\nbase_map_imp(color_intensity = 1, resolution = 250, globe = TRUE,\n             legend_x = .17, legend_y = .12) + \n  geom_sf(data = sf_metro, shape = 21, fill = \"white\", \n          stroke = .4, size = 4) +\n  ggtitle(\"Metro Stations in Berlin\") + \n  theme(plot.title = element_text(size = 30, hjust = .5, family = \"Bangers\"),\n        panel.grid.major = element_line(color = \"white\", linewidth = .3),\n        axis.text = element_text(color = \"black\", size = 8),\n        plot.background = element_rect(fill = \"#fff0de\", color = NA),\n        plot.margin = margin(rep(20, 4)))\n\n\n\n\nSave Map\nUnfortunately, the size of the text elements is fixed. The best\naspect ratio to export the map is 12x9 and you can save it with\nggsave() for example:\n\n\nggsave(\"metro_map.pdf\", width = 12, height = 9, device = cairo_pdf)\n\n\n\n\n\n\n",
    "preview": "https://raw.githubusercontent.com/EcoDynIZW/EcoDynIZW.github.io/main/img/hex-d6berlin.png",
    "last_modified": "2023-07-03T16:29:10+02:00",
    "input_file": {}
  },
  {
    "path": "posts/d6package/",
    "title": "Simplify Workflows of D6 Research Projects",
    "description": "Learn how to use the {d6} package to follow the project workflow within the department ‚ÄúEcological Dynamics‚Äù at the Leibniz Institute for Zoo and Wildlife Research. The package functionality allows you to set up a standardized folder structure, to use templates for standardized reports and provides some helpful utility functions.",
    "author": [
      {
        "name": "Cedric Scherer",
        "url": "https://cedricscherer.com"
      }
    ],
    "date": "2020-12-09",
    "categories": [
      "tutorial",
      "rstats",
      "workflow",
      "data management"
    ],
    "contents": "\n\nContents\nInstallation\nCreate Project Directory\nUse A Custom Root Directory\nUse Version Control\nSetup without Geo Directories\nAdd Documentation to Your Project\n\nInstall Common Packages\nUse Custom Rmarkdown Templates\nRender Rmarkdown Files to Reports\n\nThe {d6} package aims to simplify workflows of our D6 research projects by providing a standardized folder structure incl.¬†version control, Rmarkdown templates, and other utilities.\nThere are four main functionalities:\nCreate standardized project directories with\nnew_project()\nInstall a set of common packages with\ninstall_d6_packages()\nProvide custom Rmarkdown templates via\nFile > New File > Rmarkdown... > From Template\nRender all Rmarkdown documents to ./docs/report with\nrender_all_reports() or\nrender_report()\nInstallation\nThe package is not on CRAN and needs to be installed from GitHub. To do\nso, open Rstudio and run the following two lines in the console. In case\nthe {devtools} package is already installed, skip that step.\n\n\ninstall.packages(\"devtools\")\ndevtools::install_github(\"EcoDynIZW/d6\")\n\n\n(Note: If you are asked if you want to update other packages either press ‚ÄúNo‚Äù (option 3) and continue or update the packages before running the install command again.)\nCreate Project Directory\nRun the function new_project() to create a new project. This will\ncreate a standardized directory with all the scaffolding we use for all\nprojects in our department. It also add several files needed for\ndocumentation of your project.\nTo start a new project in the current working directory, simply run:\n\n\nd6::new_project(\"unicornus_wl_sdm_smith_j\")\n\n\nPlease give your project a unique and descriptive name:\nspecies_country_topic_name\nFor example, when John Smith is developing a species distribution models\nfor unicorns in Wonderland, a descriptive title could be:\nunicornus_wl_sdm_smith_j. Please use underscores and the\ninternational Alpha-2 encoding for\ncountries.\nThe full scaffolding structure created in the root folder (here\nunicornus_wl_sdm_smith_j) is the following:\nUse A Custom Root Directory\nYou do not need to change the working directory first‚Äîyou can also\nspecify a path to a custom root folder in which the new project folder\nis created:\n\n\n## both work:\nd6::new_project(\"unicornus_wl_sdm_smith_j\", path = \"absolute/path/to/the/root/folder\")\n## or:\nd6::new_project(\"unicornus_wl_sdm_smith_j\", path = \"absolute/path/to/the/root/folder/\")\n\n\nThe resulting final directory of your project would be\nabsolute/path/to/the/root/folder/unicornus_wl_sdm_smith_j.\nUse Version Control\nIf you want to create a GitHub repository for the project at the same\ntime, use instead:\n\n\nd6::new_project(\"unicornus_wl_sdm_smith_j\", github = TRUE)\n\n\nBy default, the visibility of the GitHub repository is set to ‚Äúprivate‚Äù\nbut you can also change that:\n\n\nd6::new_project(\"unicornus_wl_sdm_smith_j\", github = TRUE, private_repo = FALSE)\n\n\nNote that to create a GitHub repo you will need to have configured your\nsystem as explained here.\nSetup without Geo Directories\nIf your project does not (or will not) contain any spatial data, you can\nprevent the creation of the directories geo-raw and geo-proc by\nsetting geo to FALSE:\n\n\nd6::new_project(\"unicornus_wl_sdm_smith_j\", geo = FALSE)\n\n\nAdd Documentation to Your Project\nAfter you have set up your project directory, open the file 00_start.R\nin the R folder. Add the details of your project, fill in the readme,\nadd a MIT license (if needed) and add package dependencies.\nInstall Common Packages\nYou can install the packages that are most commonly used in our\ndepartment via install_d6_packages():\n\n\nd6::install_d6_packages()\n\n\nNote that this function is going to check preinstalled versions and will\nonly install packages that are not installed with your current R\nversion.\nAgain, there is an arguement geo so you can decide if you want to\ninstall common geodata packages as well (which is the default). If you\nare not intending to process geodata, set geo to FALSE:\n\n\nd6::install_d6_packages(geo = FALSE)\n\n\nThe default packages that are going to be installed are:\n\ntidyverse (tibble, dplyr, tidyr, ggplot2, readr, forecats, stringr, purrr), lubridate, here, vroom, patchwork, usethis\n\nThe following packages will be installed in case you specify\ngeo = TRUE:\n\nrgdal, geos, raster, sp, sf, tmap\n\nUse Custom Rmarkdown Templates\nThe package also provides several templates for your scripts. In\nRstudio, navigate to File > New File > RMarkdown... > Templates and\nchoose the template you want to use. All templates come with a\npreformatted YAML header and chunks for the setup.\nThe following templates are available for now:\nEcoDynIZW Basic: Template for a basic Rmarkdown research report\nincluding bits of codes and comments to get started\nEcoDynIZW Minimal: Template for an Rmarkdown research report\n(almost empty)\nRender Rmarkdown Files to Reports\nThe render_*() functions take care of knitting your Rmarkdown files\ninto HTML reports. The functions assume that your .Rmd files are saved\nin the R directory or any subdirectory, and will store the resulting\n.html files in the according directory, namely ./docs/reports/.\nYou can render all .Rmd files that are placed in the R directory and\nsubdirectories in one step:\n\n\nd6::render_all_reports()\n\n\nYou can also render single Rmarkdown documents via render_report():\n\n\nd6::render_report(\"my-report.Rmd\")\nd6::render_report(\"notsurewhybutIhaveasubfolder/my-report.Rmd\")\n\n\n\n\n\n",
    "preview": "https://raw.githubusercontent.com/EcoDynIZW/EcoDynIZW.github.io/main/img/hex-d6.png",
    "last_modified": "2023-07-03T16:29:12+02:00",
    "input_file": {}
  }
]
